# CIE6032 and MDS6232: Homework 2 Programming - CNN on CIFAR

A simple CNN
implementation for image classification. We use
[Mxnet](https://mxnet.apache.org/)/ [Pytorch](http://pytorch.org/) framework and
the installation is detailed on the corresponding
official website.

## How to
submit your code

Please write an `.ipynb` file including all your
code/figure/necessary(and brief!) comments. 

For data science students, you can
use Amazon AWS web service to launch GPU mode if you do not have it in local
computer.
Maximum reimbursement of 180 RMB per person at the end of the course.
**Remember** to close the instance every time you log off.

Compress your answer
of Problem 1, theory part in Problem 2 and ipython notebook file to
Assignment2.zip and submit the zip file to TA email before A2 deadline.
**NOTE**
that the derivation of theory part in Problem 2 has to be written
alongside with
other problems; no need to write it again in the notebook.


## Problem1

### a）
$[[L_tf]*w](x)=\sum_{y\in Z^2}
\sum^{K}_{k=1}
[L_tf]_k(y)w_k(y-x))=\sum_{y\in
Z^2} \sum^{K}_{k=1}
f_k(y-t)w_k(y-x)=\sum_{y\in
Z^2} \sum^{K}_{k=1}
f_k(y)w_k(y-x+t)=\sum_{y\in Z^2} \sum^{K}_{k=1}
f_k(y)w_k(y-(x-t))=[L_t[f*w]](x)
$

### b)
$ R=\left\{
\begin{matrix}
cos(\pi/2) & -sin(\pi/2)\\
   sin(\pi/2) &
cos(\pi/2)
\end{matrix}
\right\}=\left\{
\begin{matrix}
   0& -1\\
   1 & 0
\end{matrix}
  \right\}
R^{-1}=\left\{
\begin{matrix}
   0& 1\\
   -1 & 0
\end{matrix}
  \right\}$
$[[L_Rf]*w](x)=\sum_{y\in Z^2} \sum^{K}_{k=1}
[L_Rf]_k(y)w_k(y-x)= \sum_{y\in
Z^2} \sum^{K}_{k=1} f_k(R^{-1}y)w_k(y-x)=
\sum_{y\in Z^2} \sum^{K}_{k=1}
f_k(R^{-1}y)w_k(y-x)=\sum_{y\in Z^2}
\sum^{K}_{k=1} f_k(y)w_k(Ry-x)=\sum_{y\in
Z^2} \sum^{K}_{k=1}
f_k(y)w_k(R(y-R^{-1}x))= \sum_{y\in Z^2} \sum^{K}_{k=1}
f_k(y)[L_{R^{-1}}w]_k(y-R^{-1}x)=L_R[f*[L_{R^{-1}}w]](x)$      


### c)
$[[L_uf]*w](g)=\sum_{h\in G} \sum^{K}_{k=1}[L_uf]_k(h)w_k(g^{-1}h)=\sum_{h\in G}
\sum^{K}_{k=1}f_k(u^{-1}h)w_k(g^{-1}h)=\sum_{h\in G}
\sum^{K}_{k=1}f_k(h)w_k(g^{-1}uh)=\sum_{h\in G}
\sum^{K}_{k=1}f_k(h)w_k((u^{-1}g)^{-1}h)=[f*w](u^{-1}g)=[L_u[f*w]](g)$

To
implement how to implement this group convolution with traditional convolution
and by rotating the feature map or filter, we need to duplicate and rotate the
input image by $0^{o}, 90^{o}, 180^{o}, 270^{o}$ (input channels*4)and do the
same to the original filter. Then each filter receives four times of input
channels. In the end, we compute the 1/4 of the results.

## Problem2

#### b)
Prove

Equa10:

$E[x_l]=0$(As the x has been normalized.) =>
$Var[x_l]=E[x_l^2]-(E[x_l])^2=E[x_l^2]$
$Var[w_l*x_l]=E[w_l^2*x_l^2]-(E[w_l*x_l])^2$($w_l$ and $x_l$ are independent) =>
$Var[w_l*x_l]=E[w_l^2]E[x_l^2]-(E[w_l]E[x_l])^2=E[w_l^2]E[x_l^2]=E[w_l^2]Var(x_l)=(E[w_l^2]-0)Var(x_l)=(E[w_l^2]-(E[w_l])^2)Var(x_l)$($w_l$
has been normalized)=$Var(w_l)*E[w_l^2]$

Thus
$Var[y_l]=n_l*Var[w_l*x_l]=n_l*Var(w_l)*E[w_l^2]$

Equa11:

$y_l=Relu(x_l)$

we
set $z_l=Relu(-x_l)$, Thus $z_l||y_l=x_l$, and $Var(z_l)=Var(y_l)=1/2Var(x_l)$
### a)

```{.python .input  n=3}
from __future__ import division

import argparse, time, logging, random, math

import numpy as np
import mxnet as mx

from mxnet import gluon, nd
from mxnet import autograd as ag
from mxnet.gluon import nn
from mxnet.gluon.data.vision import transforms

from gluoncv.model_zoo import get_model
from gluoncv.utils import makedirs, TrainingHistory


################################################################
#
# There are numerous structures for convolutional neural networks.
# Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the
# tutorial.

# number of GPUs or CPU to use if you have
# num_gpus = 1
ctx = mx.cpu(0)
```

```{.python .input  n=18}
train_data = gluon.data.DataLoader(
    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),
    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)

# Set train=False for validation data
val_data = gluon.data.DataLoader(
    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),
    batch_size=batch_size, shuffle=False, num_workers=num_workers)

```

```{.python .input  n=5}
class MyInit(mx.init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.normal(loc=0, scale=0.01, shape=data.shape )
        
class MyInit2(mx.init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        print 1./data.shape[0]
        data[:] = nd.random.normal(loc=0, scale=1./data.shape[0], shape=data.shape )
```

```{.python .input  n=8}
############################################################
# your code here to define your net according to problem 2 #
num_output=10
net = nn.Sequential()
with net.name_scope():
    net.add(nn.Conv2D(channels=6, kernel_size=5,activation='relu')) 
    net.add(nn.MaxPool2D(pool_size=2, strides=2))
    net.add(nn.Conv2D(channels=16,kernel_size=5, activation='relu')) 
    net.add(nn.Flatten())
    net.add(nn.Dense(120, activation="relu")) 
    net.add(nn.Dense(84,activation="relu")) 
    net.add(nn.Dense(num_output)) 

print net
net.initialize(MyInit(),ctx=ctx)

```

```{.json .output n=8}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (2): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (3): Flatten\n  (4): Dense(None -> 120, Activation(relu))\n  (5): Dense(None -> 84, Activation(relu))\n  (6): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=9}
################################################################
# Data Augmentation and Data Loader
# ---------------------------------
#
# Data augmentation is a common technique used for training. It is
# base on the assumption that, for the same object, photos under different
# composition, lighting condition, or color should all yield the same prediction.
#
# Here are photos of the Golden Bridge, taken by many people,
# at different time from different angles.
# We can easily tell that they are photos of the same thing.
#
# |image-golden-bridge|
#
# We want to teach this invariance to our model, by playing "augmenting"
# input image. Our augmentation transforms the image with
# resizing, cropping, flipping and other techniques.
#
# With ``Gluon``, we can create our transform function as following:

transform_train = transforms.Compose([
    # Randomly crop an area, and then resize it to be 32x32
    transforms.RandomResizedCrop(32),
    # Randomly flip the image horizontally
    transforms.RandomFlipLeftRight(),
    # Randomly jitter the brightness, contrast and saturation of the image
    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
    # Randomly adding noise to the image
    transforms.RandomLighting(0.1),
    # Transpose the image from height*width*num_channels to num_channels*height*width
    # and map values from [0, 255] to [0,1]
    transforms.ToTensor(),
    # Normalize the image with mean and standard deviation calculated across all images
    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])
])

################################################################
# You may have noticed that most of the operations are randomized. This in effect
# increases the number of different images the model sees during training.
# The more data we have, the better our model generalizes over
# unseen images.
#
# On the other hand, when making prediction, we would like to remove all
# random operations in order to get a deterministic result. The transform
# function for prediction is:

transform_test = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])
])

################################################################
# Note that it is important to keep the normalization step, since the
# model only works well on inputs from the same distribution.
#
# With the transform functions, we can define data loaders for our
# training and validation datasets.

# Batch Size for Each GPU
per_device_batch_size = 256
# Number of data loader workers
num_workers = 8
# Calculate effective total batch size
num_cpus=1

batch_size = per_device_batch_size * num_cpus

# Set train=True for training data
# Set shuffle=True to shuffle the training data


```

```{.python .input  n=10}
#net.initialize(ctx=ctx)
################################################################
# Optimizer, Loss and Metric
# --------------------------
#
# Optimizer improves the model during training. Here we use the popular
# Nesterov accelerated gradient descent algorithm.

# Learning rate decay factor
lr_decay = 0.1
# Epochs where learning rate decays
lr_decay_epoch = [80, 160, np.inf]

# net.collect_params().initialize(MyInit(),ctx=ctx)

# standard SGD gradient descent
optimizer = 'sgd'
# Set parameters
optimizer_params = {'learning_rate': 0.01, 'wd': 0.0005, 'momentum': 0.9}

# Define our trainer for net
trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)

################################################################
# In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly
# used in ``trainer``. One important idea in model training is to
# gradually decrease learning rate. This means the optimizer takes large
# steps at the beginning, but step size becomes smaller and smaller in time.
#
#
# In order to optimize our model, we need a loss function.
# In essence, loss functions compute the difference between predictions and the
# ground-truth as a measure of model performance.
# We can then take the gradients of the loss w.r.t. the weights.
# Gradients points the optimizer to the direction weights should move to
# improve model performance.
#
# For classification tasks, we usually use softmax cross entropy as the
# loss function.

loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()

################################################################
# Metrics are similar to loss functions, but they are different in the
# following aspects:
#
# -  Metric is how we evaluate model performance. Each metric is related to a
#    specific task, but independent from the model training process.
# -  For classification, we usually only use one loss function to train
#    our model, but we can have several metrics for evaluating
#    performance.
# -  Loss function can be used as a metric, but sometimes its values are hard
#    to interpretate. For instance, the concept "accuracy" is
#    easier to understand than "softmax cross entropy"
#
# For simplicity, we use accuracy as the metric to monitor our training
# process. Besides, we record metric values, and will print them at the
# end of training.

train_metric = mx.metric.Accuracy()
train_history = TrainingHistory(['training-error', 'validation-error'])

################################################################
# Validation
# ----------
#
# Validation dataset provides us a way of monitoring the training process.
# We have labels for validation data, but they are held out during training.
# Instead, we use them to evaluate the models performance on unseen data
# and prevent overfitting.

ctx_list=[mx.cpu(0)]
epochs = 150
lr_decay_count = 0

```

```{.python .input  n=23}
def test(ctx, val_data,net):
    metric = mx.metric.Accuracy()
    for i, batch in enumerate(val_data):
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)
        outputs = [net(X) for X in data]
        metric.update(label, outputs)
    return metric.get()
```

```{.python .input  n=119}
################################################################
# In order to evaluate performance, we need a metric. Then, we loop
# through the validation data and predict with our model.
# We'll run this function at the end of every epoch to show improvement.
# over the last epoch.
#
# Training
# --------
#
# After all the preparations, we can finally start training!
# Following is the script.
#
# .. note::
#   In order to finish the tutorial quickly, we only train for 3 epochs.
#   In your experiments, we recommend setting ``epochs=240``.



for epoch in range(epochs):
    tic = time.time()
    train_metric.reset()
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        trainer.set_learning_rate(trainer.learning_rate*lr_decay)
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
            ###print net.collect_params().get('weights').data()
#             print net[i].weight.data()
        # AutoGrad
        with ag.record():
            output = [net(X) for X in data]
            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]

        # Backpropagation
        for l in loss:
            l.backward()

        # Optimize
        trainer.step(batch_size)

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric.update(label, output)

    name, acc = train_metric.get()
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net)

    # Update history and print metrics
    train_history.update([1-acc, 1-val_acc])
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net[0].weight.data()[0][0]






```

```{.json .output n=119}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "0\n\n[[ 0.01115617  0.00460684  0.00282667  0.00141643 -0.01132535]\n [ 0.01495352 -0.00533932 -0.02279573  0.0051406   0.02575142]\n [ 0.01201282 -0.00538834 -0.00667704 -0.00141113  0.00353873]\n [ 0.00386042  0.00539556 -0.02671838  0.01025087 -0.00604055]\n [ 0.01755032 -0.01117481  0.00038735 -0.02430729 -0.00583536]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01113201  0.00459573  0.00281802  0.00140993 -0.01130598]\n [ 0.0149207  -0.00533112 -0.02275318  0.0051251   0.0256937 ]\n [ 0.01198504 -0.0053816  -0.00666934 -0.00141507  0.00352502]\n [ 0.00384846  0.00537846 -0.02667117  0.01022162 -0.00603618]\n [ 0.01750979 -0.01115858  0.00037871 -0.0242657  -0.00583181]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.01109788  0.00457654  0.0028025   0.00139854 -0.01128469]\n [ 0.01487601 -0.00532718 -0.02270696  0.00510244  0.025621  ]\n [ 0.01194622 -0.00537927 -0.0066656  -0.00142423  0.00350496]\n [ 0.00382853  0.00535241 -0.02661945  0.01018089 -0.00603514]\n [ 0.01745489 -0.01114427  0.00036204 -0.02422163 -0.00583311]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.0110661   0.00456089  0.00279037  0.00139008 -0.01126143]\n [ 0.01483406 -0.00531928 -0.02265732  0.00508284  0.02555064]\n [ 0.01191011 -0.00537314 -0.00665818 -0.00142974  0.00348796]\n [ 0.0038107   0.00532924 -0.02656469  0.01014379 -0.00603085]\n [ 0.01740142 -0.01112835  0.0003476  -0.02417472 -0.0058314 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 0] train=0.096975 val=0.100000 loss=114950.722412 time: 24.672896\n0\n\n[[ 0.01103717  0.00454611  0.00277896  0.00138202 -0.01124074]\n [ 0.01479593 -0.00531306 -0.02261292  0.00506543  0.02548779]\n [ 0.01187733 -0.00536846 -0.00665174 -0.00143423  0.00347322]\n [ 0.00379448  0.00530792 -0.02651533  0.01011152 -0.00602588]\n [ 0.01735329 -0.01111406  0.00033528 -0.02413123 -0.00582861]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01100693  0.00453114  0.00276698  0.00137359 -0.01121752]\n [ 0.01475519 -0.00530526 -0.02256336  0.00504634  0.02541828]\n [ 0.01184167 -0.00536326 -0.00664461 -0.0014391   0.00345687]\n [ 0.00377731  0.00528444 -0.02646044  0.01007592 -0.00602051]\n [ 0.01730095 -0.01109756  0.00032241 -0.02408228 -0.0058251 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.0109758   0.00451519  0.0027546   0.00136485 -0.01119491]\n [ 0.01471313 -0.00529908 -0.02251546  0.0050257   0.02534722]\n [ 0.01180469 -0.00535965 -0.00663942 -0.00144663  0.00343788]\n [ 0.00375905  0.00525975 -0.02640754  0.01003761 -0.00601774]\n [ 0.0172484  -0.01108162  0.00030824 -0.02403547 -0.00582329]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01094269  0.00449759  0.00274044  0.00135442 -0.0111736 ]\n [ 0.01466959 -0.0052942  -0.02246914  0.00500395  0.02527593]\n [ 0.01176708 -0.00535662 -0.00663512 -0.00145441  0.00341964]\n [ 0.00374046  0.00523513 -0.02635485  0.0099999  -0.00601385]\n [ 0.01719489 -0.01106608  0.00029431 -0.02398837 -0.00582107]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 1] train=0.097756 val=0.100000 loss=114951.113464 time: 24.078214\n0\n\n[[ 0.01091334  0.00448193  0.00272804  0.00134518 -0.01115442]\n [ 0.01463065 -0.00528951 -0.0224268   0.00498469  0.02521163]\n [ 0.01173319 -0.00535347 -0.00663042 -0.00146146  0.00340217]\n [ 0.00372323  0.00521286 -0.02630774  0.00996518 -0.00601205]\n [ 0.0171464  -0.01105294  0.00028034 -0.02394781 -0.00582094]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01088357  0.0044676   0.00271661  0.00133632 -0.01113222]\n [ 0.01458995 -0.00528162 -0.02237796  0.00496479  0.02514189]\n [ 0.0116974  -0.00534802 -0.00662368 -0.00146767  0.00338482]\n [ 0.00370589  0.00519031 -0.02625329  0.00992865 -0.00600828]\n [ 0.01709486 -0.0110353   0.00026813 -0.02390021 -0.00581888]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.01085342  0.00445255  0.00270367  0.00132636 -0.01111052]\n [ 0.01454966 -0.00527408 -0.02233052  0.00494397  0.02507163]\n [ 0.01166208 -0.00534323 -0.00661841 -0.00147523  0.003366  ]\n [ 0.00368859  0.00516634 -0.02620134  0.00989007 -0.00600653]\n [ 0.01704249 -0.01102035  0.00025251 -0.02385566 -0.00581951]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01082155  0.00443578  0.00269     0.00131609 -0.01108955]\n [ 0.0145069  -0.00526903 -0.02228414  0.00492307  0.02500126]\n [ 0.01162429 -0.00534089 -0.00661418 -0.00148281  0.00334735]\n [ 0.00366901  0.00514024 -0.02615076  0.009851   -0.00600484]\n [ 0.01698842 -0.01100716  0.00023606 -0.02381162 -0.00581989]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 2] train=0.098077 val=0.100000 loss=114950.438232 time: 23.993111\n0\n\n[[ 0.0107938   0.00442096  0.00267759  0.00130632 -0.01107143]\n [ 0.01446932 -0.00526432 -0.02224312  0.00490281  0.02493631]\n [ 0.01159058 -0.00533896 -0.00661154 -0.00149142  0.00332878]\n [ 0.00365134  0.00511661 -0.02610616  0.00981493 -0.00600433]\n [ 0.01693981 -0.01099533  0.00022094 -0.02377261 -0.00582088]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01076421  0.00440618  0.0026657   0.00129717 -0.01105005]\n [ 0.01442949 -0.00525659 -0.02219484  0.00488301  0.0248665 ]\n [ 0.01155615 -0.00533353 -0.00660497 -0.00149747  0.00331104]\n [ 0.00363462  0.00509335 -0.0260536   0.00977818 -0.00600091]\n [ 0.01688853 -0.0109796   0.00020657 -0.02372702 -0.00581959]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.01073299  0.00438971  0.00265206  0.00128692 -0.01102904]\n [ 0.0143875  -0.00525148 -0.0221494   0.00486094  0.02479566]\n [ 0.01151901 -0.00533142 -0.00660173 -0.00150625  0.00329184]\n [ 0.00361541  0.00506739 -0.02600396  0.00973904 -0.00599909]\n [ 0.0168353  -0.0109661   0.00018996 -0.0236837  -0.00582005]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01070042  0.0043717   0.00263679  0.00127392 -0.01101167]\n [ 0.01434451 -0.00524731 -0.02210507  0.0048373   0.02472233]\n [ 0.01148131 -0.00532947 -0.00659874 -0.0015161   0.00327049]\n [ 0.00359605  0.00504164 -0.0259544   0.0096994  -0.00599911]\n [ 0.01678203 -0.01095257  0.00017348 -0.02364058 -0.00582153]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 3] train=0.098077 val=0.100000 loss=114950.601624 time: 23.037423\n0\n\n[[ 0.01067068  0.00435456  0.00262162  0.00126184 -0.01099533]\n [ 0.01430521 -0.00524476 -0.02206705  0.00481534  0.02465681]\n [ 0.01144598 -0.00532937 -0.00659818 -0.00152622  0.00325063]\n [ 0.00357686  0.00501632 -0.02591253  0.00966164 -0.00600051]\n [ 0.01673239 -0.01094275  0.0001559  -0.02360471 -0.00582499]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01063782  0.00433699  0.00260727  0.00125079 -0.01097551]\n [ 0.01426226 -0.00524018 -0.02202205  0.0047934   0.02458605]\n [ 0.01140908 -0.00532642 -0.00659412 -0.00153495  0.00323085]\n [ 0.00355808  0.00499145 -0.0258624   0.00962246 -0.00599969]\n [ 0.01667913 -0.01092942  0.0001397  -0.0235619  -0.00582599]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.01060493  0.00431867  0.00259189  0.00123877 -0.01095634]\n [ 0.01421896 -0.00523678 -0.02197855  0.00477024  0.02451441]\n [ 0.0113707  -0.00532546 -0.00659195 -0.00154522  0.00320958]\n [ 0.00353728  0.00496426 -0.02581455  0.0095816  -0.00600064]\n [ 0.01662411 -0.01091817  0.00012143 -0.02352138 -0.00582945]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01057544  0.00430363  0.00257989  0.00123004 -0.0109343 ]\n [ 0.01417898 -0.00523002 -0.0219317   0.00475079  0.0244465 ]\n [ 0.01133604 -0.0053209  -0.00658609 -0.00155118  0.00319269]\n [ 0.00352046  0.00494118 -0.02576256  0.00954586 -0.00599687]\n [ 0.0165735  -0.01090243  0.00010795 -0.02347557 -0.00582781]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 4] train=0.097977 val=0.100000 loss=114950.804565 time: 23.275161\n0\n\n[[ 1.05489371e-02  4.28950554e-03  2.56781629e-03  1.22056948e-03\n  -1.09157525e-02]\n [ 1.41426288e-02 -5.22537762e-03 -2.18918305e-02  4.73094545e-03\n   2.43840050e-02]\n [ 1.13038467e-02 -5.31853084e-03 -6.58346759e-03 -1.55915739e-03\n   3.17598670e-03]\n [ 3.50386580e-03  4.91852546e-03 -2.57185791e-02  9.51117370e-03\n  -5.99493273e-03]\n [ 1.65261161e-02 -1.08904066e-02  9.33230040e-05 -2.34368946e-02\n  -5.82780270e-03]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 1.05193304e-02  4.27332288e-03  2.55352515e-03  1.20895624e-03\n  -1.08968997e-02]\n [ 1.41024515e-02 -5.21988655e-03 -2.18470562e-02  4.70913900e-03\n   2.43139397e-02]\n [ 1.12688588e-02 -5.31504024e-03 -6.57923566e-03 -1.56716211e-03\n   3.15724476e-03]\n [ 3.48699582e-03  4.89497837e-03 -2.56679747e-02  9.47420765e-03\n  -5.99241443e-03]\n [ 1.64759755e-02 -1.08749140e-02  7.92671417e-05 -2.33919583e-02\n  -5.82667999e-03]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 1.0487929e-02  4.2555025e-03  2.5379364e-03  1.1964688e-03\n  -1.0878773e-02]\n [ 1.4060135e-02 -5.2168844e-03 -2.1805156e-02  4.6848822e-03\n   2.4241617e-02]\n [ 1.1230844e-02 -5.3151064e-03 -6.5789446e-03 -1.5791781e-03\n   3.1347983e-03]\n [ 3.4657016e-03  4.8667151e-03 -2.5622768e-02  9.4318120e-03\n  -5.9947330e-03]\n [ 1.6420627e-02 -1.0865030e-02  5.9286165e-05 -2.3353303e-02\n  -5.8307308e-03]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 1.04538202e-02  4.23525320e-03  2.51996983e-03  1.18150609e-03\n  -1.08632678e-02]\n [ 1.40165845e-02 -5.21499664e-03 -2.17645727e-02  4.65914141e-03\n   2.41680779e-02]\n [ 1.11926896e-02 -5.31509798e-03 -6.57877279e-03 -1.59158907e-03\n   3.11163417e-03]\n [ 3.44512402e-03  4.83941240e-03 -2.55767107e-02  9.39026009e-03\n  -5.99724799e-03]\n [ 1.63663719e-02 -1.08541679e-02  4.02308033e-05 -2.33142469e-02\n  -5.83551778e-03]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 5] train=0.098257 val=0.100000 loss=114952.020020 time: 22.807251\n0\n\n[[ 1.0425455e-02  4.2193513e-03  2.5064298e-03  1.1709511e-03\n  -1.0845862e-02]\n [ 1.3978731e-02 -5.2117445e-03 -2.1725953e-02  4.6387417e-03\n   2.4105106e-02]\n [ 1.1159295e-02 -5.3140009e-03 -6.5766973e-03 -1.6002570e-03\n   3.0936121e-03]\n [ 3.4279074e-03  4.8163901e-03 -2.5533000e-02  9.3554668e-03\n  -5.9968601e-03]\n [ 1.6319884e-02 -1.0842049e-02  2.6167307e-05 -2.3276281e-02\n  -5.8369837e-03]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 1.0391151e-02  4.1991905e-03  2.4889759e-03  1.1565107e-03\n  -1.0829921e-02]\n [ 1.3933797e-02 -5.2109761e-03 -2.1685973e-02  4.6131643e-03\n   2.4032053e-02]\n [ 1.1119620e-02 -5.3153480e-03 -6.5771476e-03 -1.6127796e-03\n   3.0708758e-03]\n [ 3.4057300e-03  4.7876849e-03 -2.5487997e-02  9.3136365e-03\n  -5.9991302e-03]\n [ 1.6264750e-02 -1.0832372e-02  6.8568002e-06 -2.3237189e-02\n  -5.8408137e-03]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 1.03602419e-02  4.18189354e-03  2.47346843e-03  1.14366785e-03\n  -1.08121084e-02]\n [ 1.38920853e-02 -5.20772161e-03 -2.16442924e-02  4.58936580e-03\n   2.39610747e-02]\n [ 1.10821128e-02 -5.31535130e-03 -6.57680677e-03 -1.62423577e-03\n   3.04916129e-03]\n [ 3.38476151e-03  4.75947000e-03 -2.54431628e-02  9.27199703e-03\n  -6.00149482e-03]\n [ 1.62102859e-02 -1.08227385e-02 -1.28497777e-05 -2.31986791e-02\n  -5.84529387e-03]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 1.0327043e-02  4.1625341e-03  2.4563922e-03  1.1294029e-03\n  -1.0796257e-02]\n [ 1.3849205e-02 -5.2051637e-03 -2.1602953e-02  4.5651356e-03\n   2.3889106e-02]\n [ 1.1043901e-02 -5.3151930e-03 -6.5758224e-03 -1.6356285e-03\n   3.0269623e-03]\n [ 3.3639087e-03  4.7322102e-03 -2.5397204e-02  9.2312200e-03\n  -6.0035130e-03]\n [ 1.6156631e-02 -1.0811752e-02 -3.0858853e-05 -2.3159241e-02\n  -5.8493414e-03]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 6] train=0.098237 val=0.100000 loss=114950.981079 time: 23.309420\n0\n\n[[ 1.0300022e-02  4.1474029e-03  2.4430242e-03  1.1190505e-03\n  -1.0779339e-02]\n [ 1.3812656e-02 -5.2017011e-03 -2.1565111e-02  4.5446763e-03\n   2.3826569e-02]\n [ 1.1011183e-02 -5.3142509e-03 -6.5745604e-03 -1.6448317e-03\n   3.0088702e-03]\n [ 3.3465512e-03  4.7084643e-03 -2.5355412e-02  9.1956835e-03\n  -6.0037957e-03]\n [ 1.6109738e-02 -1.0801224e-02 -4.6544566e-05 -2.3122748e-02\n  -5.8514480e-03]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 1.0268135e-02  4.1285702e-03  2.4259109e-03  1.1047272e-03\n  -1.0763653e-02]\n [ 1.3770650e-02 -5.1996084e-03 -2.1524915e-02  4.5200870e-03\n   2.3754984e-02]\n [ 1.0973897e-02 -5.3147427e-03 -6.5746242e-03 -1.6564282e-03\n   2.9875366e-03]\n [ 3.3264989e-03  4.6810112e-03 -2.5310267e-02  9.1553377e-03\n  -6.0042739e-03]\n [ 1.6056582e-02 -1.0790458e-02 -6.4591899e-05 -2.3082634e-02\n  -5.8533694e-03]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 1.02365464e-02  4.11029859e-03  2.41018832e-03  1.09193136e-03\n  -1.07464595e-02]\n [ 1.37287611e-02 -5.19710127e-03 -2.14836393e-02  4.49653855e-03\n   2.36844998e-02]\n [ 1.09364744e-02 -5.31490613e-03 -6.57385821e-03 -1.66761107e-03\n   2.96620629e-03]\n [ 3.30599281e-03  4.65374021e-03 -2.52647083e-02  9.11488757e-03\n  -6.00565597e-03]\n [ 1.60026699e-02 -1.07802600e-02 -8.28818738e-05 -2.30435748e-02\n  -5.85675566e-03]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01020468  0.00409244  0.0023944   0.00107882 -0.01072892]\n [ 0.01368636 -0.00519481 -0.02144313  0.00447268  0.02361444]\n [ 0.01089835 -0.00531565 -0.00657397 -0.00167927  0.0029448 ]\n [ 0.00328446  0.00462538 -0.02522094  0.00907312 -0.00600803]\n [ 0.01594765 -0.01077169 -0.00010339 -0.02300671 -0.00586178]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 7] train=0.097837 val=0.100000 loss=114949.155029 time: 22.534239\n0\n\n[[ 0.01017543  0.00407524  0.00237901  0.00106605 -0.01071445]\n [ 0.01364764 -0.00519376 -0.02140793  0.00445013  0.02355056]\n [ 0.01086427 -0.00531665 -0.00657485 -0.00169044  0.00292535]\n [ 0.00326608  0.00460023 -0.02518203  0.00903539 -0.00600984]\n [ 0.01589964 -0.01076328 -0.00012183 -0.02297363 -0.00586574]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01014499  0.00405789  0.00236345  0.00105273 -0.01069841]\n [ 0.01360649 -0.00519116 -0.02136768  0.00442616  0.02348008]\n [ 0.01082789 -0.00531632 -0.00657439 -0.00170158  0.00290446]\n [ 0.00324674  0.00457371 -0.02513703  0.00899525 -0.00601074]\n [ 0.01584825 -0.01075189 -0.00013948 -0.02293444 -0.00586835]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.01011517  0.00404116  0.00234881  0.00104106 -0.01068025]\n [ 0.01356713 -0.00518675 -0.02132575  0.00440404  0.02341161]\n [ 0.01079382 -0.00531376 -0.00657212 -0.00171111  0.002885  ]\n [ 0.00323031  0.00454986 -0.02509029  0.00895688 -0.00601029]\n [ 0.01579897 -0.01073888 -0.00015621 -0.02289444 -0.0058702 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01008423  0.00402299  0.00233185  0.00102611 -0.01066577]\n [ 0.01352568 -0.00518538 -0.02128771  0.00437757  0.02333886]\n [ 0.01075609 -0.00531574 -0.00657455 -0.00172539  0.00286081]\n [ 0.00320879  0.00452027 -0.02504924  0.00891326 -0.00601495]\n [ 0.01574465 -0.01073143 -0.0001782  -0.02285949 -0.0058767 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 8] train=0.097336 val=0.100000 loss=114951.990295 time: 24.682576\n0\n\n[[ 0.01005471  0.0040052   0.00231558  0.00101227 -0.01065253]\n [ 0.0134879  -0.00518418 -0.02125315  0.00435453  0.02327456]\n [ 0.01072296 -0.00531632 -0.00657517 -0.00173651  0.0028408 ]\n [ 0.00319087  0.00449594 -0.02500973  0.00887671 -0.00601683]\n [ 0.01569714 -0.01072227 -0.00019482 -0.02282522 -0.00587995]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01002624  0.00398994  0.00230154  0.00099999 -0.01063573]\n [ 0.01344953 -0.00517896 -0.02121118  0.00433192  0.02320537]\n [ 0.01068922 -0.00531343 -0.00657238 -0.00174591  0.0028212 ]\n [ 0.00317401  0.00447197 -0.02496309  0.00883848 -0.00601635]\n [ 0.01564769 -0.01070952 -0.00021099 -0.02278528 -0.00588134]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.0099924   0.00396832  0.00228109  0.00098211 -0.01062377]\n [ 0.01340521 -0.00518093 -0.02117687  0.00430262  0.02313114]\n [ 0.01064974 -0.00531779 -0.00657793 -0.00176339  0.00279462]\n [ 0.00315127  0.00444089 -0.02492517  0.00879157 -0.00602429]\n [ 0.01559274 -0.01070361 -0.00023525 -0.02275409 -0.00589132]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00996232  0.00395011  0.00226378  0.00096696 -0.01060955]\n [ 0.01336577 -0.00517875 -0.02113864  0.00427724  0.02306038]\n [ 0.01061519 -0.00531782 -0.00657906 -0.00177629  0.00277231]\n [ 0.00313347  0.00441428 -0.02488285  0.00874953 -0.00602763]\n [ 0.01554265 -0.0106933  -0.00025496 -0.02271825 -0.00589666]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 9] train=0.099139 val=0.100000 loss=114950.048279 time: 28.393581\n0\n\n[[ 0.00993554  0.00393444  0.00224914  0.00095448 -0.01059548]\n [ 0.01333045 -0.00517592 -0.02110313  0.00425584  0.02299827]\n [ 0.0105849  -0.00531639 -0.00657798 -0.00178554  0.00275453]\n [ 0.00311874  0.00439228 -0.02484219  0.00871474 -0.00602775]\n [ 0.01549929 -0.01068185 -0.00026989 -0.02268283 -0.00589794]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.0099043   0.00391504  0.00223026  0.00093774 -0.01058279]\n [ 0.01329054 -0.00517416 -0.02106588  0.00422981  0.02292698]\n [ 0.01055028 -0.00531677 -0.0065793  -0.00179837  0.00273207]\n [ 0.00310088  0.00436556 -0.02479988  0.00867339 -0.00603081]\n [ 0.01544916 -0.01067184 -0.00028936 -0.02264673 -0.00590269]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.0098751   0.00389754  0.00221321  0.00092266 -0.01056899]\n [ 0.01325257 -0.00517107 -0.02102756  0.00420505  0.0228571 ]\n [ 0.01051752 -0.00531558 -0.00657966 -0.00181034  0.00271097]\n [ 0.00308491  0.00434084 -0.02475608  0.00863356 -0.00603206]\n [ 0.01540106 -0.01065946 -0.00030664 -0.02260873 -0.00590538]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00984584  0.00388014  0.00219657  0.00090802 -0.01055437]\n [ 0.01321456 -0.00516758 -0.02098836  0.00418103  0.02278821]\n [ 0.01048588 -0.00531311 -0.00657831 -0.00182108  0.00269097]\n [ 0.00307139  0.00431841 -0.02471037  0.00859533 -0.00603182]\n [ 0.01535561 -0.01064515 -0.00032235 -0.02256955 -0.00590644]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 10] train=0.098938 val=0.100000 loss=114948.188477 time: 28.916001\n0\n\n[[ 0.00982175  0.0038664   0.00218307  0.00089563 -0.01054084]\n [ 0.01318297 -0.00516202 -0.02095151  0.00416034  0.02272707]\n [ 0.01045972 -0.00530842 -0.00657549 -0.00182972  0.00267389]\n [ 0.00306128  0.00430035 -0.02466796  0.00856197 -0.00603067]\n [ 0.01531718 -0.01063014 -0.00033489 -0.02253319 -0.00590649]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.00979318  0.0038487   0.002165    0.0008795  -0.01052739]\n [ 0.0131455  -0.00515929 -0.02091489  0.00413395  0.02265672]\n [ 0.01042847 -0.00530649 -0.00657599 -0.00184235  0.00265237]\n [ 0.00304775  0.00427735 -0.02462432  0.00852198 -0.00603214]\n [ 0.01527245 -0.01061571 -0.00035126 -0.02249546 -0.00590887]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.00976336  0.00382978  0.00214591  0.0008616  -0.01051646]\n [ 0.01310775 -0.00515748 -0.0208792   0.0041061   0.0225842 ]\n [ 0.0103967  -0.0053062  -0.00657827 -0.00185713  0.00262814]\n [ 0.00303347  0.00425283 -0.02458241  0.00847999 -0.00603632]\n [ 0.01522627 -0.01060355 -0.00036966 -0.02245985 -0.00591393]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00973203  0.0038087   0.00212444  0.00084208 -0.01050674]\n [ 0.01306851 -0.00515801 -0.02084581  0.00407736  0.02251183]\n [ 0.0103643  -0.00530744 -0.00658224 -0.00187239  0.00260444]\n [ 0.00301919  0.00422736 -0.02454186  0.00843785 -0.00603969]\n [ 0.01518087 -0.01059161 -0.00038857 -0.02242447 -0.00591818]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 11] train=0.099139 val=0.100000 loss=114947.668884 time: 27.614028\n0\n\n[[ 0.00970194  0.00378756  0.00210319  0.00082292 -0.0104993 ]\n [ 0.01303159 -0.00516027 -0.02081775  0.00404968  0.0224455 ]\n [ 0.01033365 -0.00530972 -0.0065872  -0.00188781  0.00258211]\n [ 0.00300535  0.00420407 -0.02450647  0.00839868 -0.00604349]\n [ 0.01514028 -0.0105805  -0.00040596 -0.02239322 -0.00592199]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.00967376  0.0037688   0.0020836   0.00080456 -0.01048883]\n [ 0.01299679 -0.00515738 -0.02078204  0.00402272  0.02237498]\n [ 0.0103065  -0.00530645 -0.00658755 -0.00190047  0.00256115]\n [ 0.0029967   0.00418386 -0.02446167  0.00836017 -0.00604365]\n [ 0.01510082 -0.01056348 -0.00042045 -0.02235443 -0.00592303]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.00964158  0.00374545  0.00205793  0.00077976 -0.01048508]\n [ 0.01295885 -0.00515866 -0.02075222  0.00398988  0.02229846]\n [ 0.01027646 -0.00530716 -0.00659352 -0.00191886  0.0025341 ]\n [ 0.00298526  0.00415997 -0.02442229  0.00831607 -0.0060498 ]\n [ 0.01505877 -0.01054998 -0.00043991 -0.0223213  -0.00592969]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00960788  0.0037196   0.00202933  0.00075198 -0.01048273]\n [ 0.01291999 -0.00516228 -0.02072513  0.00395478  0.02222173]\n [ 0.01024602 -0.00530958 -0.00660121 -0.00193804  0.0025082 ]\n [ 0.00297409  0.00413494 -0.0243843   0.00827195 -0.00605399]\n [ 0.01501665 -0.01053832 -0.0004609  -0.02228843 -0.00593438]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 12] train=0.102123 val=0.100000 loss=114943.892944 time: 27.218722\n0\n\n[[ 0.00957995  0.00369891  0.00200627  0.00072862 -0.01048042]\n [ 0.0128877  -0.00516283 -0.02069834  0.00392465  0.02215331]\n [ 0.01022226 -0.00530838 -0.0066056  -0.00195413  0.0024855 ]\n [ 0.00296852  0.00411675 -0.024347    0.00823413 -0.00605687]\n [ 0.01498463 -0.01052193 -0.00047484 -0.02225561 -0.00593643]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.00954581  0.00367088  0.00197382  0.00069556 -0.0104848 ]\n [ 0.01285097 -0.00516632 -0.02067349  0.00388598  0.02207176]\n [ 0.01019629 -0.00530851 -0.0066135  -0.00197551  0.0024558 ]\n [ 0.00296295  0.00409569 -0.02430807  0.00818899 -0.00606388]\n [ 0.01494987 -0.01050435 -0.00049242 -0.02222192 -0.00594127]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.00951858  0.00364769  0.00194462  0.00066477 -0.01048611]\n [ 0.01282054 -0.00516611 -0.02064714  0.0038485   0.02199292]\n [ 0.01017707 -0.00530486 -0.00662011 -0.00199579  0.00242903]\n [ 0.00296436  0.00407874 -0.02426761  0.00814539 -0.00606732]\n [ 0.01492173 -0.01048322 -0.00050854 -0.02218748 -0.00594323]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.0095038   0.0036345   0.0019226   0.00064034 -0.01048109]\n [ 0.01280527 -0.00515436 -0.02061217  0.00381984  0.02192348]\n [ 0.01017448 -0.00528875 -0.00661736 -0.00200677  0.00241165]\n [ 0.00298364  0.00407515 -0.02421772  0.00811078 -0.00606173]\n [ 0.01491127 -0.01044838 -0.00051444 -0.02214371 -0.00593586]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 13] train=0.107752 val=0.153200 loss=114924.866211 time: 24.330214\n0\n\n[[ 0.00948756  0.0036163   0.00189261  0.0006054  -0.01049047]\n [ 0.01279013 -0.0051502  -0.02059196  0.00378054  0.0218481 ]\n [ 0.01017159 -0.00528047 -0.00662562 -0.00202918  0.00238503]\n [ 0.0030008   0.0040664  -0.02418285  0.00806847 -0.00606588]\n [ 0.01490284 -0.01042035 -0.00052757 -0.0221145  -0.00593778]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.0094852   0.00360424  0.00186314  0.00057091 -0.01049422]\n [ 0.01279802 -0.00513141 -0.02056133  0.00374749  0.02178025]\n [ 0.01019997 -0.0052497  -0.00661956 -0.00203762  0.0023772 ]\n [ 0.00305838  0.00408668 -0.02412088  0.00804371 -0.00604453]\n [ 0.01493472 -0.01035639 -0.00051549 -0.02205685 -0.00591121]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.00948164  0.00357498  0.00179874  0.0004949  -0.01053529]\n [ 0.01282747 -0.00511539 -0.02055576  0.00368495  0.0216903 ]\n [ 0.01027043 -0.00520524 -0.00662505 -0.00206429  0.00235754]\n [ 0.00317072  0.0041321  -0.02405995  0.00800918 -0.00602757]\n [ 0.01502597 -0.01026187 -0.00049622 -0.02200068 -0.00588176]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00975247  0.00359203  0.00158214  0.00020233 -0.01073368]\n [ 0.01325771 -0.00500558 -0.02070145  0.00342822  0.02150672]\n [ 0.0108458  -0.0049912  -0.00672446 -0.00223256  0.00231175]\n [ 0.00387671  0.00444286 -0.02399969  0.00791376 -0.00595757]\n [ 0.01576879 -0.0098057  -0.0003585  -0.02188975 -0.00569749]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 14] train=0.147917 val=0.186400 loss=112973.085083 time: 25.297552\n0\n\n[[ 0.00882248  0.00205522 -0.00041012 -0.00189396 -0.01254029]\n [ 0.01283928 -0.00620929 -0.02244291  0.00159145  0.020013  ]\n [ 0.01086085 -0.00582808 -0.00817217 -0.00372562  0.00121673]\n [ 0.00416838  0.00389328 -0.02509754  0.00668744 -0.00675337]\n [ 0.01602826 -0.01021666 -0.00134415 -0.02290944 -0.00639958]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.00588455 -0.00214757 -0.00552784 -0.00709112 -0.01699366]\n [ 0.0109212  -0.00981334 -0.02717792 -0.00317359  0.01614026]\n [ 0.00970448 -0.00879437 -0.01240114 -0.00798331 -0.00209125]\n [ 0.00338323  0.00135592 -0.02886189  0.00275496 -0.00973566]\n [ 0.01494716 -0.0127249  -0.00502371 -0.02668772 -0.00944349]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.00437451 -0.00595586 -0.01092246 -0.01265122 -0.02128429]\n [ 0.01077647 -0.01281757 -0.03208775 -0.00812933  0.01281869]\n [ 0.0104074  -0.01110735 -0.01677126 -0.01235532 -0.00466735]\n [ 0.0043796  -0.00050265 -0.03269072 -0.00128956 -0.01201645]\n [ 0.01522647 -0.0147844  -0.00889406 -0.03074789 -0.01208628]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00537858 -0.00793405 -0.01487366 -0.01675103 -0.02363138]\n [ 0.01403132 -0.01334142 -0.03518088 -0.01136863  0.01175609]\n [ 0.0153535  -0.01026742 -0.01883695 -0.01459976 -0.00461899]\n [ 0.00979911  0.00102589 -0.03392074 -0.00296047 -0.0116043 ]\n [ 0.01959407 -0.01357711 -0.01006907 -0.03234933 -0.01216232]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 15] train=0.180929 val=0.204900 loss=106430.259521 time: 27.357966\n0\n\n[[ 0.0059547  -0.01154181 -0.02173491 -0.02437411 -0.02929227]\n [ 0.01666547 -0.0160367  -0.04199637 -0.01893646  0.00673413]\n [ 0.01941438 -0.01177149 -0.02477278 -0.0214091  -0.00879409]\n [ 0.01441821  0.00052812 -0.03861125 -0.00869045 -0.01500319]\n [ 0.02357398 -0.01365213 -0.01366203 -0.03679487 -0.01494434]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.01397439 -0.00897295 -0.02407555 -0.0290851  -0.03285216]\n [ 0.02809465 -0.01167346 -0.0440681  -0.02354008  0.00405386]\n [ 0.03213556 -0.00623111 -0.02624611 -0.02559085 -0.01091033]\n [ 0.02693076  0.00651782 -0.03920924 -0.01214907 -0.01660533]\n [ 0.0336191  -0.00865219 -0.01386943 -0.03932787 -0.01622983]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.03195356  0.00325791 -0.01792667 -0.02826384 -0.0348638 ]\n [ 0.04687468  0.0004104  -0.03972052 -0.02518485  0.00018126]\n [ 0.04925226  0.00405982 -0.0243899  -0.02999526 -0.01709761]\n [ 0.04199556  0.01515358 -0.03856911 -0.01765888 -0.02391943]\n [ 0.04526179 -0.00232313 -0.01400221 -0.04478079 -0.02433555]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.04512048  0.01053832 -0.01606823 -0.03077583 -0.04040121]\n [ 0.06275499  0.00981265 -0.03786735 -0.02942448 -0.00697003]\n [ 0.06636609  0.014906   -0.0220057  -0.03458824 -0.02465622]\n [ 0.0592614   0.02687722 -0.03481955 -0.02084507 -0.03022344]\n [ 0.05989524  0.00806456 -0.0096931  -0.04621001 -0.02962204]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 16] train=0.199079 val=0.227500 loss=102122.153931 time: 23.463193\n0\n\n[[ 0.05351103  0.0153312  -0.01452452 -0.03263317 -0.04511249]\n [ 0.071201    0.01410992 -0.03827298 -0.03443572 -0.01441048]\n [ 0.0738034   0.01844453 -0.02346988 -0.0410985  -0.03360339]\n [ 0.06551214  0.02986073 -0.03634697 -0.02720273 -0.03916632]\n [ 0.0639321   0.00985953 -0.01107638 -0.0515239  -0.03820307]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.06160527  0.02081962 -0.0112644  -0.03181211 -0.04573591]\n [ 0.07989918  0.01933197 -0.03743174 -0.03732298 -0.01827765]\n [ 0.08367693  0.02484374 -0.02210215 -0.04429903 -0.03798774]\n [ 0.07641827  0.03814536 -0.03251226 -0.02824351 -0.04220117]\n [ 0.07399115  0.01856677 -0.00484923 -0.04901958 -0.0389775 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.06630505  0.02322096 -0.01254372 -0.03588749 -0.05000968]\n [ 0.08499816  0.02170162 -0.04128053 -0.04598819 -0.02637969]\n [ 0.0883491   0.02675642 -0.0269082  -0.05452832 -0.04749613]\n [ 0.07952905  0.03908191 -0.03758401 -0.03842802 -0.05169554]\n [ 0.07351663  0.01693886 -0.01028053 -0.05774727 -0.04766873]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.0834342   0.03835841 -0.00136233 -0.02819554 -0.04341887]\n [ 0.10182809  0.03567822 -0.0332829  -0.04266344 -0.02339987]\n [ 0.10422043  0.0396158  -0.02047069 -0.05302499 -0.04558478]\n [ 0.09397589  0.05067031 -0.03143867 -0.03638331 -0.04910857]\n [ 0.08481915  0.02607308 -0.00429966 -0.05400786 -0.04377646]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 17] train=0.218049 val=0.238000 loss=100087.930573 time: 26.375397\n0\n\n[[ 0.08188842  0.03546716 -0.00481801 -0.03169537 -0.04546999]\n [ 0.10076714  0.03332132 -0.03726199 -0.04765999 -0.02666126]\n [ 0.10437331  0.0389128  -0.02342854 -0.05774177 -0.04868789]\n [ 0.09559118  0.05148578 -0.0324123  -0.03905331 -0.05096933]\n [ 0.08681323  0.02737374 -0.00325356 -0.05362434 -0.04354795]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.08317591  0.0362043  -0.00545136 -0.03440988 -0.04948166]\n [ 0.10222221  0.03405603 -0.03947762 -0.05305691 -0.03268956]\n [ 0.10585354  0.03955315 -0.02643874 -0.0643057  -0.05516051]\n [ 0.09688469  0.05185419 -0.03537338 -0.04549064 -0.05724519]\n [ 0.08708634  0.02680429 -0.00557073 -0.0580623  -0.04845974]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.09626674  0.04907759  0.00548105 -0.0248243  -0.03974922]\n [ 0.11483817  0.04619725 -0.03097149 -0.04744248 -0.02623534]\n [ 0.11761151  0.0508525  -0.01910547 -0.06037178 -0.05010276]\n [ 0.10788074  0.06263537 -0.02774273 -0.04072943 -0.05151996]\n [ 0.09646808  0.03656526  0.00299884 -0.05077496 -0.04092928]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.09241895  0.04475557  0.00032105 -0.0312328  -0.04573444]\n [ 0.11023622  0.04090557 -0.03853394 -0.05722338 -0.03480208]\n [ 0.1130576   0.04583477 -0.02693968 -0.07082635 -0.05922484]\n [ 0.10290388  0.05746156 -0.03495212 -0.05012252 -0.0598871 ]\n [ 0.09028436  0.03072025 -0.00303747 -0.05781952 -0.04772555]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 18] train=0.231030 val=0.249900 loss=98435.217316 time: 24.550408\n0\n\n[[ 0.09711779  0.04939103  0.0028134  -0.02997011 -0.0437778 ]\n [ 0.11548964  0.04574078 -0.03747042 -0.05850492 -0.03491418]\n [ 0.11873019  0.05125384 -0.02562205 -0.07205924 -0.05904968]\n [ 0.10850208  0.06317738 -0.03234673 -0.04968862 -0.05799566]\n [ 0.09538805  0.03663423  0.00188165 -0.05337064 -0.04233886]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.09886362  0.05041125  0.00234812 -0.03119008 -0.04472462]\n [ 0.11709236  0.04747931 -0.03780465 -0.06018193 -0.03563031]\n [ 0.11997648  0.0536849  -0.02489105 -0.07275244 -0.05887874]\n [ 0.10967425  0.06615778 -0.02999024 -0.04847617 -0.05635189]\n [ 0.09713864  0.04039317  0.00637421 -0.04905991 -0.03823377]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.10314406  0.05284124  0.00228019 -0.03189724 -0.04360208]\n [ 0.12124608  0.05052674 -0.0384095  -0.06251062 -0.03563122]\n [ 0.12291651  0.0563044  -0.02575636 -0.07587825 -0.05983536]\n [ 0.11027405  0.06709912 -0.03159778 -0.05226889 -0.05830272]\n [ 0.0950274   0.03929177  0.00410413 -0.05239069 -0.04054044]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.10765348  0.05770164  0.00633929 -0.02802855 -0.03796594]\n [ 0.12487078  0.05509529 -0.03605074 -0.06184442 -0.03273135]\n [ 0.12603149  0.0605575  -0.02416526 -0.07676987 -0.0585818 ]\n [ 0.11320498  0.07141159 -0.02922829 -0.05218527 -0.05645826]\n [ 0.0970346   0.04316407  0.00765059 -0.04962498 -0.0363144 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 19] train=0.259836 val=0.336500 loss=96395.156769 time: 24.698215\n0\n\n[[ 0.11298493  0.06392529  0.01263392 -0.02172302 -0.03035113]\n [ 0.12859362  0.06017292 -0.03161292 -0.05823372 -0.02734439]\n [ 0.12856974  0.06486167 -0.02084417 -0.07460947 -0.05433998]\n [ 0.11523967  0.07557253 -0.02551424 -0.04962243 -0.05201285]\n [ 0.09774717  0.0462518   0.01146966 -0.04578392 -0.03095338]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.11508588  0.0649425   0.0118887  -0.0241723  -0.0323929 ]\n [ 0.1333751   0.0632652  -0.0323601  -0.06230262 -0.03069182]\n [ 0.13635932  0.07096636 -0.01922871 -0.07697068 -0.05612914]\n [ 0.12556577  0.08457521 -0.02037143 -0.04857824 -0.05106422]\n [ 0.10972281  0.05735249  0.02007604 -0.03977406 -0.02565197]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.11751098  0.06673814  0.01410734 -0.02016118 -0.02690399]\n [ 0.13579927  0.06553132 -0.03060471 -0.05991903 -0.02703504]\n [ 0.13938987  0.07428802 -0.0169721  -0.07464337 -0.05262281]\n [ 0.12845273  0.08818036 -0.01746287 -0.04568197 -0.04708446]\n [ 0.11299559  0.06212959  0.0255556  -0.03325808 -0.01844904]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.12370588  0.07220444  0.01792214 -0.01739088 -0.02316071]\n [ 0.14354756  0.07279016 -0.02564654 -0.05689913 -0.02266276]\n [ 0.14770354  0.0825078  -0.01049451 -0.06977522 -0.04640653]\n [ 0.13687256  0.09687216 -0.00984626 -0.03933555 -0.03982641]\n [ 0.121714    0.07125236  0.03415432 -0.0250441  -0.00952577]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 20] train=0.284395 val=0.339900 loss=94149.715851 time: 25.777388\n0\n\n[[ 0.12388397  0.07262458  0.01875069 -0.01591253 -0.02062847]\n [ 0.14165737  0.0721866  -0.0256219  -0.05695497 -0.0214792 ]\n [ 0.14321679  0.07977239 -0.01187131 -0.07103749 -0.04608185]\n [ 0.13125627  0.0930312  -0.01220384 -0.04147255 -0.03976727]\n [ 0.1170416   0.06798851  0.03208867 -0.02633438 -0.0086202 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.12278993  0.07098896  0.01612345 -0.01802396 -0.01964245]\n [ 0.13880947  0.0696241  -0.0291395  -0.06053943 -0.02186052]\n [ 0.13938953  0.07768573 -0.014365   -0.07408235 -0.04632364]\n [ 0.12800027  0.09197505 -0.01331744 -0.04366023 -0.03956078]\n [ 0.11629664  0.06959175  0.03370114 -0.02572159 -0.00678591]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.13590029  0.08381595  0.03080622 -0.00109554  0.00017089]\n [ 0.15184881  0.08349636 -0.01356399 -0.04390421 -0.00300007]\n [ 0.15274605  0.09292702  0.00272191 -0.05670402 -0.02777474]\n [ 0.14161092  0.10781209  0.00481253 -0.02564364 -0.02084412]\n [ 0.13142154  0.08701226  0.05350655 -0.00570949  0.01360604]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.13248113  0.0805174   0.0266858  -0.0058034  -0.00426413]\n [ 0.14752696  0.07997476 -0.01854128 -0.0506201  -0.00980315]\n [ 0.14705049  0.08844716 -0.00275117 -0.06384452 -0.03484312]\n [ 0.13516378  0.10262483 -0.00020817 -0.03165846 -0.02636305]\n [ 0.12674262  0.08339912  0.05101902 -0.00802847  0.0123346 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 21] train=0.303405 val=0.384500 loss=92360.677368 time: 28.825951\n0\n\n[[ 0.14015846  0.08715813  0.03348438  0.00097851  0.00150427]\n [ 0.15338849  0.08519832 -0.01307599 -0.04566812 -0.00608495]\n [ 0.15087742  0.09151442  0.00087341 -0.06055435 -0.03261662]\n [ 0.13829388  0.10481039  0.00257627 -0.02888042 -0.02424976]\n [ 0.13102265  0.08687899  0.05501354 -0.00401621  0.01591459]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.1442297   0.09357949  0.04168025  0.01002662  0.01130533]\n [ 0.15672669  0.09136622 -0.00591179 -0.03920126 -0.00046598]\n [ 0.15287758  0.09703024  0.00771878 -0.0548414  -0.02856496]\n [ 0.14026129  0.11095983  0.01157677 -0.02060901 -0.01835491]\n [ 0.13487126  0.09474193  0.0671187   0.0092917   0.02726698]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.14840536  0.09907939  0.04718528  0.01645375  0.01803339]\n [ 0.16079836  0.09670406 -0.001254   -0.03499141  0.00385798]\n [ 0.15560159  0.10139338  0.0116853  -0.051994   -0.02620801]\n [ 0.14128003  0.11427373  0.01560658 -0.01756678 -0.01629094]\n [ 0.1352199   0.09684397  0.07051442  0.01334865  0.03099351]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 1.4380655e-01  9.5206760e-02  4.4262812e-02  1.4235433e-02\n   1.5649980e-02]\n [ 1.5610667e-01  9.2882544e-02 -4.5938520e-03 -3.8514689e-02\n  -8.0034253e-05]\n [ 1.5044807e-01  9.7298369e-02  8.2868179e-03 -5.6096788e-02\n  -3.1577472e-02]\n [ 1.3530990e-01  1.0949861e-01  1.2239428e-02 -2.1449981e-02\n  -2.2053037e-02]\n [ 1.2887473e-01  9.1823362e-02  6.7592651e-02  1.0992260e-02\n   2.6663022e-02]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 22] train=0.322376 val=0.386900 loss=89731.234100 time: 25.864757\n0\n\n[[ 0.15836976  0.11031024  0.06108966  0.03219986  0.03313405]\n [ 0.16911891  0.10656688  0.01046345 -0.02290974  0.01433386]\n [ 0.16030157  0.10791398  0.02044492 -0.04392139 -0.0206936 ]\n [ 0.14212751  0.11707013  0.02125852 -0.01233281 -0.01369953]\n [ 0.13467374  0.09818142  0.07507059  0.01906407  0.03494525]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.16456583  0.11558409  0.06713302  0.03996537  0.04239142]\n [ 0.17457953  0.11196681  0.01549262 -0.01826043  0.01959837]\n [ 0.16440146  0.11307762  0.02542948 -0.0400105  -0.01731553]\n [ 0.14525037  0.12257135  0.02806488 -0.00666172 -0.00998102]\n [ 0.13753247  0.10378188  0.08375002  0.02808127  0.04160341]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.1657711   0.11782987  0.07160512  0.04594596  0.04997083]\n [ 0.17470975  0.11273059  0.01635794 -0.0182833   0.02050786]\n [ 0.16322461  0.11291886  0.02479846 -0.04274405 -0.02062544]\n [ 0.14126275  0.1207371   0.02710436 -0.00974429 -0.01426666]\n [ 0.13147281  0.10024906  0.08253233  0.02664068  0.03918317]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.16703592  0.11812399  0.07286359  0.04893319  0.05449607]\n [ 0.1769174   0.1150575   0.01953133 -0.01518509  0.02356236]\n [ 0.16434647  0.11651683  0.03043834 -0.03838169 -0.0185025 ]\n [ 0.14081016  0.12462878  0.03418602 -0.00420358 -0.01180703]\n [ 0.13035145  0.10420886  0.09008138  0.03388293  0.04368154]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 23] train=0.334535 val=0.398400 loss=88517.909210 time: 33.223135\n0\n\n[[ 0.1642836   0.11782503  0.07800408  0.05843478  0.06541423]\n [ 0.17452896  0.11477031  0.02331328 -0.00920903  0.02987143]\n [ 0.16065514  0.11481398  0.03260577 -0.03522556 -0.01627288]\n [ 0.13445485  0.12042436  0.03417144 -0.00334673 -0.01170298]\n [ 0.12139989  0.09746446  0.08790923  0.03417534  0.04361469]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.16526082  0.12028679  0.08335102  0.06742656  0.07677572]\n [ 0.17494799  0.11704545  0.02707782 -0.00480718  0.03497373]\n [ 0.1592607   0.11606458  0.03533422 -0.03422985 -0.01632243]\n [ 0.12992391  0.11985534  0.0360864  -0.00304828 -0.01305701]\n [ 0.11291113  0.09383217  0.08855706  0.03497355  0.04290167]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.16162282  0.11674353  0.08344588  0.0703696   0.08036736]\n [ 0.17466418  0.1161457   0.0275401  -0.00498553  0.03332052]\n [ 0.15994681  0.11601978  0.0357538  -0.03663523 -0.02161966]\n [ 0.12856877  0.11884736  0.03630692 -0.00563706 -0.01916055]\n [ 0.10957253  0.09206119  0.08938422  0.03418522  0.03916667]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.16417465  0.12076066  0.09114093  0.08191786  0.09397602]\n [ 0.17893133  0.12110711  0.03380346  0.00234569  0.04199863]\n [ 0.16544224  0.12199539  0.0417008  -0.03158677 -0.01630693]\n [ 0.13273962  0.12419305  0.04255557 -0.00063522 -0.01537366]\n [ 0.11257499  0.09647092  0.09528685  0.04041041  0.04373473]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 24] train=0.352183 val=0.444000 loss=86724.806580 time: 30.749941\n0\n\n[[ 0.16480185  0.12437622  0.09816182  0.09294907  0.10769174]\n [ 0.18097748  0.12572931  0.04102419  0.01159874  0.05313187]\n [ 0.1656441   0.12574562  0.04748423 -0.02511304 -0.00890848]\n [ 0.12926376  0.12529609  0.04694424  0.00372938 -0.01113274]\n [ 0.10570863  0.09495942  0.09861768  0.04495876  0.04776805]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.16041611  0.12098038  0.09793486  0.09702432  0.11330359]\n [ 0.17811798  0.12344556  0.03978026  0.01214978  0.05403564]\n [ 0.16214105  0.1231915   0.04506039 -0.02668568 -0.01016592]\n [ 0.12378273  0.12177116  0.04435442  0.00133822 -0.01333745]\n [ 0.09798653  0.09011976  0.09661617  0.04484301  0.04811972]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.16192576  0.1242791   0.10402871  0.10773442  0.12784769]\n [ 0.18263297  0.12901436  0.04578639  0.02009894  0.06407959]\n [ 0.16592522  0.12907316  0.05095684 -0.02123926 -0.00403797]\n [ 0.12375674  0.12577802  0.05048461  0.00663025 -0.00818216]\n [ 0.09431829  0.09216349  0.10247562  0.05196474  0.05563207]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.15203175  0.11544565  0.09633037  0.10089164  0.12296358]\n [ 0.17648849  0.12434909  0.04066388  0.01303723  0.05683061]\n [ 0.16274065  0.12798056  0.04910186 -0.02662545 -0.01175844]\n [ 0.12147339  0.12604448  0.05046827  0.00271091 -0.01575495]\n [ 0.08997242  0.09133349  0.10254033  0.0496578   0.04959799]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 25] train=0.368169 val=0.465000 loss=85276.041901 time: 28.588507\n0\n\n[[ 0.14425664  0.11146495  0.09694693  0.1048288   0.12871158]\n [ 0.1714619   0.1224883   0.04190331  0.01553272  0.06019089]\n [ 0.15803497  0.12682126  0.05068059 -0.02518056 -0.01041255]\n [ 0.11636233  0.12452305  0.05166353  0.00383448 -0.01503893]\n [ 0.08319651  0.08746585  0.10160506  0.05061971  0.05138724]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.14879005  0.11813213  0.10524195  0.11266071  0.13570201]\n [ 0.18034138  0.13262635  0.0501171   0.02035394  0.0632206 ]\n [ 0.1681513   0.13798013  0.05857534 -0.022076   -0.00913612]\n [ 0.12534533  0.13347815  0.05812361  0.00632374 -0.01386068]\n [ 0.08810008  0.09204465  0.10482343  0.05229991  0.05324355]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.139103    0.11120834  0.10390857  0.11730234  0.14476228]\n [ 0.17140873  0.12608556  0.04631021  0.01906184  0.06553682]\n [ 0.15959473  0.1319175   0.05364061 -0.0274669  -0.01184118]\n [ 0.11540452  0.1257472   0.05193354 -0.00035383 -0.0181815 ]\n [ 0.0740029   0.08139881  0.097895    0.04775397  0.05104929]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.13804449  0.11129968  0.10621889  0.1210205   0.14892071]\n [ 0.17274734  0.12895219  0.04920829  0.02088336  0.06673857]\n [ 0.16224252  0.13601203  0.05619694 -0.02723591 -0.01188893]\n [ 0.1150351   0.12722418  0.05253306 -0.00164589 -0.01836872]\n [ 0.06873969  0.07843515  0.09609522  0.04577512  0.05121687]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 26] train=0.383654 val=0.448700 loss=83502.004333 time: 26.378965\n0\n\n[[ 0.13342616  0.11052428  0.11018552  0.12881477  0.15832889]\n [ 0.17102799  0.13053642  0.05341052  0.0263108   0.0736487 ]\n [ 0.16125265  0.13822183  0.05969939 -0.02381584 -0.00696114]\n [ 0.11111485  0.12725814  0.05494266  0.0014975  -0.01388492]\n [ 0.06065414  0.07502782  0.09709033  0.04975388  0.05728797]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.12377351  0.10336073  0.10674874  0.1293987   0.16311453]\n [ 0.16458622  0.12623139  0.05069277  0.02404918  0.07439371]\n [ 0.15850027  0.13746512  0.05850427 -0.02870178 -0.01035115]\n [ 0.10952931  0.1287369   0.05654797 -0.00190554 -0.01720383]\n [ 0.05893666  0.07719921  0.10082994  0.05071092  0.0586354 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.12533708  0.10848302  0.11595685  0.1396685   0.17364173]\n [ 0.16625422  0.13133034  0.05817453  0.03112928  0.08179651]\n [ 0.1587511   0.14001177  0.06241309 -0.0261575  -0.00644208]\n [ 0.10687822  0.12733065  0.05721534 -0.00179989 -0.01377537]\n [ 0.05097033  0.0704237   0.09745389  0.04901106  0.06185371]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.11946229  0.10278313  0.10943455  0.1332976   0.1692602 ]\n [ 0.16621117  0.13168895  0.05522238  0.02495266  0.07666522]\n [ 0.16102618  0.14348283  0.06146519 -0.032146   -0.01222741]\n [ 0.10791059  0.1306927   0.05744513 -0.00606822 -0.01760568]\n [ 0.04765685  0.07050391  0.09708256  0.04699257  0.06072333]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 27] train=0.394030 val=0.492900 loss=82469.678101 time: 31.751149\n0\n\n[[ 0.10990722  0.09819347  0.11001456  0.13775817  0.17504138]\n [ 0.15815596  0.12786432  0.05402733  0.02511418  0.07862341]\n [ 0.15490024  0.14121479  0.06013154 -0.03404923 -0.01152724]\n [ 0.10220292  0.12928312  0.05753436 -0.00633321 -0.01422833]\n [ 0.0415489   0.06949668  0.09945548  0.05093703  0.06857068]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.09901604  0.09329301  0.10946584  0.13960016  0.1779545 ]\n [ 0.148714    0.12275534  0.05087725  0.02230643  0.0766312 ]\n [ 0.14665839  0.1356115   0.05535157 -0.04037248 -0.01717563]\n [ 0.09541197  0.12461509  0.05371694 -0.01213308 -0.01929338]\n [ 0.03684735  0.06712411  0.09832937  0.0495025   0.06813223]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.09783501  0.09646611  0.11523093  0.14482372  0.1845053 ]\n [ 0.14858565  0.1267008   0.05439732  0.02289057  0.07955366]\n [ 0.14683323  0.14068246  0.05800265 -0.04249495 -0.0163445 ]\n [ 0.09265307  0.12731385  0.05602848 -0.01321631 -0.01709525]\n [ 0.03038697  0.06551811  0.09959438  0.05179422  0.074459  ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.09543586  0.09421828  0.11455017  0.14708392  0.19156097]\n [ 0.14676541  0.12531394  0.05211192  0.02095375  0.08136659]\n [ 0.14241666  0.13846506  0.05420855 -0.04834759 -0.01867136]\n [ 0.08385182  0.12175952  0.05134908 -0.01892639 -0.018409  ]\n [ 0.01733288  0.05678969  0.09473839  0.0495452   0.07755294]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 28] train=0.405769 val=0.499600 loss=81361.143433 time: 25.027156\n0\n\n[[ 0.09342185  0.09366412  0.11673114  0.15227975  0.19849153]\n [ 0.15274063  0.13263199  0.05924268  0.02692798  0.08747906]\n [ 0.1534976   0.15091074  0.06538662 -0.04114386 -0.01200616]\n [ 0.09439357  0.13430607  0.06373283 -0.01053298 -0.01094376]\n [ 0.0231341   0.06567017  0.10566752  0.06047872  0.08857393]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.08328826  0.08807135  0.11601005  0.15455665  0.20346299]\n [ 0.14383274  0.12901254  0.05915234  0.02683388  0.08976907]\n [ 0.14515987  0.14955887  0.06586831 -0.04277052 -0.01161755]\n [ 0.08404252  0.1320406   0.06378156 -0.01197494 -0.00994361]\n [ 0.00907408  0.05988978  0.10357934  0.05994496  0.09025866]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.0800928   0.08904307  0.11892945  0.15832467  0.2094589 ]\n [ 0.14068332  0.13054372  0.06049679  0.02652557  0.09264462]\n [ 0.14106299  0.15098633  0.06605749 -0.04527341 -0.01059842]\n [ 0.07739436  0.13109721  0.06196814 -0.01569096 -0.00876557]\n [-0.00224315  0.05441155  0.09935146  0.05631123  0.09191417]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.07521283  0.08917145  0.12310237  0.16589688  0.21939498]\n [ 0.14005163  0.13408476  0.06537676  0.03115798  0.09895592]\n [ 0.14358175  0.15846701  0.07266589 -0.0414235  -0.00504462]\n [ 0.07974159  0.13920698  0.07001856 -0.010155   -0.00137497]\n [-0.00176069  0.06024387  0.10798267  0.06467284  0.10140923]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 29] train=0.412540 val=0.519000 loss=80358.907013 time: 28.061974\n0\n\n[[ 0.06886229  0.08622672  0.12190183  0.16645709  0.22102094]\n [ 0.1374008   0.1346036   0.06555971  0.02886724  0.09695263]\n [ 0.14367947  0.16119877  0.07350148 -0.04589957 -0.00983156]\n [ 0.07866605  0.13958853  0.06901745 -0.01550145 -0.00654877]\n [-0.00557801  0.05751055  0.10502261  0.05925495  0.09634639]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.06240059  0.08379588  0.12049475  0.1655432   0.2241458 ]\n [ 0.13602307  0.13691486  0.06761923  0.02843161  0.09852821]\n [ 0.14390324  0.16548373  0.07734749 -0.04680417 -0.00980822]\n [ 0.07774459  0.14445445  0.07437763 -0.01452708 -0.00457235]\n [-0.00868863  0.06154331  0.11180683  0.06331437  0.10174296]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.06321359  0.08725722  0.12966396  0.17952159  0.24192162]\n [ 0.13572685  0.13984583  0.07301887  0.03498189  0.10918732]\n [ 0.14133915  0.16738427  0.0806121  -0.04482756 -0.00398087]\n [ 0.07071856  0.14330265  0.07564491 -0.01381227 -0.00085379]\n [-0.02072359  0.05629774  0.1109577   0.06543243  0.10678014]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.04983325  0.07724652  0.1235171   0.17381243  0.23842661]\n [ 0.12577614  0.13172323  0.06561874  0.02475267  0.10147365]\n [ 0.13315438  0.16070224  0.07307909 -0.05736933 -0.01447185]\n [ 0.06043179  0.13384545  0.06633817 -0.02642993 -0.01094972]\n [-0.03474772  0.04207261  0.09842335  0.05256914  0.09690592]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 30] train=0.417608 val=0.523100 loss=79937.609467 time: 27.910246\n0\n\n[[ 0.04609462  0.07743195  0.12455549  0.173562    0.24063571]\n [ 0.12507257  0.13468242  0.06643624  0.02209649  0.10178198]\n [ 0.13437048  0.16512349  0.07373955 -0.06206842 -0.01557841]\n [ 0.06111223  0.13763873  0.06787515 -0.02939093 -0.00976878]\n [-0.03894848  0.04051055  0.09695354  0.04960414  0.09822874]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.04371116  0.07579099  0.12209812  0.17158087  0.24040432]\n [ 0.12615147  0.13726772  0.06563514  0.0180432   0.09791959]\n [ 0.13659438  0.17071845  0.07393393 -0.06806547 -0.02161721]\n [ 0.06085754  0.14325579  0.06962705 -0.03367861 -0.01437787]\n [-0.0442056   0.04188681  0.09820383  0.0478496   0.09592552]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.04427714  0.07870103  0.12675531  0.1796508   0.25257525]\n [ 0.12894854  0.14167517  0.06899282  0.02079464  0.10385015]\n [ 0.14215346  0.17762367  0.07799772 -0.06801546 -0.0192951 ]\n [ 0.0686418   0.15312317  0.07768697 -0.03000612 -0.00940067]\n [-0.03711164  0.05257418  0.11019029  0.05844195  0.10708667]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.04162533  0.07814157  0.1293189   0.18425675  0.25634518]\n [ 0.1293435   0.14500609  0.07349154  0.02544563  0.10784518]\n [ 0.1453088   0.18469492  0.08517778 -0.06450876 -0.0167715 ]\n [ 0.07063539  0.15894845  0.08430054 -0.02855986 -0.00929435]\n [-0.03884415  0.05456357  0.11497726  0.05982308  0.10622063]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 31] train=0.423438 val=0.532000 loss=78962.419922 time: 24.564440\n0\n\n[[ 0.03413742  0.07484135  0.12809677  0.18263541  0.2571442 ]\n [ 0.11781151  0.13906829  0.07043994  0.02240768  0.1068795 ]\n [ 0.13412319  0.1785406   0.0811203  -0.06928282 -0.01844252]\n [ 0.05819733  0.15004106  0.07724849 -0.0355861  -0.01139954]\n [-0.05320203  0.04170686  0.10523308  0.05246584  0.10440701]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.03342278  0.07787064  0.13354006  0.19197309  0.27293614]\n [ 0.11989279  0.14499517  0.07600773  0.02754396  0.11797836]\n [ 0.13940024  0.18666418  0.08591554 -0.06707344 -0.01068704]\n [ 0.06248571  0.15676744  0.08119433 -0.03365289 -0.0044589 ]\n [-0.05321084  0.04413071  0.10745396  0.05474219  0.11060234]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.03119944  0.07524412  0.13313378  0.1951939   0.2778726 ]\n [ 0.11825339  0.14395264  0.07560728  0.02830602  0.12095115]\n [ 0.1376946   0.18593486  0.08468175 -0.0682914  -0.00875988]\n [ 0.05876907  0.15361601  0.07884802 -0.03458343 -0.00175948]\n [-0.05977907  0.03774596  0.10218465  0.05214301  0.1121573 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.02463434  0.06732114  0.12668182  0.19213396  0.27625537]\n [ 0.11158843  0.13628636  0.06689253  0.02099783  0.11510179]\n [ 0.13191953  0.18104917  0.07715945 -0.07639433 -0.01591283]\n [ 0.05354765  0.14961506  0.07395156 -0.04038769 -0.00707139]\n [-0.06546146  0.03394556  0.10066979  0.05209584  0.11274086]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 32] train=0.434235 val=0.523400 loss=77739.381927 time: 32.851609\n0\n\n[[ 0.01834561  0.06267051  0.12355368  0.19225243  0.2795449 ]\n [ 0.11034178  0.13815351  0.06765933  0.02039204  0.11679745]\n [ 0.134506    0.18806918  0.08120482 -0.0778098  -0.01627751]\n [ 0.05554276  0.15748334  0.07907663 -0.04081619 -0.00621405]\n [-0.06721967  0.03899403  0.10622291  0.055701    0.11749595]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.02335748  0.06775565  0.12872283  0.19853866  0.2861441 ]\n [ 0.11099765  0.14169978  0.07020168  0.02320645  0.12102315]\n [ 0.13022457  0.18855053  0.08042808 -0.0797792  -0.01627551]\n [ 0.04579885  0.1523347   0.0749798  -0.04443746 -0.00784732]\n [-0.0840499   0.0259752   0.09746061  0.05068473  0.11538346]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.02351055  0.0683158   0.12950154  0.20056976  0.2952171 ]\n [ 0.1128425   0.14575778  0.0712423   0.0216941   0.12659205]\n [ 0.13360262  0.19559479  0.08192886 -0.08436461 -0.01434394]\n [ 0.04875102  0.16028222  0.07834402 -0.04712421 -0.00496137]\n [-0.08216698  0.0316201   0.10043555  0.05170767  0.12213513]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.01325608  0.06100719  0.12207568  0.19521241  0.29211697]\n [ 0.10966406  0.14529616  0.06700384  0.01581765  0.12347058]\n [ 0.13484459  0.19999748  0.08042499 -0.09066488 -0.01748777]\n [ 0.0504492   0.16499916  0.07914346 -0.05160452 -0.0068683 ]\n [-0.08090263  0.03501133  0.10259365  0.05092015  0.12175596]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 33] train=0.437961 val=0.529000 loss=77640.286987 time: 27.546240\n0\n\n[[ 0.00719975  0.06089003  0.12709276  0.2029756   0.2982053 ]\n [ 0.10404675  0.14600137  0.07136813  0.02138795  0.12675866]\n [ 0.12686507  0.1982392   0.08139213 -0.08957238 -0.01819489]\n [ 0.03947927  0.15893872  0.07624571 -0.05288569 -0.00814637]\n [-0.09666072  0.0231853   0.09499469  0.04674635  0.11788239]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[ 0.009863    0.06449678  0.13183339  0.20585708  0.3007345 ]\n [ 0.1071522   0.14990023  0.07455491  0.02238958  0.1289045 ]\n [ 0.12976319  0.2036439   0.08568015 -0.08948065 -0.01696837]\n [ 0.04124126  0.16381222  0.08253703 -0.04985033 -0.00292623]\n [-0.09855373  0.02439973  0.1003076   0.05232549  0.12644789]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[ 0.00410891  0.05980789  0.12951759  0.20512122  0.30711982]\n [ 0.10180583  0.14626776  0.07055515  0.01600526  0.12861823]\n [ 0.12564312  0.2033587   0.0835324  -0.09790093 -0.02068026]\n [ 0.03419057  0.16127421  0.07963745 -0.05798999 -0.00621744]\n [-0.11102366  0.0162967   0.09414408  0.04412528  0.12194926]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[ 0.00115926  0.05962539  0.12942493  0.2055534   0.31045645]\n [ 0.10079712  0.14856939  0.07153721  0.01549392  0.13047682]\n [ 0.12747511  0.20972614  0.08661132 -0.09911418 -0.01856158]\n [ 0.03466795  0.16728397  0.08330766 -0.05890412 -0.00350541]\n [-0.11649673  0.01717474  0.0959354   0.04374125  0.12413207]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 34] train=0.443850 val=0.539300 loss=76726.417023 time: 24.022035\n0\n\n[[-0.00264783  0.05851762  0.12889092  0.20488288  0.3100445 ]\n [ 0.09644517  0.14919549  0.07174207  0.01330411  0.1288924 ]\n [ 0.12482744  0.21367754  0.08883673 -0.10171904 -0.02048994]\n [ 0.03008731  0.17020884  0.08707006 -0.05951976 -0.00416368]\n [-0.1251069   0.01665031  0.09907377  0.0455995   0.12617604]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.00485347  0.05732434  0.12772295  0.20256858  0.30638373]\n [ 0.09560434  0.15012342  0.07038048  0.00915358  0.12340799]\n [ 0.1253514   0.21706267  0.08874495 -0.10710618 -0.02646984]\n [ 0.02984714  0.17197207  0.08685718 -0.06414069 -0.00820904]\n [-0.13023046  0.01291243  0.09557918  0.04051174  0.12228411]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.00233488  0.06578678  0.14182135  0.21729028  0.31663537]\n [ 0.09605329  0.1557173   0.08012331  0.01897111  0.13057798]\n [ 0.12287852  0.22025977  0.09496611 -0.10188226 -0.02405599]\n [ 0.02368918  0.17249687  0.09322491 -0.05801786 -0.00463259]\n [-0.13975367  0.00968038  0.09878822  0.04628234  0.12831916]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.00527251  0.06577422  0.14123905  0.21646781  0.3211449 ]\n [ 0.08961985  0.15134737  0.0744031   0.01121319  0.12868711]\n [ 0.11388298  0.21429475  0.08885403 -0.11119291 -0.02837613]\n [ 0.01068689  0.16377977  0.08798645 -0.06538874 -0.0075923 ]\n [-0.15580943 -0.00271459  0.09300204  0.04162572  0.1281901 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 35] train=0.448217 val=0.547000 loss=76239.528503 time: 23.381826\n0\n\n[[-5.8145146e-05  7.0049196e-02  1.4391369e-01  2.1881407e-01\n   3.2317269e-01]\n [ 9.8059580e-02  1.5882266e-01  7.9193257e-02  1.4022707e-02\n   1.3075432e-01]\n [ 1.2615131e-01  2.2580844e-01  9.7092979e-02 -1.0590924e-01\n  -2.4488864e-02]\n [ 2.3985161e-02  1.7640781e-01  9.7921371e-02 -5.7989474e-02\n  -9.5122075e-04]\n [-1.4388318e-01  7.6181716e-03  1.0107134e-01  4.8070457e-02\n   1.3416687e-01]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.01129126  0.06069724  0.13818277  0.21759504  0.32671365]\n [ 0.09091741  0.15392411  0.07464937  0.0106386   0.13141283]\n [ 0.11974039  0.22242732  0.09271806 -0.11286449 -0.02850138]\n [ 0.01794858  0.17347547  0.09495781 -0.06418512 -0.00435778]\n [-0.14653033  0.00765953  0.10277805  0.04753881  0.13462207]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.0173216   0.05712201  0.13919471  0.2246651   0.3403802 ]\n [ 0.08678398  0.15271063  0.07723022  0.01815499  0.1456912 ]\n [ 0.11759017  0.22413966  0.09558146 -0.11002972 -0.02069791]\n [ 0.01704298  0.17618261  0.09939228 -0.0613209   0.00217405]\n [-0.14958706  0.00760568  0.10635808  0.05094931  0.13984913]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0151004   0.05880558  0.13883999  0.22611342  0.33998102]\n [ 0.08692957  0.15531583  0.07853406  0.01995294  0.1476852 ]\n [ 0.11913762  0.2288306   0.09730212 -0.11019157 -0.01871994]\n [ 0.01996895  0.1801129   0.10011122 -0.06212594  0.00454203]\n [-0.14991446  0.00793062  0.10554044  0.05085102  0.1411899 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 36] train=0.454467 val=0.545100 loss=75822.476929 time: 25.188837\n0\n\n[[-0.01383164  0.06199025  0.14215297  0.22618198  0.33487555]\n [ 0.08399475  0.15549052  0.07896259  0.01585112  0.13839246]\n [ 0.1158658   0.22943152  0.09723336 -0.11510611 -0.02784652]\n [ 0.01418802  0.17748572  0.09761762 -0.06730887 -0.00371975]\n [-0.16091585 -0.00117982  0.097706    0.04345709  0.13226993]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-2.24704761e-02  5.48724495e-02  1.38287127e-01  2.28406116e-01\n   3.39218467e-01]\n [ 7.56904855e-02  1.50133446e-01  7.33074844e-02  1.47438515e-02\n   1.41611010e-01]\n [ 1.08510397e-01  2.27360785e-01  9.24184322e-02 -1.17815793e-01\n  -2.53152791e-02]\n [ 6.70509646e-03  1.75585464e-01  9.45877805e-02 -6.85892925e-02\n   1.35615817e-04]\n [-1.68565601e-01 -4.26908955e-03  9.74291638e-02  4.70522754e-02\n   1.40580907e-01]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-2.6507977e-02  4.6909764e-02  1.3473007e-01  2.3137377e-01\n   3.4341902e-01]\n [ 7.0215195e-02  1.4338629e-01  6.9251105e-02  1.5456348e-02\n   1.4471725e-01]\n [ 1.0374229e-01  2.2408137e-01  9.0592526e-02 -1.1720793e-01\n  -2.2160860e-02]\n [-1.1015730e-04  1.7078502e-01  9.3140192e-02 -6.6010244e-02\n   4.5834873e-03]\n [-1.7963633e-01 -1.4965605e-02  9.3652561e-02  5.0628606e-02\n   1.4547311e-01]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.02148254  0.04817638  0.13423051  0.23163043  0.34831348]\n [ 0.07352026  0.14454046  0.0675093   0.01232294  0.14711127]\n [ 0.1068873   0.22657247  0.08982393 -0.12139091 -0.02116376]\n [ 0.00311887  0.171599    0.09194024 -0.06817681  0.00882895]\n [-0.1766029  -0.01613519  0.09086631  0.04924629  0.15161905]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 37] train=0.455188 val=0.555100 loss=75418.547638 time: 25.266918\n0\n\n[[-0.02360928  0.04867246  0.13495968  0.23429094  0.3521496 ]\n [ 0.07574678  0.14820887  0.06843321  0.01228059  0.14750576]\n [ 0.11184154  0.23373356  0.09227055 -0.12245897 -0.02196126]\n [ 0.00882461  0.17922789  0.09712175 -0.06681488  0.00992584]\n [-0.17354801 -0.01240564  0.09534806  0.05156019  0.15333912]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0282063   0.04550198  0.13358732  0.23647426  0.35140908]\n [ 0.07329283  0.14729707  0.06639808  0.0117975   0.14423148]\n [ 0.11134686  0.23465215  0.09041197 -0.12516843 -0.02644021]\n [ 0.00659504  0.17807457  0.09569086 -0.06917281  0.00678356]\n [-0.17854962 -0.01524805  0.09590288  0.05280896  0.15269588]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.03293484  0.04034054  0.13053258  0.2376017   0.3540703 ]\n [ 0.07004445  0.14564459  0.06471158  0.01298485  0.14806224]\n [ 0.11042569  0.23607592  0.08932339 -0.12472258 -0.02314883]\n [ 0.00814718  0.17977493  0.09484778 -0.06766627  0.01036774]\n [-0.17557535 -0.01419075  0.09581406  0.05589962  0.15561134]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.03097149  0.04093589  0.13266367  0.24127708  0.35962155]\n [ 0.07416673  0.14946838  0.06660084  0.0140425   0.15232678]\n [ 0.11660538  0.24146488  0.08915091 -0.12870814 -0.02304391]\n [ 0.0119853   0.18089998  0.09209471 -0.07275841  0.009806  ]\n [-0.17626952 -0.01847881  0.09193908  0.05279473  0.15584074]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 38] train=0.460497 val=0.549600 loss=74795.510223 time: 25.743415\n0\n\n[[-0.03500033  0.04135729  0.13678232  0.24387878  0.3581202 ]\n [ 0.06881393  0.14917341  0.06930929  0.01474967  0.15036558]\n [ 0.11055379  0.24123202  0.09006879 -0.13010813 -0.02411244]\n [ 0.00250415  0.17605372  0.08837919 -0.07668953  0.00862295]\n [-0.19159892 -0.02925321  0.0839408   0.04646249  0.15326937]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.02945614  0.04577865  0.14193936  0.24925098  0.35867596]\n [ 0.07071786  0.15344237  0.07430103  0.01926711  0.15033531]\n [ 0.11513146  0.24893884  0.09678882 -0.126466   -0.02493805]\n [ 0.0094988   0.18555571  0.09771784 -0.0693153   0.01211708]\n [-0.18633057 -0.02328456  0.09268115  0.05639976  0.16088478]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.03982881  0.03980235  0.13998595  0.25051722  0.35938978]\n [ 0.06062144  0.14890218  0.07123338  0.01819872  0.14878663]\n [ 0.10614092  0.24739717  0.0940813  -0.1306805  -0.02805591]\n [ 0.00115811  0.18407585  0.09507589 -0.0734754   0.00883278]\n [-0.19748572 -0.02920655  0.08856872  0.05306941  0.1570585 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.03690042  0.04602063  0.15098071  0.26628947  0.3753358 ]\n [ 0.06272496  0.15524304  0.08040899  0.02889311  0.1589776 ]\n [ 0.10628003  0.25315017  0.10048494 -0.12587145 -0.02336435]\n [-0.00292655  0.18542634  0.09890249 -0.07196852  0.00904346]\n [-0.20321934 -0.0315225   0.08982426  0.05223389  0.15382794]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 39] train=0.464103 val=0.580300 loss=74328.012146 time: 24.496947\n0\n\n[[-0.03784586  0.04304161  0.14911659  0.2638607   0.37281194]\n [ 0.05950478  0.15080623  0.07727041  0.02347856  0.15334325]\n [ 0.10192834  0.24801415  0.09514712 -0.13399525 -0.03086441]\n [-0.0061785   0.18113676  0.09520966 -0.07841726  0.00270552]\n [-0.20283902 -0.0330704   0.08847311  0.04864042  0.14886774]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.04004443  0.04143764  0.14985088  0.26347393  0.37131304]\n [ 0.05766439  0.14945878  0.07633507  0.01964627  0.15151605]\n [ 0.10316531  0.25029477  0.09547928 -0.13821949 -0.03040521]\n [-0.00224284  0.18717328  0.09990719 -0.07756939  0.00803018]\n [-0.1956325  -0.02336511  0.09937926  0.05781627  0.15990289]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04514347  0.04250398  0.15458381  0.26522446  0.36902568]\n [ 0.05093083  0.1501935   0.07973149  0.01979938  0.14907098]\n [ 0.0953235   0.24799383  0.09387296 -0.14255759 -0.03485616]\n [-0.00910514  0.18271501  0.09599431 -0.08246884  0.00507713]\n [-0.1980611  -0.02646647  0.09698977  0.05436143  0.15707381]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.04104136  0.04379162  0.15482159  0.26709864  0.37284353]\n [ 0.05333947  0.151198    0.08099384  0.02145328  0.15250425]\n [ 0.09600981  0.24976578  0.09530354 -0.1423685  -0.03159548]\n [-0.00651005  0.18598868  0.10050508 -0.0789932   0.01102705]\n [-0.19396767 -0.02176839  0.10553005  0.06391998  0.16711779]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 40] train=0.466306 val=0.565800 loss=74197.606567 time: 24.165333\n0\n\n[[-0.05056684  0.03325205  0.14874569  0.26288515  0.36748716]\n [ 0.04396814  0.14175776  0.0753649   0.01743147  0.14727157]\n [ 0.0856431   0.24090466  0.08768566 -0.15116987 -0.03996542]\n [-0.02174155  0.17308249  0.09044445 -0.08777119  0.0045437 ]\n [-0.21166298 -0.03733361  0.09450125  0.05836749  0.16380647]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.05344748  0.03232238  0.1503104   0.26357517  0.36391127]\n [ 0.04536385  0.14682932  0.08007076  0.01747766  0.14241023]\n [ 0.08993451  0.2500988   0.09346119 -0.152646   -0.04575083]\n [-0.01929091  0.18071602  0.09608041 -0.08713119  0.00181381]\n [-0.21482602 -0.03537789  0.09730612  0.06073039  0.16249713]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04626763  0.04143011  0.1593972   0.27584735  0.38032445]\n [ 0.05095619  0.15579736  0.0878489   0.02560299  0.15455087]\n [ 0.09676494  0.26181325  0.10139805 -0.14789926 -0.03829598]\n [-0.01677855  0.18783793  0.09974787 -0.0855547   0.00591324]\n [-0.21780409 -0.03594178  0.09516604  0.06011677  0.16458173]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.04686697  0.03946505  0.15748279  0.27376434  0.37457535]\n [ 0.05324651  0.15680775  0.08810186  0.02450669  0.14987765]\n [ 0.10004549  0.26603904  0.10453481 -0.14743559 -0.04163264]\n [-0.01827461  0.18785971  0.09921099 -0.08717991  0.00146751]\n [-0.22574711 -0.0433494   0.08905809  0.0570839   0.15960556]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 41] train=0.470633 val=0.587700 loss=73615.817200 time: 22.943855\n0\n\n[[-0.05298117  0.03275084  0.14907707  0.26644462  0.36953062]\n [ 0.04611975  0.14956985  0.07976674  0.01716869  0.14726363]\n [ 0.09204097  0.2584362   0.09659769 -0.15421836 -0.04250471]\n [-0.02645739  0.17919679  0.09088086 -0.09324183  0.00147936]\n [-0.23351432 -0.05324594  0.08061168  0.05301993  0.16106632]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.05275725  0.03214264  0.14826955  0.26895165  0.37732977]\n [ 0.04787403  0.15141279  0.08087359  0.02108721  0.15646724]\n [ 0.09255029  0.26232773  0.10043685 -0.14959434 -0.032927  ]\n [-0.02809469  0.1827806   0.09563144 -0.08852706  0.01098431]\n [-0.23660575 -0.05146114  0.08334114  0.05537599  0.16792895]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04971464  0.03367216  0.14913172  0.27000204  0.37605163]\n [ 0.05200982  0.15510282  0.08058845  0.01890313  0.15383506]\n [ 0.10023887  0.26997945  0.10139821 -0.15292698 -0.03633587]\n [-0.01713449  0.19231692  0.09987786 -0.0868207   0.0114416 ]\n [-0.22700113 -0.04359706  0.08824261  0.06056736  0.17149541]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.04869786  0.03247315  0.1503135   0.27260312  0.3779505 ]\n [ 0.05091714  0.15208875  0.0796036   0.01918956  0.1561053 ]\n [ 0.09588793  0.26618057  0.09981873 -0.15531993 -0.03596466]\n [-0.02503031  0.18674994  0.09892909 -0.08634644  0.01270495]\n [-0.2358842  -0.05118505  0.08787844  0.06479333  0.1739335 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 42] train=0.469631 val=0.582500 loss=73559.136383 time: 23.510608\n0\n\n[[-0.04685852  0.03046115  0.14869672  0.2733064   0.38169187]\n [ 0.04830888  0.14692542  0.07369208  0.01517414  0.159529  ]\n [ 0.09025282  0.26020336  0.09357498 -0.15917571 -0.0311069 ]\n [-0.03468188  0.1772243   0.09232925 -0.08871512  0.01731225]\n [-0.248597   -0.06522673  0.07872979  0.06228654  0.1763004 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.04519288  0.03360155  0.15327847  0.2785641   0.3846168 ]\n [ 0.05037399  0.15245776  0.07871623  0.01837124  0.16079527]\n [ 0.09249505  0.26669076  0.09819002 -0.15887073 -0.03167851]\n [-0.03613417  0.1811255   0.097211   -0.08806419  0.0159406 ]\n [-0.25836623 -0.07064293  0.0776608   0.06042116  0.17144527]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04156051  0.03635212  0.15760715  0.28436512  0.39007306]\n [ 0.05339853  0.15592822  0.08136261  0.02177741  0.1636598 ]\n [ 0.09608606  0.272224    0.10190146 -0.15468772 -0.02991919]\n [-0.03523163  0.18292785  0.09998625 -0.08386091  0.01566434]\n [-0.26018277 -0.07200792  0.0787119   0.0623461   0.16734444]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05285821  0.02763577  0.14880747  0.27470586  0.3862878 ]\n [ 0.0449196   0.15231329  0.07591373  0.01178716  0.15939572]\n [ 0.0903392   0.27175027  0.10005497 -0.16346705 -0.03403208]\n [-0.03907986  0.18368435  0.10182459 -0.08688468  0.0156604 ]\n [-0.2625401  -0.07077854  0.08251109  0.06466601  0.17234023]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 43] train=0.476102 val=0.589700 loss=72602.789642 time: 28.258581\n0\n\n[[-0.04904031  0.02866832  0.14992826  0.28122562  0.39003372]\n [ 0.05028577  0.15599135  0.07849032  0.01658671  0.1608343 ]\n [ 0.09581512  0.27686334  0.10182924 -0.16319165 -0.03639131]\n [-0.0340994   0.1877656   0.10274715 -0.08789783  0.01257017]\n [-0.26104382 -0.07067084  0.08123387  0.06213356  0.16645469]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0512425   0.0291161   0.15307595  0.28767037  0.39927983]\n [ 0.04940765  0.15759653  0.08314463  0.0233273   0.17010652]\n [ 0.0939982   0.2810866   0.10792679 -0.1579667  -0.02841888]\n [-0.03766353  0.19132136  0.10762397 -0.08410147  0.01780198]\n [-0.2680221  -0.07258214  0.08300474  0.06546798  0.17093211]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.05012012  0.02454931  0.14684422  0.2842881   0.39930138]\n [ 0.04826684  0.15138964  0.07378669  0.01584967  0.16759783]\n [ 0.09489165  0.27757075  0.0991125  -0.16808257 -0.03367883]\n [-0.03539218  0.18838097  0.10046282 -0.09432542  0.00834831]\n [-0.26681212 -0.07607648  0.07853413  0.05851601  0.16002789]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.04952846  0.02184449  0.14236088  0.28061482  0.39359334]\n [ 0.05080894  0.15402156  0.07389217  0.01399173  0.16470237]\n [ 0.09887698  0.2840596   0.10330969 -0.16673262 -0.03182392]\n [-0.03212576  0.19299233  0.10397121 -0.09091919  0.0140326 ]\n [-0.26437253 -0.07363213  0.08026977  0.06205206  0.16406035]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 44] train=0.472055 val=0.584400 loss=73239.629578 time: 26.222741\n0\n\n[[-0.04605719  0.02493589  0.14822958  0.28862467  0.40102327]\n [ 0.05148386  0.15599538  0.07647473  0.01689699  0.16854762]\n [ 0.09848034  0.2865276   0.1048869  -0.16768146 -0.03085618]\n [-0.03948955  0.19095917  0.1038347  -0.09233096  0.01505505]\n [-0.28192523 -0.08465099  0.07300974  0.05672299  0.16140936]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.04899038  0.02090996  0.14334112  0.28504723  0.39416543]\n [ 0.04682556  0.15040839  0.06899356  0.01003422  0.1598895 ]\n [ 0.09408733  0.28271192  0.09829336 -0.17604065 -0.03948682]\n [-0.04502976  0.18727645  0.10042938 -0.09837566  0.00858293]\n [-0.2872916  -0.08880367  0.06995098  0.05297924  0.15717024]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04727322  0.02417292  0.14768104  0.29385287  0.40889567]\n [ 0.05040899  0.15709925  0.07584688  0.01850463  0.17043337]\n [ 0.1005164   0.29434675  0.10860124 -0.16785228 -0.03166261]\n [-0.03772676  0.19963065  0.1122155  -0.08911759  0.01646984]\n [-0.2789294  -0.07823332  0.08180137  0.06345557  0.16535491]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05681175  0.01867189  0.14686675  0.29588738  0.4101739 ]\n [ 0.03907279  0.1506138   0.07421771  0.0183835   0.17042907]\n [ 0.09179156  0.2903258   0.10763661 -0.17016608 -0.03233859]\n [-0.04356676  0.19765775  0.11372894 -0.09016855  0.01680126]\n [-0.28322184 -0.08148424  0.08279029  0.06374306  0.16562042]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 45] train=0.479307 val=0.579200 loss=72374.306427 time: 23.747965\n0\n\n[[-0.05310389  0.01939138  0.14927603  0.2965613   0.41021237]\n [ 0.0392993   0.15112363  0.07708298  0.01825852  0.17065433]\n [ 0.09327415  0.29444182  0.11193733 -0.17145789 -0.03397058]\n [-0.04184331  0.20037206  0.1143838  -0.09417151  0.01378141]\n [-0.28257397 -0.08436634  0.07616989  0.05498791  0.15880883]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0526517   0.01594469  0.14835723  0.2989488   0.4128515 ]\n [ 0.04013806  0.14930977  0.07474871  0.01543682  0.16687909]\n [ 0.09775165  0.29881597  0.1122634  -0.17628269 -0.04101212]\n [-0.03259032  0.21018656  0.12029083 -0.09547778  0.00943   ]\n [-0.2710663  -0.07313845  0.08641482  0.05850287  0.1588473 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.05657272  0.01104378  0.14477165  0.2963528   0.40610647]\n [ 0.03640166  0.14610475  0.0726136   0.01363964  0.16323176]\n [ 0.09206305  0.2952865   0.1088309  -0.18074057 -0.04440353]\n [-0.0404305   0.20464277  0.11564458 -0.10164694  0.00562676]\n [-0.27810436 -0.07805747  0.0828855   0.05365147  0.15395333]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05407225  0.01427533  0.1521015   0.3083428   0.41851243]\n [ 0.03543307  0.14743091  0.07762064  0.02314539  0.17571583]\n [ 0.0907325   0.29539257  0.11034434 -0.1780724  -0.03607858]\n [-0.0441764   0.20205066  0.11533173 -0.10202525  0.01155357]\n [-0.2833557  -0.0830437   0.08202271  0.05388339  0.15998247]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 46] train=0.481991 val=0.591300 loss=71958.390045 time: 23.221220\n0\n\n[[-0.05756623  0.01089244  0.15202527  0.31121656  0.42113438]\n [ 0.02940892  0.14260574  0.07673208  0.02604548  0.17827705]\n [ 0.08699576  0.29417056  0.11174378 -0.1743933  -0.03203269]\n [-0.04913712  0.20018007  0.11713877 -0.09665227  0.01899912]\n [-0.2871918  -0.08592214  0.08316096  0.06160208  0.1703865 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.04873153  0.01590578  0.15226077  0.310144    0.41919658]\n [ 0.04000767  0.14956222  0.07844377  0.02575393  0.17742322]\n [ 0.09799691  0.304249    0.11615152 -0.17243302 -0.03065803]\n [-0.04098918  0.20823358  0.12117075 -0.0946266   0.02138193]\n [-0.28108084 -0.08001605  0.08595949  0.06475265  0.1743345 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.05121526  0.01358997  0.1468654   0.30571508  0.41482055]\n [ 0.03820223  0.14719483  0.07061478  0.01591177  0.16973144]\n [ 0.09843653  0.30554906  0.11133792 -0.1819024  -0.03796501]\n [-0.03909061  0.2108679   0.12081446 -0.09928057  0.01843856]\n [-0.27917448 -0.07838729  0.0846932   0.06120934  0.17176875]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05435885  0.01377614  0.14822271  0.3072586   0.41295412]\n [ 0.03286997  0.14342251  0.06468584  0.00947072  0.1615909 ]\n [ 0.09043428  0.30054197  0.10097792 -0.19509901 -0.05085558]\n [-0.04914576  0.20313911  0.10844789 -0.11372734  0.00540781]\n [-0.29055095 -0.08775263  0.07273391  0.04919937  0.16172561]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 47] train=0.484776 val=0.602200 loss=71585.254333 time: 23.819896\n0\n\n[[-0.05814127  0.01273088  0.14972816  0.3099437   0.4131485 ]\n [ 0.03238363  0.14649285  0.06915689  0.01508509  0.16702771]\n [ 0.09109252  0.30456445  0.10645015 -0.18794078 -0.04219791]\n [-0.04954492  0.20645237  0.11327466 -0.10526091  0.01685344]\n [-0.2934041  -0.08733463  0.07719304  0.05953826  0.17474988]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.05052208  0.01550359  0.15111084  0.31169766  0.41710976]\n [ 0.03734218  0.14619644  0.06903794  0.01769696  0.17234287]\n [ 0.09403035  0.30401045  0.10560287 -0.18724918 -0.03839958]\n [-0.04954544  0.20408668  0.11034071 -0.10761756  0.0183718 ]\n [-0.297057   -0.09410103  0.07117759  0.05631138  0.17435133]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.05517316  0.00661996  0.14225915  0.30977023  0.41809675]\n [ 0.03305639  0.13772485  0.06052647  0.01535574  0.17268914]\n [ 0.0904647   0.2980837   0.10002661 -0.18864927 -0.0372083 ]\n [-0.05270163  0.20085149  0.10969853 -0.10459705  0.02253145]\n [-0.30009204 -0.0954188   0.07403502  0.06221058  0.17897886]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05500399  0.00768444  0.14659189  0.31861788  0.4291335 ]\n [ 0.03522318  0.14123632  0.06648393  0.0237902   0.18369493]\n [ 0.0913904   0.30212516  0.10710652 -0.18120496 -0.02851993]\n [-0.05553988  0.20187753  0.11618348 -0.09629129  0.03075701]\n [-0.3027267  -0.09576145  0.08059191  0.07367887  0.18913323]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 48] train=0.487720 val=0.593900 loss=71176.047424 time: 22.847379\n0\n\n[[-0.05849494  0.00489769  0.14421947  0.31685317  0.426204  ]\n [ 0.03162777  0.13983004  0.06303638  0.01878155  0.17883088]\n [ 0.084731    0.29909483  0.10230266 -0.18851446 -0.03558393]\n [-0.06443559  0.19586055  0.10993733 -0.10362177  0.02299418]\n [-0.31350955 -0.10590162  0.07086876  0.06348058  0.17763904]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.05396373  0.0080173   0.14934655  0.32726964  0.43683994]\n [ 0.0390898   0.14317118  0.06595999  0.02591518  0.18787113]\n [ 0.09211731  0.30228853  0.1044781  -0.18474668 -0.02846398]\n [-0.05874119  0.19893032  0.11261511 -0.0999447   0.02843347]\n [-0.30537722 -0.10059971  0.07506458  0.06859156  0.18352464]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04812145  0.0079332   0.1444029   0.3209767   0.43009645]\n [ 0.04559988  0.14634176  0.06362058  0.02232659  0.18501993]\n [ 0.09393886  0.30491537  0.10284252 -0.18845412 -0.03021551]\n [-0.06074752  0.19900198  0.11100551 -0.10312149  0.02549804]\n [-0.30986983 -0.10239087  0.07313653  0.06630199  0.17957884]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05242966  0.00366903  0.13843141  0.31618422  0.42627335]\n [ 0.04303543  0.14384086  0.06025483  0.02066727  0.1878061 ]\n [ 0.09077039  0.3052794   0.10442591 -0.18657829 -0.02338018]\n [-0.06641268  0.20075181  0.1168221  -0.09914936  0.03146221]\n [-0.31627935 -0.1019675   0.07930886  0.07176472  0.18469623]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 49] train=0.484856 val=0.599000 loss=71181.152405 time: 22.553960\n0\n\n[[-0.0523789   0.00290432  0.1374256   0.32015458  0.43757862]\n [ 0.04088133  0.14178091  0.05583438  0.01877321  0.19331639]\n [ 0.08738033  0.30277792  0.09913401 -0.19374184 -0.02446109]\n [-0.0727064   0.19757456  0.11314692 -0.10729422  0.02797785]\n [-0.32531145 -0.1078529   0.07523517  0.06498053  0.18178636]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.05832025 -0.00227615  0.13401766  0.31867477  0.43429804]\n [ 0.03525633  0.1379866   0.05388048  0.01927779  0.19385976]\n [ 0.08221535  0.30126333  0.09969314 -0.19304898 -0.02326567]\n [-0.07506133  0.19867045  0.11575367 -0.10635439  0.02743504]\n [-0.3287285  -0.10948762  0.07512524  0.06322579  0.17866412]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.04738997  0.00953811  0.1406234   0.32349202  0.43989882]\n [ 0.04574816  0.14854456  0.058348    0.0219445   0.19822586]\n [ 0.0938544   0.3147273   0.10784038 -0.18908322 -0.02006775]\n [-0.06209118  0.21313806  0.12662601 -0.10072298  0.02912434]\n [-0.3169052  -0.09730224  0.08570958  0.07010052  0.17996317]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.05789726 -0.0020346   0.13065974  0.31735316  0.43385008]\n [ 0.03412773  0.1361987   0.0450041   0.01078963  0.18878068]\n [ 0.08561924  0.30830806  0.09778239 -0.20184778 -0.03219816]\n [-0.07004622  0.20768191  0.11961753 -0.11110208  0.01911218]\n [-0.32761768 -0.10623115  0.07715001  0.06009389  0.17257969]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 50] train=0.492167 val=0.604100 loss=70868.857849 time: 23.723485\n0\n\n[[-0.0606888  -0.00716622  0.12661615  0.31662515  0.43061534]\n [ 0.0340319   0.13475867  0.04344779  0.01366526  0.19022575]\n [ 0.08573318  0.3100121   0.10009207 -0.19527538 -0.02521054]\n [-0.06947011  0.20957194  0.12322941 -0.10357158  0.02705665]\n [-0.3260258  -0.10571849  0.07928805  0.06586543  0.17838128]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-6.06467985e-02  5.11878170e-07  1.34685516e-01  3.22903931e-01\n   4.34171468e-01]\n [ 3.38291377e-02  1.40700787e-01  4.76027913e-02  1.84584670e-02\n   1.94078445e-01]\n [ 8.38730931e-02  3.13511670e-01  9.99276862e-02 -1.94866955e-01\n  -2.33530123e-02]\n [-7.30708838e-02  2.08435446e-01  1.19099125e-01 -1.07017986e-01\n   2.54668966e-02]\n [-3.29756558e-01 -1.10228784e-01  7.29465708e-02  6.12001605e-02\n   1.75763652e-01]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.06404004 -0.00353508  0.13126509  0.32218048  0.4329962 ]\n [ 0.02884191  0.13602757  0.04204064  0.01507648  0.19237363]\n [ 0.08187182  0.31345192  0.09873587 -0.19553787 -0.02083223]\n [-0.07285352  0.21290717  0.12320153 -0.10379589  0.03066595]\n [-0.32437062 -0.10140528  0.08267434  0.0702721   0.18474041]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.06086431  0.00101319  0.1381466   0.32992986  0.43870798]\n [ 0.02770897  0.13568057  0.04292737  0.01631038  0.19482835]\n [ 0.07425982  0.3071321   0.09211051 -0.2032419  -0.02520404]\n [-0.08646987  0.20083317  0.11153825 -0.11793339  0.01929787]\n [-0.33969134 -0.11683666  0.06867643  0.05448215  0.1711082 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 51] train=0.491346 val=0.596800 loss=70961.070496 time: 26.137349\n0\n\n[[-0.053229    0.00058719  0.1357618   0.3303496   0.43683353]\n [ 0.03256974  0.13591406  0.04163064  0.0178309   0.19382232]\n [ 0.0775147   0.30935755  0.09326237 -0.2014283  -0.02501458]\n [-0.0847941   0.20185386  0.11306321 -0.11610853  0.0177324 ]\n [-0.3378846  -0.11734872  0.06999259  0.05633681  0.16753662]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.05469257 -0.0048082   0.13103935  0.32699624  0.42883453]\n [ 0.03608254  0.13618468  0.03924313  0.01579426  0.18865524]\n [ 0.08516335  0.31427217  0.09379426 -0.20296344 -0.02812201]\n [-0.07854351  0.20641395  0.11533668 -0.11495373  0.01676852]\n [-0.3339156  -0.11414946  0.07379466  0.06195302  0.17061885]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.05559801 -0.00781049  0.1316859   0.33402747  0.4368106 ]\n [ 0.03915155  0.13757297  0.04022509  0.02012562  0.19689256]\n [ 0.09042603  0.31760868  0.09354734 -0.2027475  -0.02226755]\n [-0.07422268  0.20862049  0.11458741 -0.11502629  0.02183213]\n [-0.3293222  -0.11142431  0.07421557  0.06457768  0.17608722]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.06639402 -0.01652617  0.12795545  0.33283064  0.43134376]\n [ 0.02905575  0.13128291  0.0362133   0.0159463   0.18891156]\n [ 0.08170759  0.3145866   0.09107041 -0.20803314 -0.02878668]\n [-0.08261152  0.20510337  0.11220088 -0.1184333   0.01927537]\n [-0.3373211  -0.11669484  0.07152843  0.06443802  0.1752016 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 52] train=0.495613 val=0.602400 loss=70107.943848 time: 24.009547\n0\n\n[[-0.05824403 -0.00827109  0.13605186  0.3384231   0.4310991 ]\n [ 0.03337241  0.1364182   0.04273714  0.0238029   0.19231476]\n [ 0.08608671  0.32158586  0.09949365 -0.19863853 -0.02451874]\n [-0.08004833  0.21061349  0.12058671 -0.10898738  0.0243919 ]\n [-0.33646503 -0.1147417   0.07732505  0.07138211  0.17915465]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07071297 -0.0143885   0.13305937  0.337807    0.43166193]\n [ 0.02394011  0.13320933  0.04175447  0.02308509  0.1930647 ]\n [ 0.08169419  0.3242396   0.10280739 -0.19875768 -0.02450509]\n [-0.07978553  0.21653809  0.12688181 -0.10705326  0.02446333]\n [-0.33379775 -0.10845673  0.08464921  0.07541153  0.18018962]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.06594153 -0.00959159  0.13803633  0.34425247  0.4389005 ]\n [ 0.02818491  0.13752382  0.04588065  0.02847581  0.19967455]\n [ 0.08476472  0.32962704  0.10713608 -0.19684185 -0.02278799]\n [-0.07902758  0.21918921  0.12850985 -0.10754918  0.02278203]\n [-0.33512577 -0.10862716  0.08405625  0.07400016  0.17554171]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07328839 -0.01867402  0.1292014   0.33867294  0.43963683]\n [ 0.0237201   0.1302053   0.03659986  0.02064387  0.19987646]\n [ 0.08397084  0.32691082  0.10157243 -0.2036706  -0.02288774]\n [-0.08101255  0.2141883   0.1223893  -0.11511032  0.01866946]\n [-0.34492502 -0.12023471  0.07499149  0.06379301  0.1656839 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 53] train=0.495052 val=0.615200 loss=70394.574799 time: 23.989776\n0\n\n[[-0.07521345 -0.02336647  0.12354428  0.33748814  0.4414247 ]\n [ 0.02309072  0.12952228  0.03572465  0.02253588  0.20529649]\n [ 0.08599278  0.33047453  0.10227635 -0.20298955 -0.01857939]\n [-0.07717181  0.22060603  0.12504347 -0.11312208  0.0225338 ]\n [-0.34036732 -0.11437382  0.07865265  0.06920683  0.17208202]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07731393 -0.02663215  0.11998352  0.33419463  0.43674248]\n [ 0.02251556  0.13041414  0.03421926  0.01861058  0.1992361 ]\n [ 0.08791152  0.33457118  0.09902238 -0.21140042 -0.02758013]\n [-0.07623888  0.2225339   0.11901361 -0.12231766  0.01450177]\n [-0.34483856 -0.11963736  0.06897167  0.06116537  0.1657105 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07428287 -0.02140832  0.12780043  0.34419042  0.4444928 ]\n [ 0.01962135  0.13039263  0.03670616  0.02358943  0.2048949 ]\n [ 0.0836134   0.33305314  0.0977428  -0.21152376 -0.02407675]\n [-0.07784324  0.22252314  0.11981888 -0.12005264  0.01927021]\n [-0.34462366 -0.11939318  0.07063016  0.06597707  0.16998729]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07677523 -0.02271162  0.12905236  0.34354866  0.43960562]\n [ 0.01660737  0.12922728  0.03842923  0.02332037  0.20211397]\n [ 0.08176755  0.33259326  0.09818973 -0.21288443 -0.02406374]\n [-0.07956778  0.22192664  0.12066963 -0.11971819  0.02179368]\n [-0.3483888  -0.12331209  0.07061525  0.06922741  0.17707665]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 54] train=0.498337 val=0.598500 loss=69845.867126 time: 22.747347\n0\n\n[[-0.07798465 -0.02298892  0.12955621  0.34425607  0.43787435]\n [ 0.01831336  0.13136016  0.03786723  0.02072488  0.19859385]\n [ 0.08383299  0.33674416  0.0973695  -0.21883827 -0.03031278]\n [-0.07928289  0.22481284  0.12154905 -0.12445824  0.0157731 ]\n [-0.34950346 -0.12368459  0.07215057  0.06806826  0.17234947]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0867244  -0.03406578  0.12016258  0.33653095  0.43019542]\n [ 0.00954259  0.12191983  0.02753156  0.01101164  0.19064431]\n [ 0.08008808  0.3333952   0.09110649 -0.22620858 -0.03602474]\n [-0.07890148  0.22670162  0.12179343 -0.12608656  0.01245619]\n [-0.34851602 -0.12271623  0.07278634  0.06833608  0.16704805]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.0779472  -0.03212887  0.12085716  0.34351435  0.4428241 ]\n [ 0.01515796  0.12329759  0.02603975  0.01337788  0.1999646 ]\n [ 0.08330695  0.33602396  0.09148891 -0.22485575 -0.03059676]\n [-0.07772842  0.22977966  0.12290695 -0.12493866  0.01393972]\n [-0.351899   -0.1249544   0.06895282  0.06643254  0.16356994]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07661023 -0.03197459  0.12287918  0.34931394  0.44869787]\n [ 0.0184893   0.12426054  0.02660979  0.01508791  0.20334622]\n [ 0.08703429  0.33924612  0.09326302 -0.22490251 -0.02877708]\n [-0.07689161  0.23229831  0.1256186  -0.12554799  0.01342295]\n [-0.35038668 -0.12140651  0.07479303  0.07035055  0.16495445]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 55] train=0.502504 val=0.611400 loss=69762.264282 time: 23.461399\n0\n\n[[-0.07417661 -0.02988925  0.12792888  0.35709092  0.45469207]\n [ 0.02161073  0.12809573  0.03214702  0.02035104  0.2052891 ]\n [ 0.09231605  0.345662    0.09773357 -0.22294104 -0.03014838]\n [-0.07003195  0.23866592  0.13000657 -0.12433275  0.01167395]\n [-0.34320566 -0.11603281  0.07864132  0.07371137  0.16635433]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.06765974 -0.0263635   0.13123529  0.35799217  0.45150527]\n [ 0.01754417  0.12363106  0.02856886  0.01646391  0.19891493]\n [ 0.08418825  0.33933535  0.09271194 -0.22772096 -0.03566992]\n [-0.07632149  0.23441441  0.12729175 -0.12732553  0.00759283]\n [-0.34858036 -0.1213402   0.07520377  0.07006991  0.1599048 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07246917 -0.02773591  0.13354963  0.3588071   0.44785434]\n [ 0.01450546  0.12463338  0.03187069  0.01764323  0.19737606]\n [ 0.08158307  0.34127536  0.0948026  -0.23001908 -0.03806804]\n [-0.08257812  0.23287259  0.12791121 -0.12983175  0.00734996]\n [-0.35835582 -0.12607488  0.07546584  0.0700547   0.16258219]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.06841231 -0.02239855  0.1393487   0.36294234  0.4477894 ]\n [ 0.019243    0.13122876  0.0375821   0.02110516  0.19471759]\n [ 0.08866654  0.3512644   0.10081978 -0.2284033  -0.04213573]\n [-0.07530596  0.24361888  0.13579147 -0.12839311  0.00327693]\n [-0.3508887  -0.11542084  0.08558785  0.07449557  0.16208065]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 56] train=0.501142 val=0.603900 loss=69727.752350 time: 25.234763\n0\n\n[[-7.20503777e-02 -2.29351055e-02  1.42288595e-01  3.67868543e-01\n   4.46212411e-01]\n [ 1.67969596e-02  1.32781446e-01  3.96709330e-02  2.38260143e-02\n   1.92894295e-01]\n [ 8.70529637e-02  3.52435082e-01  1.00139424e-01 -2.28886396e-01\n  -4.50616404e-02]\n [-7.82105252e-02  2.41210580e-01  1.33233771e-01 -1.29673392e-01\n  -7.09985907e-05]\n [-3.53258610e-01 -1.19176514e-01  8.18494335e-02  7.33383372e-02\n   1.58748835e-01]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07604893 -0.02731618  0.14046505  0.36644834  0.4434761 ]\n [ 0.01627609  0.13119085  0.03974829  0.02423727  0.19311902]\n [ 0.08994759  0.35600302  0.10484356 -0.22608215 -0.04193363]\n [-0.07457965  0.24605645  0.13996606 -0.12360318  0.00562119]\n [-0.35085863 -0.11562978  0.08804648  0.08154844  0.16429035]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.06294137 -0.02217616  0.14168826  0.36836016  0.44454345]\n [ 0.02435249  0.13353325  0.04011339  0.02618047  0.19684184]\n [ 0.09584355  0.36149746  0.10839237 -0.22490528 -0.03796693]\n [-0.07391265  0.247744    0.1413471  -0.1240228   0.00721725]\n [-0.35275912 -0.1174724   0.08686108  0.08045514  0.163688  ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.06825833 -0.02997336  0.13510934  0.36611244  0.4452079 ]\n [ 0.02030289  0.12959997  0.03633261  0.02607265  0.20112433]\n [ 0.0905468   0.36013398  0.10703298 -0.2254342  -0.03505282]\n [-0.08228877  0.24461822  0.14114264 -0.12360354  0.00932659]\n [-0.3654821  -0.12577377  0.08461998  0.08186031  0.16550016]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 57] train=0.506110 val=0.606300 loss=69271.390442 time: 25.630033\n0\n\n[[-0.06519091 -0.02624931  0.13616955  0.36419135  0.44144002]\n [ 0.02371285  0.13293895  0.03710411  0.02348513  0.19659616]\n [ 0.0929042   0.3606862   0.10495824 -0.23019865 -0.04113435]\n [-0.08547603  0.23905383  0.13478512 -0.13004695  0.00208271]\n [-0.37087703 -0.133726    0.07612582  0.07363381  0.15617427]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07234376 -0.03449987  0.12761123  0.35828078  0.43806046]\n [ 0.01989318  0.12913047  0.03039309  0.01744073  0.19468597]\n [ 0.0909261   0.36095086  0.10150549 -0.23577985 -0.04258967]\n [-0.08977557  0.23774071  0.13342822 -0.13273254  0.00100754]\n [-0.37821114 -0.1383521   0.07560221  0.0748883   0.1574865 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07218336 -0.03047886  0.13238032  0.36491954  0.44839552]\n [ 0.0224254   0.13567294  0.03679663  0.02522261  0.20673442]\n [ 0.09187692  0.3658602   0.10604453 -0.23041196 -0.03166254]\n [-0.08748407  0.24149068  0.13739952 -0.12834805  0.01106324]\n [-0.37028253 -0.13314538  0.0785175   0.07855806  0.16666001]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07727037 -0.03376243  0.13211249  0.36326763  0.4431385 ]\n [ 0.01889618  0.1352142   0.03880386  0.02654677  0.20508769]\n [ 0.08732787  0.36766794  0.1105879  -0.22673456 -0.02896531]\n [-0.09289555  0.24187823  0.140399   -0.12576012  0.01419064]\n [-0.37526825 -0.13472895  0.07851518  0.07883707  0.16703181]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 58] train=0.504267 val=0.603900 loss=69249.810059 time: 23.371034\n0\n\n[[-0.07335272 -0.02955991  0.1370848   0.36574206  0.44398662]\n [ 0.02171696  0.13715747  0.04218715  0.02805028  0.20573956]\n [ 0.08946933  0.37072355  0.11291668 -0.2290012  -0.03131686]\n [-0.09130071  0.24447559  0.14213891 -0.12925883  0.00906056]\n [-0.37242362 -0.13426729  0.07815441  0.07445896  0.15834129]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08051193 -0.03150283  0.13928787  0.3662949   0.44267482]\n [ 0.0147467   0.13465019  0.04220619  0.02470023  0.20360959]\n [ 0.08266332  0.3688571   0.11091543 -0.23620711 -0.03572429]\n [-0.09782686  0.2420451   0.14069586 -0.134798    0.00479024]\n [-0.3763648  -0.13789576  0.07605477  0.06941947  0.15218975]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08446359 -0.03969534  0.13558848  0.36627278  0.44611275]\n [ 0.00896657  0.12378246  0.03416064  0.01973218  0.20076422]\n [ 0.07814687  0.3625371   0.1047313  -0.24180935 -0.0410247 ]\n [-0.09921458  0.2428906   0.14316702 -0.13222405  0.00501461]\n [-0.37627444 -0.13517189  0.08271832  0.0775139   0.15740272]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07499548 -0.03412848  0.1379364   0.36874753  0.44804338]\n [ 0.01713437  0.12772477  0.03625415  0.02321773  0.2042397 ]\n [ 0.084057    0.3664666   0.10892982 -0.23571445 -0.03603657]\n [-0.09764023  0.24448563  0.14778739 -0.12453456  0.00984637]\n [-0.3781641  -0.13846166  0.08315405  0.08046795  0.15721947]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 59] train=0.505248 val=0.622100 loss=69166.050323 time: 22.172830\n0\n\n[[-0.06177317 -0.02540359  0.13971162  0.36733302  0.44187316]\n [ 0.02969705  0.13650538  0.03693509  0.01965326  0.19681798]\n [ 0.09439313  0.37456816  0.11139959 -0.23856601 -0.04253652]\n [-0.0893312   0.25172243  0.15258646 -0.12576675  0.00288724]\n [-0.36984047 -0.13079691  0.08871543  0.07976485  0.14934479]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.06909284 -0.03285832  0.13299352  0.3611928   0.44099253]\n [ 0.01946603  0.12602478  0.02644594  0.00939378  0.19245099]\n [ 0.08653231  0.36861125  0.10399436 -0.24910907 -0.04764208]\n [-0.09309151  0.24920225  0.14913276 -0.131087   -0.00052711]\n [-0.3729821  -0.13363327  0.08681003  0.07750571  0.14551128]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07367054 -0.0293459   0.14171804  0.37063763  0.45105734]\n [ 0.01449453  0.1302524   0.03474882  0.0161242   0.2009321 ]\n [ 0.0828037   0.37411538  0.11028475 -0.24652775 -0.04119364]\n [-0.09888191  0.2491623   0.1482201  -0.13388133  0.00233502]\n [-0.38216063 -0.13888754  0.08141825  0.07243907  0.14536287]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07967589 -0.03419972  0.140524    0.3774327   0.46391797]\n [ 0.0077188   0.12410287  0.03100393  0.02062126  0.21280502]\n [ 0.07615986  0.36977622  0.10709763 -0.24501762 -0.03227671]\n [-0.1054985   0.24345875  0.14454947 -0.13283111  0.00888686]\n [-0.3868096  -0.14544351  0.0757379   0.07223656  0.14851882]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 60] train=0.503085 val=0.607300 loss=69195.754028 time: 22.459406\n0\n\n[[-0.07652949 -0.03334966  0.14515577  0.38145766  0.46126553]\n [ 0.01109343  0.12549756  0.03492784  0.02444496  0.21155508]\n [ 0.08073451  0.37421134  0.11142863 -0.24233124 -0.03152708]\n [-0.10031043  0.2492418   0.15105326 -0.12887707  0.00999334]\n [-0.38568333 -0.14410317  0.08029232  0.07650901  0.14998993]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0710522  -0.0335226   0.14118819  0.3769061   0.45588055]\n [ 0.01626386  0.12628265  0.0315206   0.01860996  0.20712867]\n [ 0.08365222  0.3776445   0.11204961 -0.24502146 -0.03227612]\n [-0.09598076  0.25550973  0.15643248 -0.12760754  0.01143533]\n [-0.37627918 -0.13375865  0.08995284  0.08087863  0.15053315]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.069741   -0.03226753  0.14183128  0.37550473  0.44966602]\n [ 0.01869004  0.12499481  0.02829058  0.01266687  0.19956268]\n [ 0.08697236  0.37673777  0.10865204 -0.25470164 -0.04250477]\n [-0.09231286  0.25443545  0.15449373 -0.134096    0.00408386]\n [-0.36992034 -0.13273568  0.09112819  0.08076511  0.14830846]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07534888 -0.03605545  0.13845353  0.37466466  0.45260563]\n [ 0.01060078  0.11765664  0.02494388  0.01327739  0.20576245]\n [ 0.07868713  0.3672712   0.10317381 -0.25566152 -0.0367278 ]\n [-0.10142165  0.24319561  0.14704311 -0.13619748  0.00641978]\n [-0.3780537  -0.1444355   0.07955996  0.07369095  0.14449473]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 61] train=0.506771 val=0.616100 loss=68902.775177 time: 24.033722\n0\n\n[[-0.07603484 -0.03645027  0.13970262  0.37641534  0.45146215]\n [ 0.01301943  0.12006849  0.02676629  0.0130139   0.20366438]\n [ 0.08217429  0.37227187  0.10485945 -0.25892082 -0.0403336 ]\n [-0.09832459  0.24798207  0.1498974  -0.13966371  0.00059837]\n [-0.37620655 -0.14171879  0.08328768  0.07110001  0.13667782]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0765059  -0.03624469  0.14452125  0.3848393   0.45849898]\n [ 0.01013101  0.11707924  0.027627    0.0172323   0.20619522]\n [ 0.08034381  0.3724413   0.10906237 -0.25431067 -0.03891955]\n [-0.09899636  0.24980767  0.15710425 -0.13031447  0.00538684]\n [-0.37598327 -0.13984145  0.08987369  0.08223695  0.1442299 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07637997 -0.0371904   0.14322777  0.3848176   0.45808864]\n [ 0.0116442   0.11863577  0.02907902  0.01947426  0.20836735]\n [ 0.08169878  0.37448332  0.11167868 -0.25147507 -0.03737411]\n [-0.09879985  0.2511255   0.15994368 -0.12760533  0.00570887]\n [-0.37576476 -0.13919769  0.09056755  0.08241557  0.1439771 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07145014 -0.03508247  0.14219025  0.38165718  0.45434928]\n [ 0.0129006   0.11869252  0.0258979   0.01385157  0.20537616]\n [ 0.08208562  0.3751757   0.10772765 -0.25776446 -0.03898714]\n [-0.09642049  0.25200477  0.15668082 -0.13267237  0.0055504 ]\n [-0.3752231  -0.14141156  0.08559555  0.07827327  0.14403467]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 62] train=0.507792 val=0.617500 loss=68768.515656 time: 22.483988\n0\n\n[[-0.07124846 -0.03829551  0.13921171  0.38445842  0.4624973 ]\n [ 0.01508363  0.1194512   0.02574084  0.0183895   0.21545507]\n [ 0.08395791  0.37705445  0.10882296 -0.25386217 -0.02936737]\n [-0.09801524  0.25062037  0.15423104 -0.13349837  0.0099706 ]\n [-0.37969336 -0.14531708  0.08056105  0.07485806  0.14515874]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08980138 -0.05475863  0.12460478  0.37000602  0.4458811 ]\n [ 0.00134415  0.10793079  0.0127668   0.0070106   0.20265333]\n [ 0.07613018  0.37357947  0.10060018 -0.26232955 -0.03775728]\n [-0.10349271  0.2513656   0.15209316 -0.13703267  0.00549447]\n [-0.3838086  -0.1423414   0.08295224  0.07800653  0.1460829 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07686745 -0.04499915  0.13063703  0.3764343   0.45238647]\n [ 0.00878485  0.11331892  0.01543833  0.01007836  0.20720641]\n [ 0.08104768  0.37888393  0.10187399 -0.26196405 -0.03604419]\n [-0.10289198  0.25158718  0.14970122 -0.13855189  0.00587657]\n [-0.38758418 -0.14657246  0.07891015  0.07728383  0.1471193 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07815014 -0.04661503  0.12826093  0.37517652  0.44918144]\n [ 0.00930265  0.11742672  0.01961899  0.01329507  0.20808093]\n [ 0.08037227  0.38560244  0.10752439 -0.26031622 -0.03466233]\n [-0.1041351   0.25626412  0.15310818 -0.13826111  0.00727491]\n [-0.39029235 -0.14655584  0.0806604   0.07932729  0.14855829]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 63] train=0.508634 val=0.626700 loss=68572.709473 time: 25.301981\n0\n\n[[-0.07921708 -0.0490289   0.12695473  0.37156987  0.4432863 ]\n [ 0.00476195  0.11294021  0.01722045  0.01067368  0.20378478]\n [ 0.07583194  0.38150692  0.10452677 -0.26377603 -0.03895862]\n [-0.10726997  0.2528983   0.15074837 -0.1410075   0.0046366 ]\n [-0.3961851  -0.1528133   0.07727607  0.07939807  0.14837758]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07638802 -0.049824    0.12896273  0.37835243  0.4511681 ]\n [ 0.01033506  0.11528363  0.02127761  0.02000385  0.21596995]\n [ 0.08198495  0.3850384   0.10808603 -0.2568379  -0.02809876]\n [-0.10169128  0.25379166  0.15044847 -0.1380411   0.01437177]\n [-0.38955793 -0.15206477  0.07609907  0.08187912  0.15732542]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07373483 -0.04693688  0.1340331   0.37970594  0.4450432 ]\n [ 0.01550462  0.11954493  0.0236456   0.01780839  0.20793945]\n [ 0.0890547   0.3911697   0.1081344  -0.26432517 -0.04038654]\n [-0.0934026   0.26125553  0.15255043 -0.14476497  0.00079172]\n [-0.38298777 -0.14606464  0.07821964  0.07552052  0.14261407]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07043825 -0.04195508  0.14088492  0.39036027  0.4572386 ]\n [ 0.01703347  0.12417745  0.03198889  0.02986044  0.22209594]\n [ 0.08506955  0.39174718  0.11246637 -0.25543633 -0.02495156]\n [-0.09258222  0.26553857  0.16001447 -0.13338725  0.01925834]\n [-0.3734424  -0.13456748  0.09095745  0.08956101  0.15783618]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 64] train=0.507432 val=0.620700 loss=68551.474854 time: 28.144904\n0\n\n[[-0.06934135 -0.0402027   0.14320336  0.39156935  0.45594162]\n [ 0.01393802  0.12218492  0.03055266  0.02668274  0.21698107]\n [ 0.08007339  0.38766047  0.10850324 -0.26124522 -0.0303208 ]\n [-0.09692226  0.26052934  0.15439412 -0.1399785   0.01392905]\n [-0.37663397 -0.13978371  0.08449363  0.08423113  0.1552525 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07777357 -0.04605146  0.13736424  0.38863719  0.4551364 ]\n [ 0.00695285  0.11725012  0.0245414   0.02350779  0.2179029 ]\n [ 0.07417292  0.38669726  0.10716518 -0.26270795 -0.02695864]\n [-0.10172183  0.260108    0.15480901 -0.14233135  0.01497307]\n [-0.38031977 -0.14365707  0.08144185  0.07978557  0.15419741]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07675303 -0.04769618  0.13568911  0.38445693  0.44283238]\n [ 0.01041923  0.11866058  0.02638924  0.02401459  0.21009119]\n [ 0.07683837  0.3877968   0.10941029 -0.26126194 -0.03174431]\n [-0.09719227  0.26216847  0.15493542 -0.14374055  0.01033125]\n [-0.37441567 -0.1423008   0.07824259  0.07537007  0.14710341]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0842078  -0.05689145  0.12519374  0.37752274  0.43824288]\n [ 0.00422324  0.10859558  0.0131171   0.01464694  0.20661615]\n [ 0.06971457  0.37757084  0.09823505 -0.27015567 -0.03420734]\n [-0.10968372  0.24697874  0.1420106  -0.15236118  0.00885677]\n [-0.39275706 -0.16269748  0.06262693  0.06680807  0.14527896]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 65] train=0.513662 val=0.628000 loss=68436.739777 time: 25.948667\n0\n\n[[-0.07096019 -0.04482568  0.13430451  0.38475627  0.44251734]\n [ 0.01864923  0.12077137  0.02245359  0.02444238  0.21617179]\n [ 0.08522177  0.39171565  0.10944788 -0.25906113 -0.02273157]\n [-0.09276523  0.26262957  0.15660371 -0.1375581   0.02276262]\n [-0.3734688  -0.1455304   0.07850029  0.08383283  0.15990505]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0807285  -0.05325957  0.12420698  0.3729678   0.4310298 ]\n [ 0.01473711  0.11852164  0.01640799  0.01443782  0.20482637]\n [ 0.08012764  0.38959578  0.10294459 -0.27255368 -0.0369744 ]\n [-0.10164896  0.2563698   0.14732574 -0.15295011  0.00654073]\n [-0.38512188 -0.15791294  0.06471276  0.06742902  0.14135464]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08084138 -0.04959657  0.1340414   0.38527393  0.44297782]\n [ 0.01473066  0.12215605  0.02460159  0.02396757  0.21470891]\n [ 0.08166612  0.3946569   0.1116215  -0.2646972  -0.03095211]\n [-0.09953599  0.25951833  0.15349142 -0.14895116  0.00763758]\n [-0.38184878 -0.15473884  0.0698607   0.06968892  0.13906178]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08289837 -0.0544401   0.12725991  0.37943378  0.43512106]\n [ 0.00954549  0.11766552  0.01722671  0.01565138  0.20597096]\n [ 0.07829601  0.39397573  0.10793753 -0.27078828 -0.03845887]\n [-0.0994743   0.2615983   0.15443715 -0.1474826   0.00745327]\n [-0.38052344 -0.15271182  0.07418145  0.07808059  0.14539039]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 66] train=0.511719 val=0.619800 loss=68201.195953 time: 24.865101\n0\n\n[[-0.07232291 -0.04346445  0.13583477  0.38807777  0.44226882]\n [ 0.016406    0.12538052  0.02304801  0.01965799  0.21093439]\n [ 0.08076715  0.3987014   0.11099407 -0.27128354 -0.03547619]\n [-0.09928032  0.26385745  0.15616873 -0.14894482  0.0081057 ]\n [-0.38146278 -0.15326963  0.07188366  0.07127335  0.13933377]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07133308 -0.045766    0.1358734   0.3895167   0.4423507 ]\n [ 0.01106068  0.1180052   0.0200235   0.01785205  0.20859548]\n [ 0.07506552  0.39426324  0.11078802 -0.27154776 -0.03513289]\n [-0.10649171  0.25916243  0.15751092 -0.14720188  0.0099364 ]\n [-0.38540223 -0.15331046  0.07643837  0.07761872  0.1461304 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.0716207  -0.04937869  0.13254642  0.38964537  0.44481358]\n [ 0.00868979  0.11275676  0.01347501  0.0128841   0.20629938]\n [ 0.07081015  0.38795963  0.10358566 -0.27945626 -0.04159218]\n [-0.11097403  0.25366408  0.15087374 -0.1556727   0.00183967]\n [-0.39327016 -0.16085355  0.0689939   0.0697606   0.14017232]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0742254  -0.05300525  0.13085032  0.38832015  0.4412622 ]\n [ 0.00739986  0.11146048  0.01317095  0.01471042  0.20609637]\n [ 0.06926584  0.38761368  0.10516053 -0.27482557 -0.0391157 ]\n [-0.11064666  0.25593185  0.15572017 -0.1472368   0.00723558]\n [-0.39057606 -0.15941535  0.07330361  0.07905815  0.14718631]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 67] train=0.509235 val=0.628400 loss=68511.444275 time: 24.085325\n0\n\n[[-0.07444838 -0.05195538  0.1312081   0.38644826  0.43800572]\n [ 0.00532315  0.11236314  0.01148176  0.01126014  0.20221844]\n [ 0.06724429  0.39006352  0.10501112 -0.27824703 -0.04440343]\n [-0.11106758  0.25780967  0.15620226 -0.14858983  0.0025991 ]\n [-0.39025962 -0.15917206  0.07236175  0.07815523  0.14288937]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07485179 -0.05417285  0.12720872  0.38170332  0.430203  ]\n [ 0.00806426  0.11482011  0.01093389  0.00849119  0.19868205]\n [ 0.07143021  0.39438626  0.10602205 -0.28012654 -0.04527127]\n [-0.10609768  0.2646752   0.16085225 -0.14650571  0.00468731]\n [-0.38371524 -0.1511106   0.08043786  0.08538375  0.14712635]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.06666854 -0.0465331   0.13218322  0.38731787  0.43526962]\n [ 0.00945156  0.11633553  0.00986762  0.00664468  0.19984488]\n [ 0.07112825  0.39539027  0.10462924 -0.28433058 -0.04614718]\n [-0.10650184  0.26309407  0.15860744 -0.14775571  0.00710077]\n [-0.38455695 -0.1548592   0.0773462   0.08692729  0.15250458]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.06157438 -0.03899524  0.1424347   0.39599657  0.43955275]\n [ 0.0153586   0.1243825   0.0188438   0.01364847  0.20534877]\n [ 0.07780243  0.4036835   0.11333826 -0.27686682 -0.03890015]\n [-0.09868191  0.27201682  0.16848753 -0.13752446  0.01680236]\n [-0.37437096 -0.14398883  0.08944936  0.09875857  0.16099788]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 68] train=0.513081 val=0.627200 loss=67964.535706 time: 26.669072\n0\n\n[[-0.06235895 -0.03869766  0.14608812  0.39939943  0.44178906]\n [ 0.01277043  0.12188824  0.01914364  0.01398773  0.20654936]\n [ 0.07640274  0.40269813  0.11251956 -0.27841765 -0.0368035 ]\n [-0.10019592  0.26861984  0.16352493 -0.14166257  0.01935259]\n [-0.37524587 -0.14942043  0.08126409  0.09316561  0.1612165 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.06483199 -0.04277755  0.14294235  0.39914557  0.4392421 ]\n [ 0.01424759  0.12193305  0.01903309  0.01428942  0.20319772]\n [ 0.0816062   0.4079206   0.11495113 -0.27853885 -0.04026636]\n [-0.09461611  0.27335224  0.16416231 -0.14372754  0.01638898]\n [-0.37256807 -0.15017454  0.0760977   0.0874136   0.15620625]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.06637442 -0.05001301  0.13702822  0.39877093  0.4415978 ]\n [ 0.01587345  0.1190295   0.01710216  0.01801069  0.21151924]\n [ 0.08449197  0.40924764  0.1162296  -0.27480936 -0.03156366]\n [-0.09379593  0.27521113  0.16820636 -0.13852745  0.02571114]\n [-0.37378886 -0.14939214  0.08051005  0.09363398  0.16452004]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.06806706 -0.04801293  0.13988894  0.39906722  0.43784377]\n [ 0.01174858  0.11899762  0.01679615  0.01565154  0.20885308]\n [ 0.07597647  0.4041401   0.11131828 -0.280444   -0.03582148]\n [-0.10987511  0.26354212  0.16024828 -0.145731    0.01896093]\n [-0.39331645 -0.16515675  0.07067234  0.08568847  0.15609623]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 69] train=0.515405 val=0.625200 loss=67755.374146 time: 25.766795\n0\n\n[[-0.08000759 -0.05634227  0.13129285  0.39138135  0.43170372]\n [ 0.00338433  0.11448941  0.01090338  0.0105033   0.20464797]\n [ 0.0709386   0.40127996  0.10689016 -0.2838735  -0.03887397]\n [-0.11189254  0.26125103  0.15730408 -0.14722109  0.01767648]\n [-0.3927162  -0.16582257  0.07025228  0.08763842  0.15884006]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07253191 -0.05382375  0.1285636   0.3876796   0.4308119 ]\n [ 0.01489197  0.12096872  0.00910715  0.00538725  0.2044303 ]\n [ 0.08302929  0.4092613   0.10594187 -0.28839818 -0.03822206]\n [-0.10014798  0.26925674  0.1596536  -0.14617553  0.02184497]\n [-0.380071   -0.1569649   0.07709786  0.09661806  0.16979232]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.06597266 -0.0469213   0.13519     0.3956542   0.4389697 ]\n [ 0.02208191  0.1297453   0.01656333  0.01258029  0.21275344]\n [ 0.08567826  0.41612715  0.11084299 -0.28754848 -0.03786683]\n [-0.1045393   0.2706259   0.1621271  -0.14881851  0.01612086]\n [-0.38859096 -0.16288781  0.07433058  0.08943187  0.15841633]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.07970341 -0.05653461  0.12635307  0.3866648   0.4308732 ]\n [ 0.00898231  0.1210257   0.00879561  0.00474701  0.2069139 ]\n [ 0.07546291  0.4108613   0.10671396 -0.2920129  -0.03990849]\n [-0.1105681   0.26814008  0.1624323  -0.14849295  0.01825767]\n [-0.38711458 -0.16153574  0.07823619  0.09382571  0.16323774]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 70] train=0.518790 val=0.632100 loss=67392.026459 time: 26.427576\n0\n\n[[-0.08033188 -0.05338685  0.13064092  0.38883612  0.4234761 ]\n [ 0.01250815  0.12716308  0.01345757  0.00679314  0.20132066]\n [ 0.07933315  0.4187919   0.11247364 -0.2906223  -0.04538421]\n [-0.11050215  0.270847    0.16513535 -0.1501      0.01044687]\n [-0.3910761  -0.16372554  0.07709037  0.0894901   0.1547927 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09035694 -0.06097528  0.12889236  0.38981578  0.42593542]\n [ 0.00372658  0.12179633  0.01349074  0.00963642  0.207765  ]\n [ 0.07167067  0.41616124  0.11442387 -0.28623983 -0.03409661]\n [-0.11950748  0.26642022  0.16490161 -0.14721252  0.02205494]\n [-0.39858714 -0.16940923  0.07509765  0.08994773  0.161977  ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.10096125 -0.070664    0.12436767  0.39032134  0.42488527]\n [-0.00783494  0.10976258  0.00511241  0.00783838  0.20583911]\n [ 0.06271324  0.40766212  0.10809906 -0.28795195 -0.03575529]\n [-0.12253593  0.26366925  0.16284607 -0.14674352  0.02208571]\n [-0.4004963  -0.17341076  0.07118693  0.0891298   0.16205744]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09381099 -0.06382664  0.13520588  0.40184394  0.4357289 ]\n [-0.00258157  0.11510175  0.01262797  0.01326128  0.20985185]\n [ 0.06477903  0.41184443  0.11253802 -0.28832033 -0.03864373]\n [-0.11694499  0.27085426  0.16964093 -0.14785776  0.01592759]\n [-0.39051428 -0.16400278  0.0783241   0.08886296  0.15568064]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 71] train=0.517388 val=0.631900 loss=67493.644775 time: 23.907899\n0\n\n[[-0.08649421 -0.05644842  0.14272381  0.40781385  0.43724358]\n [ 0.00434981  0.12116575  0.01919427  0.02058949  0.21404007]\n [ 0.07123686  0.41760144  0.12003069 -0.27895218 -0.03195972]\n [-0.10746874  0.27987355  0.17831737 -0.13930331  0.02151138]\n [-0.37724373 -0.15175615  0.08816312  0.09893809  0.16272362]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09561274 -0.06570554  0.13264415  0.40083504  0.4320154 ]\n [-0.00077477  0.11620852  0.01037178  0.01387916  0.21389934]\n [ 0.06814934  0.41740048  0.11490285 -0.2843426  -0.02788141]\n [-0.1107161   0.27886936  0.17335933 -0.14523345  0.02406166]\n [-0.37899733 -0.1533351   0.08609661  0.09854273  0.1656639 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08102635 -0.0496719   0.14659208  0.41586682  0.44477883]\n [ 0.00296442  0.12217719  0.0162646   0.02388224  0.22501254]\n [ 0.06440688  0.41719782  0.11345235 -0.2836836  -0.02499408]\n [-0.11640216  0.27495977  0.16653441 -0.15304352  0.01655369]\n [-0.38680345 -0.16301121  0.07373089  0.08604381  0.15216778]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09261592 -0.06737224  0.12887186  0.398606    0.42767388]\n [-0.0074455   0.10750999  0.00262708  0.01061708  0.2095873 ]\n [ 0.0553216   0.40763238  0.10631558 -0.29317513 -0.03654855]\n [-0.12057722  0.27278018  0.16828556 -0.15589486  0.00934224]\n [-0.38647684 -0.16165692  0.07680216  0.08513994  0.14772987]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 72] train=0.516887 val=0.629900 loss=67544.648193 time: 24.666664\n0\n\n[[-8.0045037e-02 -5.6941774e-02  1.3213857e-01  3.9677328e-01\n   4.2772257e-01]\n [ 3.7299888e-04  1.1413813e-01  4.6453550e-03  9.2492215e-03\n   2.1201275e-01]\n [ 5.8886349e-02  4.1467050e-01  1.1223268e-01 -2.9048842e-01\n  -3.0490946e-02]\n [-1.1745900e-01  2.8144717e-01  1.7862360e-01 -1.4993326e-01\n   1.4168158e-02]\n [-3.8465345e-01 -1.5596217e-01  8.2794949e-02  8.8607550e-02\n   1.4788444e-01]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-8.16232190e-02 -5.84545285e-02  1.34018913e-01  4.04396385e-01\n   4.38761801e-01]\n [ 8.70269723e-05  1.14485025e-01  6.34379452e-03  1.50927715e-02\n   2.20849767e-01]\n [ 5.85542582e-02  4.17541653e-01  1.15186416e-01 -2.87051141e-01\n  -2.41790935e-02]\n [-1.18942067e-01  2.83380806e-01  1.80760637e-01 -1.48787752e-01\n   1.63623486e-02]\n [-3.87924254e-01 -1.56779617e-01  8.33525881e-02  9.03797597e-02\n   1.49232715e-01]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08461673 -0.06075472  0.12747434  0.39842436  0.4358807 ]\n [-0.0031119   0.11056358 -0.00525772  0.00160685  0.21174753]\n [ 0.05523288  0.41460836  0.103783   -0.3027586  -0.03579628]\n [-0.12337675  0.27951384  0.16958852 -0.16472593  0.00410124]\n [-0.38954407 -0.15912738  0.07647105  0.08007937  0.14027242]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0732218  -0.04923124  0.13671944  0.40579173  0.44200495]\n [ 0.0058906   0.11895151  0.00079594  0.00741927  0.2164689 ]\n [ 0.06053198  0.41926992  0.1081467  -0.29879275 -0.03428692]\n [-0.12041347  0.2825036   0.17487621 -0.15992305  0.00596665]\n [-0.39150307 -0.16159466  0.07551543  0.08177368  0.14054509]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 73] train=0.516206 val=0.626600 loss=67637.449585 time: 23.997617\n0\n\n[[-7.51568452e-02 -5.26154377e-02  1.34760931e-01  4.10851121e-01\n   4.49517727e-01]\n [-2.77886924e-04  1.11705616e-01 -3.71757103e-03  1.02294348e-02\n   2.22927257e-01]\n [ 5.11039235e-02  4.08687413e-01  1.01781696e-01 -2.99053639e-01\n  -3.16655301e-02]\n [-1.30854264e-01  2.72875696e-01  1.71264723e-01 -1.59014896e-01\n   7.70970201e-03]\n [-3.99029374e-01 -1.67544961e-01  7.51623064e-02  8.46419930e-02\n   1.42405033e-01]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.06175625 -0.04377826  0.14046434  0.41685036  0.4556034 ]\n [ 0.01050962  0.12161978  0.00388847  0.01971383  0.23577899]\n [ 0.05949033  0.41990864  0.11138047 -0.28952515 -0.01797239]\n [-0.12449872  0.28373206  0.18217    -0.14902198  0.01899892]\n [-0.39808095 -0.16241023  0.08195103  0.09220199  0.14820407]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07285395 -0.05374489  0.12795134  0.40402824  0.4342335 ]\n [ 0.00519374  0.11828244 -0.00338374  0.01090831  0.21952781]\n [ 0.05626974  0.41801482  0.10677961 -0.29534903 -0.02985028]\n [-0.12652922  0.282898    0.1827754  -0.14888754  0.01226247]\n [-0.39778316 -0.16066222  0.085329    0.09523354  0.1444552 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-6.44695610e-02 -5.15801907e-02  1.24226086e-01  3.95890117e-01\n   4.22386795e-01]\n [ 1.39599694e-02  1.23134285e-01 -4.59893700e-03  5.17654419e-03\n   2.09668905e-01]\n [ 5.94947152e-02  4.20005769e-01  1.05197459e-01 -3.00841540e-01\n  -3.87074500e-02]\n [-1.29527658e-01  2.76818871e-01  1.75486013e-01 -1.58813953e-01\n   1.03313942e-05]\n [-4.03837830e-01 -1.71694770e-01  7.52670541e-02  8.32211822e-02\n   1.28991976e-01]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 74] train=0.520513 val=0.638800 loss=67154.455688 time: 24.686259\n0\n\n[[-0.06621753 -0.04748902  0.13545376  0.40878275  0.43302748]\n [ 0.01099204  0.12706724  0.0060481   0.0163928   0.21915762]\n [ 0.05826294  0.4261764   0.11767415 -0.2914281  -0.0303021 ]\n [-0.13164964  0.2826979   0.18884842 -0.14760135  0.0113515 ]\n [-0.40588933 -0.16751674  0.08626792  0.09608277  0.14239983]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08289808 -0.06097362  0.1240586   0.39740586  0.41732228]\n [-0.00202357  0.11805262 -0.00247544  0.00645258  0.20618793]\n [ 0.05211883  0.42310604  0.1144693  -0.29831654 -0.04008008]\n [-0.13427041  0.283144    0.19177116 -0.14710987  0.00816912]\n [-0.4086058  -0.16769585  0.09220169  0.10232116  0.14527176]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08387733 -0.05925673  0.131087    0.4104699   0.4323826 ]\n [-0.00217903  0.11904419  0.00110096  0.01561881  0.21865208]\n [ 0.04781653  0.4201922   0.1149332  -0.29370072 -0.03423697]\n [-0.14352137  0.274384    0.18765777 -0.14535812  0.01107186]\n [-0.4151737  -0.1758304   0.08888745  0.10505164  0.14925508]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09163705 -0.06916327  0.12039984  0.40243113  0.4227114 ]\n [-0.00463916  0.1139849  -0.00558692  0.01132279  0.2138366 ]\n [ 0.04928942  0.42023763  0.11510897 -0.2928057  -0.03705862]\n [-0.13882183  0.2784891   0.19239588 -0.14081284  0.01258061]\n [-0.41093245 -0.17244226  0.09344646  0.1113996   0.15399691]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 75] train=0.519151 val=0.624100 loss=67357.770111 time: 23.126304\n0\n\n[[-0.07694726 -0.05944578  0.12681676  0.40999895  0.42957973]\n [ 0.00837079  0.12377013  0.00190221  0.01961395  0.22069691]\n [ 0.05740524  0.4279215   0.12251535 -0.2856161  -0.03188717]\n [-0.1308974   0.28748614  0.2029488  -0.1321109   0.0174415 ]\n [-0.3977778  -0.1591779   0.10688286  0.12240472  0.16024046]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08104473 -0.06449541  0.12150001  0.40975693  0.43037146]\n [-0.00147835  0.11519257 -0.00581612  0.01473383  0.21823299]\n [ 0.04664872  0.4200151   0.11549393 -0.29408982 -0.03684749]\n [-0.13811614  0.28011787  0.1961042  -0.14182006  0.01074755]\n [-0.40176153 -0.16754158  0.09600828  0.10982505  0.14925902]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08872975 -0.07228635  0.11622283  0.40749002  0.42529833]\n [-0.00349865  0.11157398 -0.00811579  0.01431817  0.21791469]\n [ 0.04667027  0.41908607  0.11571869 -0.29465672 -0.03646033]\n [-0.13771541  0.28091842  0.19788918 -0.14353684  0.00826767]\n [-0.4009716  -0.1682638   0.09621772  0.10723576  0.14558706]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08528652 -0.06800723  0.12197094  0.41324097  0.4261317 ]\n [ 0.00056082  0.11688322 -0.00260239  0.01942345  0.2185077 ]\n [ 0.05050476  0.42327017  0.11695952 -0.29497752 -0.03638306]\n [-0.13889705  0.27986312  0.19456643 -0.14814557  0.00569277]\n [-0.40313783 -0.1734884   0.08751796  0.09829824  0.13904624]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 76] train=0.520052 val=0.635300 loss=66887.399933 time: 22.790079\n0\n\n[[-0.08298463 -0.06817652  0.11742807  0.41015705  0.4242794 ]\n [ 0.00390576  0.1204229  -0.0030357   0.01851372  0.22004229]\n [ 0.05490532  0.431626    0.12033335 -0.29585567 -0.03471218]\n [-0.1341419   0.28985462  0.20064384 -0.14610521  0.01152335]\n [-0.40256855 -0.16837718  0.09122189  0.10154863  0.14662035]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08772969 -0.07108036  0.11173533  0.40955958  0.4306989 ]\n [-0.00080662  0.11977848 -0.00477558  0.02069408  0.22827134]\n [ 0.04716342  0.4293458   0.1186782  -0.29540196 -0.02733979]\n [-0.14569108  0.28547642  0.1985235  -0.1457902   0.01858542]\n [-0.41348216 -0.17336725  0.0870712   0.10304775  0.1538825 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07839149 -0.06793207  0.11437475  0.41848063  0.44285703]\n [ 0.00819218  0.12249099 -0.00441106  0.0261742   0.23785828]\n [ 0.0558766   0.43290794  0.11686921 -0.29434887 -0.02100835]\n [-0.13996182  0.28808329  0.19824399 -0.1433122   0.02384122]\n [-0.40357494 -0.1677      0.09095479  0.11047146  0.16228567]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09436432 -0.08487982  0.09317736  0.39616737  0.41999018]\n [-0.00560303  0.10842832 -0.02083277  0.00754717  0.21779104]\n [ 0.04723009  0.4258664   0.10682633 -0.30890724 -0.03635208]\n [-0.14260462  0.2878982   0.19495064 -0.15118653  0.01435484]\n [-0.39958638 -0.16428432  0.08992011  0.10404235  0.1528472 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 77] train=0.520673 val=0.611800 loss=66933.064148 time: 25.807895\n0\n\n[[-0.08509042 -0.0768576   0.0988757   0.40246263  0.42716125]\n [ 0.00387291  0.11642348 -0.01649406  0.01126309  0.2246526 ]\n [ 0.05280542  0.42892063  0.10564747 -0.30906427 -0.03127517]\n [-0.14105786  0.28619698  0.190141   -0.15245661  0.0174124 ]\n [-0.40092412 -0.16947977  0.08288544  0.10205845  0.15551949]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.07823969 -0.07452417  0.09782156  0.40016314  0.42470592]\n [ 0.01044249  0.12024222 -0.01658224  0.00929377  0.22076249]\n [ 0.05775627  0.4328893   0.10702372 -0.30981237 -0.03442   ]\n [-0.13652733  0.28869292  0.18946731 -0.15338126  0.01442983]\n [-0.39679357 -0.16733895  0.08195899  0.09999415  0.15092833]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.07409079 -0.06846458  0.10784337  0.41056648  0.43278754]\n [ 0.0132379   0.12613752 -0.00720363  0.0178342   0.2271673 ]\n [ 0.05643392  0.43545118  0.11220511 -0.3045816  -0.02986443]\n [-0.1410493   0.2884873   0.19041596 -0.15334877  0.01446788]\n [-0.40298522 -0.17257935  0.07658832  0.09390587  0.14682606]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08812195 -0.08456588  0.0908851   0.396213    0.41939968]\n [ 0.00207089  0.11267909 -0.02201231  0.00544422  0.21905033]\n [ 0.04911539  0.42633384  0.09997679 -0.31683    -0.0385334 ]\n [-0.1474736   0.2808411   0.17973417 -0.16559345  0.0032416 ]\n [-0.40883213 -0.18005289  0.06703828  0.08303595  0.13448751]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 78] train=0.523858 val=0.618900 loss=66818.077179 time: 25.646117\n0\n\n[[-0.09359003 -0.08397042  0.09641621  0.40122122  0.42556956]\n [-0.0025662   0.11298321 -0.01654988  0.0127391   0.22837102]\n [ 0.0446836   0.42808086  0.10778236 -0.3074081  -0.02488824]\n [-0.14890845  0.28534997  0.19165291 -0.15150012  0.02097317]\n [-0.4067115  -0.17334981  0.08117897  0.0992126   0.1522486 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09159143 -0.07876952  0.09757006  0.3989766   0.4163211 ]\n [-0.00142326  0.11773119 -0.01802916  0.00701718  0.2160982 ]\n [ 0.05106038  0.43594083  0.10706316 -0.3143753  -0.03666917]\n [-0.13739032  0.2963628   0.19150707 -0.1578434   0.01350684]\n [-0.3972718  -0.16593342  0.07809778  0.0919572   0.14640702]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09104202 -0.07823277  0.10300023  0.41373456  0.4295317 ]\n [-0.00606274  0.11258291 -0.01817476  0.01567459  0.22584097]\n [ 0.04395786  0.42865497  0.1036497  -0.31233612 -0.0330998 ]\n [-0.14354247  0.28809965  0.18500571 -0.15910268  0.01269611]\n [-0.4004103  -0.17279367  0.07294834  0.09334148  0.14663051]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09209226 -0.07525296  0.11073248  0.4251429   0.4388987 ]\n [-0.00757345  0.11753134 -0.00736186  0.03046227  0.24005881]\n [ 0.04442841  0.43554822  0.11431705 -0.30031386 -0.02162677]\n [-0.14518563  0.29193133  0.19137836 -0.14952514  0.02153913]\n [-0.40821669 -0.17570388  0.07362109  0.09791435  0.15024899]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 79] train=0.521795 val=0.634600 loss=66832.210144 time: 25.879912\n0\n\n[[-0.10640593 -0.08600028  0.10568961  0.42285663  0.43957186]\n [-0.0249176   0.10485023 -0.01297765  0.0284913   0.24176909]\n [ 0.03011989  0.42589757  0.10873931 -0.30362955 -0.02281341]\n [-0.15861338  0.284329    0.18594524 -0.15280724  0.01762016]\n [-0.42316142 -0.1851998   0.06691571  0.09484243  0.14405331]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.10448802 -0.08329802  0.10828116  0.4246443   0.44210944]\n [-0.02357539  0.10750675 -0.00989238  0.03049771  0.24452513]\n [ 0.03250326  0.42943722  0.11208225 -0.30100372 -0.01911902]\n [-0.1562932   0.28783384  0.18860044 -0.15006876  0.02147396]\n [-0.42196625 -0.18317582  0.06787009  0.09620896  0.14669338]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.1040474  -0.08318328  0.10812976  0.4239404   0.44070375]\n [-0.02329113  0.1074396  -0.0102247   0.02975429  0.24313454]\n [ 0.03246004  0.42933723  0.11171453 -0.30172658 -0.02040203]\n [-0.15649267  0.28769624  0.18819784 -0.15064788  0.02051614]\n [-0.42167816 -0.18288442  0.0677536   0.09601085  0.14628561]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.10484435 -0.08430777  0.10692553  0.42284322  0.43923253]\n [-0.02335759  0.10706653 -0.01086796  0.02899907  0.24197489]\n [ 0.03305674  0.42972243  0.11153077 -0.30237895 -0.02138373]\n [-0.15569809  0.2883948   0.18845741 -0.15088561  0.01983708]\n [-0.42094287 -0.18227969  0.06813011  0.09610971  0.14597389]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 80] train=0.534535 val=0.656300 loss=65220.545624 time: 24.469815\n0\n\n[[-0.10412484 -0.08422101  0.10644186  0.42203474  0.43820018]\n [-0.02340438  0.10637712 -0.01183482  0.02801977  0.24088654]\n [ 0.03264072  0.42917678  0.11075003 -0.30348206 -0.02283464]\n [-0.15610796  0.28825232  0.18824166 -0.15164305  0.01836278]\n [-0.4212604  -0.18255879  0.06772309  0.09504895  0.14409497]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.10414029 -0.08389223  0.10710106  0.42287558  0.43909982]\n [-0.0231006   0.10719638 -0.010656    0.02916576  0.24199823]\n [ 0.03336133  0.4304396   0.11222876 -0.3020658  -0.02149391]\n [-0.15517567  0.28961194  0.18979679 -0.14999954  0.02005087]\n [-0.420258   -0.1814387   0.0690919   0.09675499  0.14607354]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.10511861 -0.08491032  0.10636517  0.42186043  0.43753815]\n [-0.02416068  0.10638281 -0.01112266  0.02844707  0.24072312]\n [ 0.03197762  0.4295216   0.11170651 -0.30279845 -0.02280482]\n [-0.15643957  0.2886897   0.18918593 -0.15063444  0.01903144]\n [-0.42056224 -0.18180439  0.06875678  0.09631146  0.14521466]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.10563452 -0.08598115  0.10516827  0.4203684   0.4359573 ]\n [-0.02461682  0.10549373 -0.01230949  0.02705136  0.23930404]\n [ 0.03188516  0.4289665   0.11063666 -0.30405292 -0.02417841]\n [-0.1562789   0.28833497  0.18850446 -0.15149222  0.01771793]\n [-0.42019585 -0.18206377  0.06821208  0.0955008   0.14386201]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 81] train=0.547416 val=0.653800 loss=64055.835266 time: 25.532470\n0\n\n[[-0.10325115 -0.08378242  0.10739878  0.42279413  0.43804845]\n [-0.0221416   0.10796291 -0.00995275  0.02929583  0.24101095]\n [ 0.03434861  0.43148616  0.11290824 -0.3021456  -0.02290537]\n [-0.15377244  0.2908953   0.1909889  -0.14946732  0.01897816]\n [-0.4179691  -0.17988066  0.07037772  0.09731343  0.14498536]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.10380107 -0.08457517  0.10643853  0.4214223   0.43614453]\n [-0.02264178  0.107228   -0.01113598  0.02777468  0.2391659 ]\n [ 0.03349441  0.4305365   0.11149531 -0.3039072  -0.02480437]\n [-0.15498151  0.2895268   0.18911871 -0.15169783  0.01676039]\n [-0.41909343 -0.1814117   0.06810311  0.09461346  0.14249197]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.10362672 -0.0843196   0.10610522  0.42078966  0.435729  ]\n [-0.02223089  0.10769857 -0.01096646  0.02784424  0.23929349]\n [ 0.03462934  0.43167067  0.11229357 -0.30316687 -0.02411451]\n [-0.15332748  0.29116204  0.19021587 -0.15071236  0.01762627]\n [-0.41720936 -0.17970362  0.06912854  0.09545638  0.14329895]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.10401192 -0.08474802  0.10544141  0.4194454   0.43384516]\n [-0.02187357  0.10778505 -0.01140331  0.02680109  0.23797144]\n [ 0.0358285   0.4324226   0.11237778 -0.3035269  -0.02468271]\n [-0.15174913  0.2922678   0.19096416 -0.15033339  0.01747385]\n [-0.41566813 -0.17846905  0.07030275  0.09614502  0.14339964]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 82] train=0.545012 val=0.653100 loss=63739.427185 time: 28.877917\n0\n\n[[-0.10375196 -0.08454276  0.10600892  0.41980797  0.4343115 ]\n [-0.02199406  0.10779218 -0.01097308  0.02716034  0.23853022]\n [ 0.03529779  0.43218523  0.1125702  -0.30340353 -0.02436181]\n [-0.15230803  0.2920034   0.19116591 -0.15003778  0.01780581]\n [-0.4159893  -0.17869875  0.07053822  0.09661954  0.14391766]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.10524431 -0.08566576  0.10509171  0.41830266  0.43220994]\n [-0.02390015  0.10637883 -0.01215313  0.0255553   0.23666324]\n [ 0.03284512  0.43018788  0.11079777 -0.3052866  -0.02610115]\n [-0.15494242  0.28967616  0.18903416 -0.15227419  0.01586076]\n [-0.4184205  -0.18108854  0.06834427  0.09431294  0.14172104]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.10310406 -0.08371709  0.10685138  0.41944808  0.4327819 ]\n [-0.02263796  0.10758573 -0.01093415  0.02668609  0.23764536]\n [ 0.03339165  0.4307434   0.11166928 -0.30412182 -0.02486037]\n [-0.15450984  0.2901725   0.19000976 -0.150963    0.01738984]\n [-0.41717833 -0.17984083  0.06985153  0.0958613   0.14316013]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.10147043 -0.08233927  0.10799947  0.42043498  0.43330577]\n [-0.02099537  0.10909968 -0.00977674  0.02769623  0.2384278 ]\n [ 0.03494603  0.43247163  0.11288124 -0.30332088 -0.02426881]\n [-0.15305202  0.2916427   0.19096012 -0.15032847  0.0176457 ]\n [-0.41600302 -0.17868115  0.07044207  0.09603748  0.14279881]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 83] train=0.547196 val=0.651500 loss=63616.513855 time: 28.376855\n0\n\n[[-0.10118211 -0.08271075  0.10757857  0.42004347  0.4329584 ]\n [-0.02076515  0.10857429 -0.01056037  0.02699309  0.23783162]\n [ 0.03517937  0.43241176  0.1123457  -0.3040533  -0.02486539]\n [-0.15245761  0.29228228  0.19109401 -0.15063341  0.01713429]\n [-0.4149706  -0.1775445   0.07131322  0.09643145  0.14269045]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.10183725 -0.08326151  0.10712232  0.41938767  0.4322098 ]\n [-0.0214026   0.1079751  -0.01104758  0.02638725  0.23718375]\n [ 0.03467457  0.43211403  0.11203816 -0.30443543 -0.02507089]\n [-0.15275085  0.29213333  0.19097522 -0.15064436  0.01744484]\n [-0.41517818 -0.17775561  0.07131151  0.09656475  0.1428427 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.10048801 -0.08245075  0.10782761  0.41983917  0.43219236]\n [-0.02081928  0.10809144 -0.01110068  0.02600463  0.2365194 ]\n [ 0.03449608  0.43152314  0.11109246 -0.30564198 -0.02615523]\n [-0.15331519  0.29111424  0.1894063  -0.15225567  0.01631915]\n [-0.41607922 -0.17903118  0.06959207  0.09489699  0.14160691]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09885126 -0.08107381  0.10886842  0.42016706  0.43170115]\n [-0.01933653  0.10917019 -0.01026143  0.02632334  0.23626302]\n [ 0.03600223  0.4326491   0.11213738 -0.30502102 -0.02581931]\n [-0.15192398  0.2921362   0.19045624 -0.15139319  0.01713181]\n [-0.41476062 -0.17806484  0.07077321  0.09613802  0.14300242]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 84] train=0.549900 val=0.655900 loss=63251.390259 time: 28.171571\n0\n\n[[-0.09745904 -0.08026659  0.10915656  0.42036545  0.43233666]\n [-0.01843093  0.10948292 -0.01026514  0.02625887  0.23679225]\n [ 0.03662619  0.43260923  0.11222362 -0.30485827 -0.02510152]\n [-0.15101369  0.29244897  0.19096893 -0.15089794  0.01781822]\n [-0.41317618 -0.17697966  0.07188903  0.09693891  0.14359003]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09866768 -0.08170144  0.10774048  0.4191748   0.43160328]\n [-0.02012928  0.10758491 -0.01179564  0.02528325  0.23643811]\n [ 0.03445178  0.43031946  0.11032823 -0.30588916 -0.02521325]\n [-0.15361573  0.28982177  0.18888365 -0.151882    0.01784631]\n [-0.41617    -0.1799455   0.06962068  0.09567184  0.14314942]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09937676 -0.0817856   0.10797445  0.41948545  0.4318529 ]\n [-0.02085813  0.10753386 -0.01164355  0.0254479   0.23668897]\n [ 0.03368126  0.43000072  0.11008535 -0.30626374 -0.02539491]\n [-0.1542699   0.28935748  0.18848664 -0.15238309  0.01737212]\n [-0.41631892 -0.17999548  0.06948074  0.09547228  0.1428107 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09955325 -0.08208541  0.1074418   0.41880533  0.43106067]\n [-0.02102968  0.10723472 -0.01206382  0.02489533  0.2360501 ]\n [ 0.03378743  0.429948    0.10999073 -0.30662546 -0.02579716]\n [-0.15373784  0.28972566  0.1887639  -0.15236206  0.01745161]\n [-0.4156257  -0.17956501  0.06987984  0.09581628  0.14313549]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 85] train=0.548638 val=0.656500 loss=63193.580017 time: 27.017652\n0\n\n[[-0.09917029 -0.08207211  0.1076619   0.41970396  0.4320114 ]\n [-0.02092505  0.10716601 -0.01202991  0.0253644   0.23667246]\n [ 0.03401656  0.4300309   0.10998455 -0.3063446  -0.02522322]\n [-0.15337165  0.2898502   0.18876892 -0.1520444   0.01805845]\n [-0.41537783 -0.17939688  0.07006088  0.09625924  0.14368224]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09865654 -0.08162928  0.10844818  0.42149645  0.43440017]\n [-0.0208353   0.10749234 -0.01127217  0.02700596  0.2393125 ]\n [ 0.03340923  0.4301032   0.11068478 -0.30509117 -0.02308362]\n [-0.15441881  0.28956756  0.1893564  -0.15108816  0.01954024]\n [-0.41659787 -0.18001664  0.07021506  0.09670886  0.14453132]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09930883 -0.08209047  0.10826914  0.4207928   0.43305033]\n [-0.02189075  0.10670781 -0.01149754  0.02654403  0.23842758]\n [ 0.03201539  0.42913973  0.11039001 -0.30512437 -0.02325875]\n [-0.15539564  0.28874168  0.18906222 -0.1509372   0.01951537]\n [-0.41695535 -0.18064244  0.06968994  0.09647065  0.14419147]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09994788 -0.08259079  0.10775233  0.42017055  0.43197575]\n [-0.0228194   0.10579412 -0.01219692  0.02600715  0.23755583]\n [ 0.03139941  0.4284668   0.10986381 -0.3055732  -0.02411139]\n [-0.15524808  0.28863984  0.18902065 -0.15096402  0.01896969]\n [-0.41643775 -0.18053138  0.06974661  0.09636337  0.14345898]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 86] train=0.547376 val=0.654100 loss=63479.905518 time: 27.927020\n0\n\n[[-0.09912886 -0.08240648  0.1072925   0.41996434  0.43205076]\n [-0.02240044  0.1055369  -0.01285728  0.02579021  0.23779549]\n [ 0.03169478  0.42834088  0.10972565 -0.30552462 -0.02392417]\n [-0.15514736  0.28864148  0.18922572 -0.15078746  0.01888157]\n [-0.4165231  -0.1806905   0.06951843  0.09595644  0.14250217]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09642716 -0.07959256  0.1099129   0.4220496   0.4338065 ]\n [-0.02054482  0.10741439 -0.01129202  0.02703722  0.23899502]\n [ 0.03315807  0.42987484  0.11081837 -0.30492058 -0.02316719]\n [-0.15369603  0.29007575  0.19035879 -0.15024173  0.01957136]\n [-0.41476122 -0.17908779  0.07077394  0.09678923  0.1432786 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09624032 -0.07979513  0.10939412  0.42154458  0.43287098]\n [-0.02053787  0.10713014 -0.01210853  0.02634131  0.23794706]\n [ 0.03343055  0.4298839   0.10998856 -0.3057185  -0.0242288 ]\n [-0.15301923  0.29074898  0.19018158 -0.15073101  0.01847316]\n [-0.41389072 -0.17809783  0.07111799  0.09680639  0.14250466]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09503783 -0.0785376   0.11020524  0.4221386   0.43330088]\n [-0.01938019  0.10852846 -0.01106358  0.027329    0.23898242]\n [ 0.0343185   0.43136615  0.11133356 -0.30445966 -0.0228336 ]\n [-0.15189327  0.29252917  0.19169432 -0.14940815  0.01997617]\n [-0.41212532 -0.1761012   0.07249512  0.09785099  0.14385803]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 87] train=0.546915 val=0.654500 loss=63531.094025 time: 28.292568\n0\n\n[[-0.09577602 -0.07938122  0.10935297  0.42118266  0.4319167 ]\n [-0.02063148  0.10718145 -0.01237729  0.02611521  0.23736128]\n [ 0.03234047  0.42924234  0.1092144  -0.30634135 -0.02498556]\n [-0.1543691   0.28988364  0.18905216 -0.15194657  0.01734212]\n [-0.4144842  -0.17872187  0.06970339  0.09513056  0.14111575]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09476754 -0.07845049  0.11030091  0.42214558  0.4330136 ]\n [-0.01993661  0.10787567 -0.0116589   0.02675343  0.23817994]\n [ 0.03274199  0.43001613  0.1099694  -0.30571654 -0.02397961]\n [-0.15443365  0.29042786  0.18943061 -0.15140991  0.01826189]\n [-0.41512245 -0.17898321  0.06919908  0.09504508  0.14119323]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09443792 -0.07835051  0.11021286  0.4217812   0.43244493]\n [-0.01951256  0.10818167 -0.01133176  0.02690682  0.23812091]\n [ 0.03368209  0.43097273  0.11081745 -0.3050896  -0.02359811]\n [-0.1529368   0.29187465  0.19069558 -0.1502114   0.01915179]\n [-0.41303635 -0.17688993  0.07106166  0.09692957  0.14271052]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0940092  -0.07792117  0.1108754   0.42228943  0.4323125 ]\n [-0.01936161  0.10839073 -0.0111608   0.02713579  0.23806909]\n [ 0.03321435  0.43084812  0.11053181 -0.30538788 -0.02418223]\n [-0.15330537  0.29176778  0.1905398  -0.15031695  0.01854467]\n [-0.41292402 -0.1766463   0.07140989  0.09740442  0.14264897]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 88] train=0.551382 val=0.658500 loss=63345.485107 time: 29.295849\n0\n\n[[-0.09496575 -0.07865769  0.11041112  0.42208967  0.4324279 ]\n [-0.02002535  0.10775458 -0.01162999  0.02726397  0.2387908 ]\n [ 0.03274248  0.43048674  0.11035863 -0.30501848 -0.02333362]\n [-0.15363455  0.29160675  0.19076242 -0.14963451  0.01945405]\n [-0.41287175 -0.17668225  0.07146138  0.09789927  0.14325777]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09558914 -0.07958491  0.10932424  0.42137048  0.43162015]\n [-0.02111064  0.10649893 -0.01315123  0.02612328  0.2377506 ]\n [ 0.03174546  0.42942172  0.10878549 -0.30652514 -0.02457888]\n [-0.15473579  0.2904257   0.1891998  -0.15132976  0.01813192]\n [-0.41408664 -0.17816018  0.06984315  0.09632718  0.1420196 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09576003 -0.07963828  0.10924032  0.4210331   0.43075377]\n [-0.0211678   0.10678928 -0.01288693  0.02598017  0.23719473]\n [ 0.03189532  0.4301723   0.10957213 -0.3064382  -0.02468171]\n [-0.15412638  0.29141304  0.19014473 -0.15097389  0.01843574]\n [-0.4132368  -0.17717712  0.07083604  0.09676401  0.14234585]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09711895 -0.08095241  0.10822623  0.42035446  0.43004343]\n [-0.02269764  0.10538398 -0.01358742  0.02593659  0.23726492]\n [ 0.03063544  0.42920637  0.10932328 -0.3059925  -0.02390181]\n [-0.15476727  0.2910739   0.19033186 -0.15021686  0.01964122]\n [-0.41361803 -0.17727628  0.07113078  0.0974885   0.14324684]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 89] train=0.548918 val=0.657000 loss=63258.750427 time: 28.727416\n0\n\n[[-0.09548831 -0.0795892   0.10926118  0.42083812  0.4298266 ]\n [-0.0217086   0.10609239 -0.0130811   0.0260719   0.23709872]\n [ 0.03098397  0.42921597  0.10923703 -0.30614892 -0.02385156]\n [-0.15445758  0.290952    0.19011354 -0.15043487  0.01970217]\n [-0.41293392 -0.17722908  0.07115971  0.09769938  0.1436427 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09631389 -0.08086672  0.10787155  0.41928613  0.42792726]\n [-0.02266883  0.10450353 -0.01494296  0.02424802  0.23541212]\n [ 0.0298386   0.42772177  0.10757402 -0.30782846 -0.02526236]\n [-0.15537548  0.28987792  0.18920879 -0.15144095  0.01885222]\n [-0.4133148  -0.1779908   0.07045679  0.09693459  0.14284502]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09560359 -0.0802691   0.10847642  0.42029545  0.42903808]\n [-0.02138282  0.10554621 -0.01430816  0.02510293  0.23661311]\n [ 0.03184362  0.429444    0.1085901  -0.30701163 -0.02405148]\n [-0.15337868  0.29161412  0.19041383 -0.15048349  0.02004652]\n [-0.41163912 -0.17661633  0.07156715  0.09772824  0.14353995]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09600148 -0.08092721  0.10794982  0.42021677  0.42882666]\n [-0.02132996  0.10562406 -0.01397637  0.02601885  0.23752102]\n [ 0.03220276  0.43003443  0.10933161 -0.30592772 -0.02292038]\n [-0.15313761  0.2918214   0.19062933 -0.1498183   0.02089084]\n [-0.4118407  -0.1770243   0.0711306   0.09775471  0.14395514]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 90] train=0.550000 val=0.655400 loss=63388.843079 time: 26.175874\n0\n\n[[-0.09599723 -0.08099305  0.10804471  0.4207372   0.4297335 ]\n [-0.02128218  0.10567965 -0.01407827  0.02619118  0.23842683]\n [ 0.03213229  0.43005872  0.1090518  -0.30616242 -0.02225972]\n [-0.15332395  0.2915997   0.19024737 -0.1501518   0.0213025 ]\n [-0.41204453 -0.17726502  0.07084946  0.09760804  0.14419714]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09629066 -0.08124064  0.10776654  0.4207978   0.42971867]\n [-0.02177271  0.10515767 -0.01450097  0.02619214  0.23855819]\n [ 0.0311048   0.42901942  0.1082132  -0.30650005 -0.02232395]\n [-0.15490417  0.2900477   0.18892658 -0.15105103  0.02056384]\n [-0.4136478  -0.1789327   0.06920174  0.09621532  0.14280054]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.0955135  -0.08069093  0.10877397  0.42247325  0.4318232 ]\n [-0.02109431  0.10571238 -0.01362687  0.0276036   0.24035081]\n [ 0.03194254  0.42985284  0.10914613 -0.30532873 -0.02095296]\n [-0.15381272  0.29118183  0.18998663 -0.14996807  0.02180983]\n [-0.41255093 -0.17789611  0.070051    0.09696867  0.1438691 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09564701 -0.08166714  0.10738283  0.42100373  0.43083486]\n [-0.02116623  0.10474374 -0.01524099  0.02577334  0.23897927]\n [ 0.03222243  0.42940766  0.1079518  -0.30706918 -0.02243489]\n [-0.1531322   0.29142594  0.1897365  -0.15084271  0.02100551]\n [-0.41190353 -0.17750923  0.07009871  0.09660231  0.14356808]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 91] train=0.548017 val=0.656800 loss=63401.047577 time: 29.000618\n0\n\n[[-0.09459689 -0.08020147  0.10904752  0.42271885  0.4325573 ]\n [-0.02081856  0.10547522 -0.01395361  0.02743161  0.24076378]\n [ 0.03183813  0.4295729   0.10908126 -0.30527365 -0.02060224]\n [-0.15325414  0.2918699   0.1910929  -0.148991    0.02257834]\n [-0.41153762 -0.17671517  0.07152017  0.09840426  0.14495124]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09450208 -0.08103982  0.10781761  0.4216665   0.4313429 ]\n [-0.02079467  0.10465909 -0.01500956  0.02672398  0.24007274]\n [ 0.03193681  0.4291409   0.1084695  -0.3057386  -0.02102643]\n [-0.15291837  0.29179966  0.19086143 -0.14922442  0.02221691]\n [-0.41093627 -0.1766077   0.07157495  0.09874132  0.14514756]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09340709 -0.08077327  0.10755973  0.42107984  0.4307185 ]\n [-0.01992459  0.10477818 -0.01533702  0.02612229  0.23947059]\n [ 0.0329324   0.42962506  0.10838518 -0.3063018  -0.02165494]\n [-0.15166366  0.29272747  0.1913154  -0.14914179  0.02214263]\n [-0.4096029  -0.17552541  0.07243793  0.09959777  0.14593023]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09349512 -0.08064286  0.10784376  0.4210191   0.43061957]\n [-0.02030532  0.10471285 -0.01537666  0.02568867  0.23925257]\n [ 0.03270897  0.4294414   0.10797619 -0.3070132  -0.02192944]\n [-0.15241233  0.29187086  0.1903995  -0.15026402  0.0212412 ]\n [-0.41070756 -0.17680122  0.07136233  0.09844775  0.14465109]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 92] train=0.550801 val=0.659100 loss=62950.559814 time: 28.275501\n0\n\n[[-0.09463737 -0.08167274  0.10669104  0.4198808   0.4295972 ]\n [-0.0216551   0.10352388 -0.0165643   0.02468156  0.23869981]\n [ 0.03160359  0.4287613   0.10715444 -0.30803734 -0.02246026]\n [-0.1532553   0.29133338  0.18982948 -0.15081884  0.02094846]\n [-0.41126046 -0.17739104  0.07100822  0.09832177  0.14451955]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09442477 -0.08164759  0.10634488  0.41928944  0.42888498]\n [-0.02176411  0.10318051 -0.01715939  0.02399464  0.23832723]\n [ 0.03161446  0.42851242  0.10669619 -0.30849975 -0.0224625 ]\n [-0.15241952  0.2917028   0.18986838 -0.15065698  0.02138122]\n [-0.40991032 -0.17660265  0.07140214  0.09883466  0.14491825]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09330466 -0.08086714  0.10683869  0.41992     0.42982483]\n [-0.02177337  0.10287055 -0.01758307  0.02396196  0.23903546]\n [ 0.0305453   0.4272874   0.10542377 -0.30945477 -0.02249674]\n [-0.15366922  0.29027027  0.18825878 -0.15203762  0.02071453]\n [-0.41134682 -0.17832963  0.06938233  0.09697222  0.14339934]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09289847 -0.08022064  0.10762083  0.42104387  0.43075424]\n [-0.02137328  0.10348476 -0.01672303  0.0250936   0.23994845]\n [ 0.03114212  0.42787308  0.1061312  -0.3087602  -0.02203248]\n [-0.15260737  0.29087824  0.18883601 -0.15151845  0.021035  ]\n [-0.40993488 -0.17747895  0.07018667  0.09779426  0.14400773]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 93] train=0.549900 val=0.659600 loss=63095.048706 time: 28.436451\n0\n\n[[-0.09284264 -0.08024186  0.10745771  0.42052013  0.42973703]\n [-0.0208729   0.1036252  -0.01670367  0.02476162  0.23911804]\n [ 0.03185532  0.42814904  0.10641161 -0.3088912  -0.02238882]\n [-0.15193668  0.29129133  0.18944387 -0.15108533  0.02135582]\n [-0.40920112 -0.17691202  0.07080983  0.09829523  0.14441775]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09260347 -0.08003156  0.10730589  0.42038137  0.4293332 ]\n [-0.02026862  0.10418243 -0.01651579  0.02477919  0.23880011]\n [ 0.03211159  0.42849112  0.10671096 -0.3089175  -0.02296444]\n [-0.15194571  0.29152474  0.18981592 -0.15094694  0.0210746 ]\n [-0.40896827 -0.17650706  0.07122103  0.09883261  0.14476646]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09283959 -0.08148632  0.10534479  0.41877794  0.42863268]\n [-0.02013044  0.10336679 -0.01807383  0.02365915  0.23874053]\n [ 0.03215612  0.4281447   0.10584067 -0.30945683 -0.02253276]\n [-0.15222935  0.29134807  0.18949667 -0.15087277  0.02187935]\n [-0.40912092 -0.17621996  0.0714898   0.09942278  0.1458456 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09164701 -0.08062226  0.10629337  0.42030162  0.43016177]\n [-0.0191996   0.1040022  -0.01734514  0.02518995  0.24063571]\n [ 0.03247746  0.42819104  0.10594548 -0.3085541  -0.02072488]\n [-0.15227166  0.29092157  0.18901274 -0.15070248  0.02299451]\n [-0.40924937 -0.17661788  0.07102527  0.09939534  0.14645149]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 94] train=0.552063 val=0.658600 loss=63102.414429 time: 30.557740\n0\n\n[[-0.09210415 -0.08115989  0.10619709  0.4203957   0.4299822 ]\n [-0.01965045  0.10357206 -0.01744224  0.0252422   0.24056986]\n [ 0.03187959  0.42786482  0.10593653 -0.3086365  -0.02073795]\n [-0.15295672  0.29051593  0.18894447 -0.15080366  0.02306515]\n [-0.40958354 -0.17660573  0.07136446  0.0999045   0.14689697]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09179377 -0.08163729  0.10553663  0.41961148  0.42872143]\n [-0.01925185  0.10342672 -0.01766361  0.02484547  0.23972133]\n [ 0.03220663  0.42787576  0.10597672 -0.30889297 -0.02134002]\n [-0.15269105  0.29052     0.1888696  -0.15108536  0.02249917]\n [-0.4090352  -0.1762698   0.07147394  0.09959032  0.14608306]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09248102 -0.08291277  0.10427292  0.4187115   0.42796397]\n [-0.02043965  0.10200379 -0.01893079  0.02400914  0.23899928]\n [ 0.03100973  0.4267092   0.10486676 -0.3097399  -0.02204827]\n [-0.1535488   0.28975123  0.18821129 -0.15154271  0.02201094]\n [-0.40972763 -0.17670679  0.07124382  0.09957153  0.1457465 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09231228 -0.0822188   0.1054062   0.42017844  0.42958942]\n [-0.0203536   0.10252361 -0.01803096  0.0252983   0.24064733]\n [ 0.03119146  0.42711675  0.10558085 -0.30863494 -0.02045175]\n [-0.15319481  0.29001394  0.18880734 -0.15055043  0.02365192]\n [-0.40943757 -0.176658    0.07169324  0.10048232  0.14710684]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 95] train=0.552644 val=0.662600 loss=63097.536469 time: 35.467014\n0\n\n[[-0.09326213 -0.082552    0.1049062   0.41970837  0.4293799 ]\n [-0.02114936  0.10224095 -0.01870813  0.02454131  0.24023493]\n [ 0.03042169  0.42664015  0.10480972 -0.30962253 -0.02120242]\n [-0.15387909  0.28923848  0.18796457 -0.15153478  0.02271996]\n [-0.41020587 -0.17757834  0.0708659   0.09952561  0.14612074]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09468298 -0.08413982  0.10333488  0.41867882  0.42843986]\n [-0.02224057  0.10099489 -0.02018532  0.02339565  0.23928832]\n [ 0.03027437  0.42636204  0.10413983 -0.31047556 -0.02207485]\n [-0.15352602  0.2893083   0.18775228 -0.15204951  0.02194953]\n [-0.40982434 -0.17741697  0.07057746  0.09856401  0.14479046]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09442356 -0.08334967  0.10497829  0.4205111   0.43009254]\n [-0.02235537  0.10146181 -0.01869521  0.02549109  0.24128407]\n [ 0.02976372  0.42638856  0.10517782 -0.3084683  -0.01970823]\n [-0.15415792  0.28900054  0.1884201  -0.15042861  0.02424052]\n [-0.41011167 -0.17739129  0.07139191  0.09978686  0.14640607]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09398414 -0.08296778  0.10538232  0.42120662  0.43030855]\n [-0.02193602  0.10173255 -0.01853066  0.02583138  0.24120793]\n [ 0.03027839  0.42680988  0.10525038 -0.30860814 -0.02031694]\n [-0.15391524  0.28910357  0.18815689 -0.15104584  0.023213  ]\n [-0.41018188 -0.17771529  0.0705439   0.09859697  0.14494519]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 96] train=0.552444 val=0.661900 loss=62864.761627 time: 29.519628\n0\n\n[[-0.09534872 -0.08456773  0.10381849  0.41952333  0.4285097 ]\n [-0.02287057  0.10064458 -0.01970129  0.02440932  0.2398082 ]\n [ 0.02996864  0.42644227  0.10450358 -0.30986053 -0.02120084]\n [-0.15380855  0.28911963  0.1877845  -0.15176372  0.0230378 ]\n [-0.40955585 -0.17714137  0.0707199   0.09849541  0.14530784]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09387929 -0.08357564  0.10497735  0.4207658   0.42958248]\n [-0.02143014  0.10179842 -0.01822932  0.02609302  0.24124344]\n [ 0.03147887  0.4278251   0.10605211 -0.30808944 -0.01973306]\n [-0.15199782  0.2907874   0.18935637 -0.15019497  0.0241319 ]\n [-0.40738192 -0.17513794  0.07245404  0.10010007  0.14635177]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09395708 -0.08355192  0.10520259  0.42143667  0.4300604 ]\n [-0.02150276  0.10179455 -0.01825553  0.0265323   0.24175791]\n [ 0.03118073  0.42761365  0.10580368 -0.30820388 -0.01963561]\n [-0.15269463  0.290367    0.18921292 -0.15046719  0.02384976]\n [-0.40861845 -0.17602894  0.07194302  0.09958297  0.14576197]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09488902 -0.08554401  0.10329146  0.4199652   0.42867237]\n [-0.02228973  0.09995882 -0.02017053  0.02466215  0.23989452]\n [ 0.03064871  0.4262401   0.10422131 -0.31002188 -0.02157528]\n [-0.15305667  0.28952277  0.18814099 -0.15191671  0.02217763]\n [-0.40855613 -0.17646155  0.07162345  0.09907012  0.14477345]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 97] train=0.549079 val=0.659600 loss=62920.324371 time: 30.877436\n0\n\n[[-0.0946946  -0.08531696  0.1034126   0.42002824  0.42879772]\n [-0.02195854  0.10036165 -0.0198229   0.02500943  0.24017112]\n [ 0.03088136  0.4267855   0.1048351  -0.3093823  -0.02108707]\n [-0.15262091  0.29036236  0.1891744  -0.1510033   0.02278945]\n [-0.4077956  -0.17533769  0.07291385  0.10016938  0.14544001]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09273709 -0.08374613  0.10428709  0.42072675  0.42995954]\n [-0.02004     0.10210013 -0.01844899  0.02616662  0.24145003]\n [ 0.03284948  0.42875612  0.10663719 -0.3078394  -0.01974932]\n [-0.15051174  0.29255444  0.19125204 -0.14927848  0.02397687]\n [-0.4056108  -0.17295437  0.07527152  0.10220104  0.14680159]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09266637 -0.08352903  0.10442207  0.42057928  0.4294378 ]\n [-0.02058757  0.1017636  -0.01889143  0.02574262  0.24098568]\n [ 0.03216811  0.4285102   0.10605133 -0.30855656 -0.02021283]\n [-0.15097119  0.2925503   0.19093347 -0.14993982  0.02336584]\n [-0.40591022 -0.17298141  0.07507952  0.1018007   0.1463917 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09430441 -0.08526596  0.10272544  0.4190087   0.42860675]\n [-0.02233256  0.10009682 -0.02049076  0.02453391  0.24074039]\n [ 0.03067867  0.42715603  0.10466257 -0.30973873 -0.02038161]\n [-0.1524003   0.29127762  0.18962213 -0.15115754  0.02302492]\n [-0.40749776 -0.17436506  0.07393782  0.10105836  0.14620304]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 98] train=0.553666 val=0.661200 loss=62969.992859 time: 28.468994\n0\n\n[[-0.0927899  -0.08398327  0.10370741  0.41964546  0.42896265]\n [-0.02152458  0.10067427 -0.02013482  0.02474842  0.2407106 ]\n [ 0.03121186  0.4274523   0.10466116 -0.30991027 -0.02076286]\n [-0.15200529  0.29146656  0.18947172 -0.15132964  0.02265627]\n [-0.4072233  -0.1743811   0.07365327  0.10110743  0.14612703]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09341683 -0.08499222  0.10241147  0.41825673  0.42752847]\n [-0.02214921  0.09996256 -0.02100497  0.02382284  0.24003124]\n [ 0.03027512  0.42682433  0.10415859 -0.31023747 -0.02062358]\n [-0.15324105  0.2906994   0.1890789  -0.1513975   0.02297524]\n [-0.408602   -0.17543067  0.07304448  0.10080274  0.14609651]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09269241 -0.08475708  0.10280859  0.41913867  0.4287105 ]\n [-0.02191219  0.09950921 -0.02162382  0.023814    0.24060796]\n [ 0.03026828  0.42635784  0.10326178 -0.31076825 -0.02047358]\n [-0.15324818  0.29026598  0.18830088 -0.15202448  0.02285082]\n [-0.40851852 -0.17597038  0.07215467  0.09985877  0.14562398]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09345888 -0.08588558  0.10184211  0.41831982  0.4277029 ]\n [-0.02286303  0.09822463 -0.02293083  0.02275878  0.23946671]\n [ 0.02955914  0.4253033   0.10209479 -0.31200904 -0.02178753]\n [-0.15366696  0.28923538  0.18705755 -0.1535656   0.02120954]\n [-0.40909162 -0.17711344  0.07042789  0.09780712  0.14352368]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 99] train=0.554627 val=0.658900 loss=62497.599396 time: 24.772876\n0\n\n[[-0.09337547 -0.08564444  0.10215493  0.41800743  0.4266698 ]\n [-0.02249714  0.09863405 -0.02228715  0.02298747  0.23885986]\n [ 0.02996001  0.4256861   0.10260309 -0.31190273 -0.02236728]\n [-0.15319917  0.28955048  0.18732263 -0.15385826  0.02018826]\n [-0.40855905 -0.1769409   0.07021595  0.09689537  0.14196678]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09133798 -0.08398356  0.10367049  0.41909608  0.42753106]\n [-0.02008246  0.10082456 -0.02012995  0.02488187  0.24055275]\n [ 0.03229012  0.42795315  0.10493619 -0.30965915 -0.02017842]\n [-0.15117674  0.2917084   0.18969469 -0.1516795   0.02225632]\n [-0.40647167 -0.17497164  0.07234731  0.09899956  0.14396706]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09221677 -0.08500348  0.10299078  0.41885656  0.42760003]\n [-0.0208024   0.09988898 -0.02080341  0.02466849  0.2407157 ]\n [ 0.03178421  0.4273094   0.10429946 -0.31001747 -0.02000292]\n [-0.15190506  0.2908851   0.18896803 -0.1521516   0.02220573]\n [-0.40737635 -0.17607594  0.07129432  0.09833138  0.14363164]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09367605 -0.08641717  0.10206761  0.4186125   0.42749962]\n [-0.02173833  0.09879979 -0.02170459  0.02438505  0.2405613 ]\n [ 0.03092759  0.4267075   0.10376825 -0.31021106 -0.02012076]\n [-0.15291698  0.29033262  0.18851571 -0.15243404  0.02185084]\n [-0.40872183 -0.17709722  0.07030527  0.09761761  0.14298517]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 100] train=0.554908 val=0.657400 loss=62808.283630 time: 26.461401\n0\n\n[[-0.09212579 -0.085613    0.10313885  0.4200655   0.428759  ]\n [-0.02011731  0.09947857 -0.0208323   0.02552278  0.24149562]\n [ 0.03265138  0.42772433  0.10487963 -0.30923614 -0.0193592 ]\n [-0.15160827  0.29126865  0.1895666  -0.15163672  0.02252935]\n [-0.4077396  -0.17642647  0.07112365  0.09839142  0.1437398 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09041276 -0.08443525  0.1040215   0.42053625  0.42860913]\n [-0.01880961  0.10022517 -0.02041029  0.02569192  0.24130093]\n [ 0.03354341  0.42814222  0.1048952  -0.30939412 -0.01954584]\n [-0.15068786  0.29182336  0.18949477 -0.15201288  0.02233369]\n [-0.40680215 -0.17591514  0.0709922   0.09787896  0.1432122 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09052288 -0.08446989  0.10385315  0.4206583   0.42889148]\n [-0.0187849   0.10014495 -0.02057709  0.02617533  0.24211255]\n [ 0.03341686  0.42804343  0.10457952 -0.3088541  -0.0183732 ]\n [-0.1511812   0.29139477  0.18895307 -0.15180507  0.02320687]\n [-0.4077195  -0.17673948  0.07018805  0.0978413   0.14345074]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0905652  -0.08442873  0.10400455  0.42080936  0.42893368]\n [-0.01780319  0.10091666 -0.02021337  0.02621027  0.24215364]\n [ 0.03481869  0.4291498   0.10484944 -0.30894437 -0.01823701]\n [-0.15002428  0.2922109   0.18905114 -0.1520997   0.02303131]\n [-0.40692002 -0.17636007  0.07000925  0.09729154  0.14285703]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 101] train=0.555629 val=0.657600 loss=62549.911591 time: 24.582873\n0\n\n[[-0.09073236 -0.08480447  0.10361416  0.4201622   0.42814496]\n [-0.0186758   0.09996121 -0.02096211  0.02538994  0.24128377]\n [ 0.03315789  0.42756054  0.10356537 -0.31004265 -0.01909821]\n [-0.1519205   0.29055503  0.18768312 -0.15324025  0.02214727]\n [-0.40839738 -0.17779464  0.06877883  0.09613378  0.14188656]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09022954 -0.08463446  0.10355586  0.41996688  0.4274953 ]\n [-0.01793537  0.10064542 -0.02019818  0.0260931   0.24176088]\n [ 0.03426537  0.42874205  0.10478716 -0.3088679  -0.01798665]\n [-0.15068682  0.29172316  0.18873563 -0.15222849  0.02316106]\n [-0.40653667 -0.17623764  0.07003606  0.09742791  0.14312959]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09004017 -0.08448231  0.1040654   0.420679    0.42778817]\n [-0.01788524  0.1005471  -0.02013828  0.02635948  0.2420199 ]\n [ 0.03413448  0.42861336  0.10477398 -0.30888858 -0.01783849]\n [-0.15106153  0.29149842  0.18871954 -0.15220723  0.02329427]\n [-0.40730703 -0.17681034  0.06970681  0.0975247   0.14330776]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09201922 -0.08599082  0.10303164  0.41974327  0.42715734]\n [-0.01925481  0.09943497 -0.0209373   0.02567746  0.24170114]\n [ 0.03331577  0.42792106  0.10438378 -0.3091891  -0.01779186]\n [-0.15116481  0.29126135  0.18878211 -0.151946    0.02384316]\n [-0.40687728 -0.1768622   0.06975009  0.09794915  0.14414217]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 102] train=0.552204 val=0.661800 loss=63045.865631 time: 27.732281\n0\n\n[[-0.09145082 -0.08562652  0.10322297  0.41937298  0.42586502]\n [-0.01879381  0.09989914 -0.02048891  0.02571687  0.24105768]\n [ 0.03339771  0.4279989   0.10460865 -0.3091766  -0.01821909]\n [-0.15110607  0.2911566   0.18882574 -0.15203089  0.0232022 ]\n [-0.40687573 -0.17721729  0.06929184  0.09737447  0.14300475]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08955178 -0.08415417  0.10444526  0.42075238  0.4272881 ]\n [-0.01777808  0.1008449  -0.01963243  0.02672001  0.24193245]\n [ 0.0337764   0.42853275  0.1050839  -0.30847842 -0.01764812]\n [-0.15078908  0.29158002  0.18918665 -0.15162583  0.02315385]\n [-0.40632468 -0.1766705   0.06970498  0.09752517  0.14252351]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09000042 -0.08500452  0.10338037  0.42018133  0.42683712]\n [-0.01813045  0.1001725  -0.02067471  0.0258815   0.24128683]\n [ 0.03330763  0.4281083   0.10438027 -0.30928937 -0.01829201]\n [-0.15120026  0.29113725  0.18861514 -0.15210222  0.0227614 ]\n [-0.40671647 -0.17721485  0.06914306  0.0973131   0.14257249]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09092966 -0.08658496  0.10123255  0.4177577   0.423718  ]\n [-0.01887503  0.09887283 -0.02261716  0.0236613   0.23857173]\n [ 0.03247816  0.427267    0.10311047 -0.3109308  -0.02018499]\n [-0.15196265  0.2905826   0.18806781 -0.15290208  0.02172712]\n [-0.40730858 -0.17757441  0.06895404  0.09701728  0.14215711]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 103] train=0.551242 val=0.657400 loss=62636.869965 time: 25.781955\n0\n\n[[-0.08916993 -0.08510097  0.1022698   0.41846362  0.42392626]\n [-0.01695647  0.10037064 -0.02146246  0.02458816  0.2391987 ]\n [ 0.03452035  0.42902935  0.10452759 -0.3097948  -0.01921272]\n [-0.14997058  0.29236543  0.18976597 -0.15134832  0.02302326]\n [-0.4053974  -0.1758623   0.07073048  0.09872516  0.14358668]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08998345 -0.08567387  0.10205365  0.41872805  0.42448202]\n [-0.01800446  0.09957092 -0.02187473  0.02464665  0.23961657]\n [ 0.03299021  0.4278682   0.10378299 -0.3100802  -0.01890383]\n [-0.15160711  0.29100585  0.18889396 -0.15175647  0.02322897]\n [-0.4064141  -0.17661518  0.07043882  0.09872241  0.14392553]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08922952 -0.08468942  0.10291927  0.41961083  0.42513072]\n [-0.0168161   0.1008429  -0.02100124  0.02579231  0.24042727]\n [ 0.03412801  0.4291292   0.10461096 -0.30882418 -0.01786635]\n [-0.15037869  0.292125    0.18971747 -0.1504527   0.02424356]\n [-0.4048405  -0.17526014  0.07149588  0.09996054  0.14478746]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08983463 -0.08544562  0.10198751  0.41848505  0.42368934]\n [-0.01815995  0.09970995 -0.02216893  0.02468335  0.23902759]\n [ 0.03240294  0.42784855  0.10347802 -0.30991217 -0.01902239]\n [-0.15236823  0.29060185  0.18854514 -0.15152283  0.02314886]\n [-0.40661    -0.17685877  0.07021467  0.09896147  0.143749  ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 104] train=0.555970 val=0.661100 loss=62456.466339 time: 29.716591\n0\n\n[[-0.08913987 -0.08490436  0.10222213  0.41850436  0.42407387]\n [-0.01782251  0.10002393 -0.02198697  0.02479786  0.23952694]\n [ 0.03245845  0.42795002  0.10356347 -0.31012458 -0.0188353 ]\n [-0.15265498  0.29044175  0.18854594 -0.15191928  0.02292247]\n [-0.4073884  -0.17757809  0.0696732   0.09813713  0.14302006]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08875621 -0.08443427  0.10260259  0.41901764  0.42435694]\n [-0.01709613  0.10088149 -0.02147019  0.02519894  0.2396455 ]\n [ 0.0335919   0.42930618  0.10454544 -0.309567   -0.018504  ]\n [-0.15116763  0.2921598   0.19027273 -0.15060936  0.02364214]\n [-0.40568373 -0.17557003  0.07202101  0.10032474  0.14442101]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09003793 -0.08554046  0.10195936  0.41884798  0.42400324]\n [-0.01821048  0.1000079  -0.02210977  0.02497838  0.23926397]\n [ 0.03282595  0.42881143  0.10388425 -0.31028563 -0.01930096]\n [-0.15162252  0.29188287  0.18984276 -0.15112908  0.02280299]\n [-0.40603384 -0.17581995  0.07170662  0.10003323  0.14379607]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.09044264 -0.08643084  0.10101882  0.417744    0.4226995 ]\n [-0.01934804  0.0981891  -0.02382365  0.0235517   0.2378185 ]\n [ 0.03109229  0.4264406   0.10157462 -0.3122991  -0.02106814]\n [-0.15342572  0.28946322  0.187281   -0.153364    0.02109777]\n [-0.40754154 -0.17786615  0.06924028  0.0978508   0.14219335]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 105] train=0.553846 val=0.666000 loss=62841.214661 time: 27.257734\n0\n\n[[-0.08880851 -0.08469296  0.10289477  0.419838    0.4249768 ]\n [-0.01712747  0.10028443 -0.02186516  0.02554248  0.24031673]\n [ 0.03332978  0.4285769   0.10357308 -0.31052637 -0.01897953]\n [-0.15193897  0.2911423   0.18910295 -0.15161027  0.02285729]\n [-0.4067993  -0.1768833   0.07059103  0.09939658  0.1435918 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08911132 -0.08512496  0.10199782  0.4189906   0.42408612]\n [-0.01747188  0.09958699 -0.0228593   0.02463187  0.2392519 ]\n [ 0.03286325  0.4278802   0.10282074 -0.3112651  -0.02008039]\n [-0.15257952  0.29040858  0.1884042  -0.15227635  0.02174057]\n [-0.4073632  -0.17757781  0.06992289  0.09876993  0.14254488]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.0851544  -0.08173545  0.10432599  0.4208799   0.42550215]\n [-0.01347372  0.10321611 -0.02019608  0.02678307  0.24097365]\n [ 0.03611015  0.43109906  0.10526961 -0.30937195 -0.01856583]\n [-0.15016527  0.2928025   0.19047688 -0.15058534  0.02301827]\n [-0.40535867 -0.17566423  0.07164898  0.10025391  0.14368947]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08640475 -0.08298846  0.10308189  0.41946694  0.4240609 ]\n [-0.01453855  0.1021762  -0.02119978  0.02550397  0.2395977 ]\n [ 0.03482764  0.42993066  0.10406028 -0.3109696  -0.0202433 ]\n [-0.15124698  0.29161978  0.1890781  -0.15250462  0.02101616]\n [-0.40659598 -0.17711802  0.06991985  0.09796569  0.14133371]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 106] train=0.556370 val=0.665600 loss=62553.982880 time: 28.837178\n0\n\n[[-0.08728787 -0.08323567  0.10333053  0.41993448  0.42479202]\n [-0.01522228  0.10225336 -0.02063832  0.02621009  0.2405852 ]\n [ 0.03431658  0.43012768  0.10459057 -0.3104154  -0.01926156]\n [-0.15103114  0.2922864   0.18990342 -0.15176998  0.02214242]\n [-0.40593904 -0.17633052  0.07080319  0.098789    0.14236206]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08880068 -0.08459601  0.10149436  0.41814607  0.42333218]\n [-0.01623134  0.10137893 -0.02217792  0.0243041   0.23875952]\n [ 0.03377502  0.4298203   0.10352733 -0.312358   -0.02125857]\n [-0.15082937  0.29282665  0.18974127 -0.15287639  0.020672  ]\n [-0.40554717 -0.17559278  0.07108752  0.09832291  0.14139867]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.09006832 -0.08639073  0.09956723  0.4169811   0.42285815]\n [-0.01777681  0.0995445  -0.02392616  0.02339401  0.23876497]\n [ 0.03253426  0.42852136  0.10244241 -0.31266776 -0.02053432]\n [-0.15145493  0.29214284  0.18915264 -0.15262763  0.02180795]\n [-0.4051381  -0.17549056  0.07099789  0.09881028  0.14252508]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08826011 -0.08496465  0.10099994  0.41844636  0.42389998]\n [-0.01616772  0.10082307 -0.02294184  0.02415982  0.23905458]\n [ 0.03366171  0.42937902  0.10281409 -0.31264418 -0.02091317]\n [-0.15042593  0.29290625  0.18952402 -0.15267667  0.02143686]\n [-0.4037563  -0.17433938  0.07187118  0.09950159  0.14268216]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 107] train=0.555268 val=0.660400 loss=62921.091827 time: 39.427245\n0\n\n[[-0.087718   -0.08464478  0.10149522  0.41909212  0.4242858 ]\n [-0.015893    0.10081749 -0.02271046  0.02479556  0.23954329]\n [ 0.03328418  0.4288862   0.10260618 -0.31227303 -0.02031793]\n [-0.15130772  0.29204044  0.18908343 -0.15262017  0.02166302]\n [-0.4046098  -0.17523141  0.07119396  0.09924813  0.14256749]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08755936 -0.08393168  0.10310656  0.42085803  0.42556486]\n [-0.01598271  0.10123796 -0.02169764  0.02583244  0.24020942]\n [ 0.03319754  0.42903703  0.10304583 -0.31184858 -0.01989572]\n [-0.15164936  0.29177648  0.18899135 -0.15249012  0.02211328]\n [-0.40518594 -0.17577726  0.07091884  0.09959789  0.14338401]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08739571 -0.08379517  0.10342253  0.42072332  0.42516917]\n [-0.01650514  0.10056597 -0.02215026  0.02522528  0.23953116]\n [ 0.03225543  0.42796922  0.10208842 -0.31287444 -0.02074998]\n [-0.15283342  0.29057452  0.18805902 -0.1534306   0.0214355 ]\n [-0.406819   -0.17740141  0.06979947  0.09871932  0.14253418]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0882288  -0.0847727   0.1025757   0.4202079   0.42417568]\n [-0.01715501  0.09975716 -0.02286476  0.02498025  0.23900001]\n [ 0.03196268  0.42756143  0.10180494 -0.31273785 -0.02072913]\n [-0.1525659   0.29072553  0.18849318 -0.1526235   0.02189372]\n [-0.405738   -0.17664503  0.07077435  0.09968725  0.14268173]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 108] train=0.554647 val=0.662200 loss=62669.675293 time: 27.093411\n0\n\n[[-0.09032097 -0.087405    0.09973902  0.41768152  0.42178553]\n [-0.01912749  0.09750268 -0.02512035  0.02320758  0.23765434]\n [ 0.0304706   0.4260306   0.10036269 -0.31368667 -0.02121259]\n [-0.15333064  0.28993732  0.1877688  -0.15296862  0.02204504]\n [-0.40587363 -0.17690116  0.07056237  0.09981158  0.1432543 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08770537 -0.08528587  0.10151549  0.41983372  0.42396164]\n [-0.01680829  0.09919692 -0.02384835  0.02481097  0.23947848]\n [ 0.03259769  0.42768657  0.1016011  -0.3122151  -0.01956086]\n [-0.15117545  0.29179126  0.18934578 -0.15134387  0.02350392]\n [-0.40395316 -0.17496619  0.07226623  0.10139138  0.14421844]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08982455 -0.08721694  0.09931552  0.41805157  0.4223471 ]\n [-0.0185461   0.09764064 -0.02580331  0.02291454  0.23762707]\n [ 0.03128075  0.42649493  0.09976244 -0.31421402 -0.02140114]\n [-0.15255465  0.29055673  0.18767785 -0.15342258  0.02141391]\n [-0.40478843 -0.17556554  0.07128738  0.09966581  0.14202163]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08948648 -0.08659131  0.09910194  0.41687784  0.42080706]\n [-0.01866234  0.0975731  -0.02645693  0.02181106  0.23616217]\n [ 0.03102866  0.4260769   0.09876657 -0.31531233 -0.02260837]\n [-0.15252846  0.29044062  0.18712717 -0.15397559  0.02055289]\n [-0.4043864  -0.17536494  0.07093942  0.0993574   0.14135706]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 109] train=0.552003 val=0.662400 loss=62924.325928 time: 26.516845\n0\n\n[[-0.09027796 -0.08727634  0.098644    0.41625753  0.41972357]\n [-0.01909944  0.09699297 -0.02702495  0.02136326  0.23556703]\n [ 0.03082583  0.42588124  0.09852578 -0.31540564 -0.02286274]\n [-0.15271635  0.29062843  0.18760395 -0.15338852  0.02053014]\n [-0.4041077  -0.17470218  0.07191899  0.1002358   0.14131725]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.09042833 -0.08730578  0.09872812  0.416773    0.4206596 ]\n [-0.0191259   0.09701888 -0.02711321  0.02175206  0.23643018]\n [ 0.03056509  0.4256501   0.09802654 -0.31562418 -0.02255352]\n [-0.15367016  0.28964064  0.18678524 -0.15398134  0.0201341 ]\n [-0.40551183 -0.17607878  0.07079299  0.09915391  0.13986659]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08725151 -0.08488144  0.10028838  0.4179535   0.42126548]\n [-0.0166224   0.09893169 -0.02580058  0.02289277  0.23711686]\n [ 0.03236843  0.42706257  0.09896673 -0.3148957  -0.02208338]\n [-0.15193854  0.29099566  0.1876041  -0.15331268  0.02063481]\n [-0.40375516 -0.17480704  0.07142399  0.09974039  0.14017765]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08780352 -0.08521627  0.10007848  0.41789165  0.4211102 ]\n [-0.01714827  0.09868538 -0.0258092   0.02323899  0.23750278]\n [ 0.03154295  0.42666674  0.09873333 -0.31462288 -0.02132715]\n [-0.15342443  0.29004434  0.18705729 -0.15307347  0.02160118]\n [-0.40531462 -0.17597897  0.0707717   0.10000812  0.14108858]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 110] train=0.554948 val=0.662900 loss=62371.071716 time: 27.777362\n0\n\n[[-0.0869455  -0.08481614  0.09984348  0.41750577  0.42044067]\n [-0.01634411  0.09911518 -0.02580497  0.0231664   0.23708372]\n [ 0.03270523  0.4275312   0.09914471 -0.31427127 -0.02135968]\n [-0.15191759  0.29125774  0.18762134 -0.1526183   0.0216641 ]\n [-0.40422583 -0.17512438  0.0710291   0.10018969  0.14086366]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08584788 -0.08383998  0.10039553  0.41752067  0.4195913 ]\n [-0.01511253  0.1001288  -0.02517533  0.02332362  0.23633663]\n [ 0.03371631  0.42847085  0.09981595 -0.31403795 -0.0219092 ]\n [-0.15077643  0.29222685  0.18842123 -0.15208045  0.02178416]\n [-0.40312973 -0.17427003  0.07166047  0.10097628  0.14139996]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08754415 -0.08540387  0.09898588  0.4161172   0.417894  ]\n [-0.01635483  0.09904979 -0.0259868   0.02249726  0.23533168]\n [ 0.03243961  0.427413    0.09882751 -0.3151884  -0.02275718]\n [-0.15253691  0.2904332   0.1866596  -0.15379205  0.02067203]\n [-0.405458   -0.17684163  0.06902709  0.09855537  0.13963518]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08747923 -0.08495545  0.09967633  0.41688952  0.41837084]\n [-0.01597105  0.09952854 -0.02526738  0.02356279  0.23611808]\n [ 0.03315383  0.42814842  0.09967566 -0.31391087 -0.02159332]\n [-0.15185186  0.29124412  0.18788649 -0.15189774  0.02251215]\n [-0.4047137  -0.17591804  0.07048514  0.10078231  0.14203955]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 111] train=0.553926 val=0.664700 loss=62648.571991 time: 28.687760\n0\n\n[[-0.0874625  -0.08503454  0.09937237  0.41640937  0.41773078]\n [-0.0161765   0.09930971 -0.02562883  0.02297012  0.23520641]\n [ 0.03278956  0.4276923   0.09933276 -0.3144955  -0.02259144]\n [-0.15204644  0.2907865   0.18757315 -0.15250656  0.02139209]\n [-0.40503672 -0.1765996   0.06989529  0.10011642  0.14100236]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08721153 -0.08483641  0.09936403  0.4165059   0.41789   ]\n [-0.01658724  0.09903642 -0.02598432  0.02259652  0.2350089 ]\n [ 0.03178167  0.42683306  0.09837823 -0.31553367 -0.02348323]\n [-0.15362552  0.28910536  0.18579167 -0.15444495  0.01950476]\n [-0.40620804 -0.178282    0.06773389  0.09778821  0.13881218]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08554158 -0.08380434  0.09970098  0.41689625  0.41838497]\n [-0.01521989  0.10006586 -0.02550732  0.02308339  0.2356759 ]\n [ 0.03284342  0.4279598   0.09928302 -0.31459546 -0.02259775]\n [-0.15254456  0.29025465  0.18674599 -0.15368886  0.01999515]\n [-0.40539566 -0.17774248  0.06800535  0.09791611  0.13853365]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08593876 -0.08458131  0.09869986  0.41624823  0.4178801 ]\n [-0.01536444  0.09956665 -0.02648615  0.02240899  0.23509799]\n [ 0.03275765  0.42767692  0.09839374 -0.31544748 -0.02324506]\n [-0.1522937   0.29003343  0.18575113 -0.15474506  0.01910876]\n [-0.4051921  -0.17838322  0.0665968   0.09642374  0.1370716 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 112] train=0.555589 val=0.662000 loss=62428.131744 time: 26.702311\n0\n\n[[-0.08489969 -0.08416181  0.09911153  0.41669363  0.41816846]\n [-0.01430014  0.10019195 -0.02583532  0.02313493  0.23557243]\n [ 0.03380166  0.42854685  0.0992004  -0.31472337 -0.02268665]\n [-0.15126133  0.29085988  0.18646368 -0.15402885  0.01983214]\n [-0.40390202 -0.17747542  0.06721615  0.09709863  0.13780853]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08472778 -0.0832103   0.10048258  0.41838732  0.42008126]\n [-0.01394659  0.10159951 -0.02381911  0.02530155  0.23780736]\n [ 0.03481156  0.43036038  0.10140757 -0.31247884 -0.02030201]\n [-0.14942637  0.29296216  0.1885061  -0.15216668  0.02214276]\n [-0.40154088 -0.17493168  0.0690854   0.09869282  0.14010736]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08646592 -0.08494433  0.09858603  0.4161375   0.41814718]\n [-0.01515252  0.10050145 -0.02533643  0.02326804  0.23610869]\n [ 0.0339807   0.4297399   0.1004097  -0.3141558  -0.02178644]\n [-0.15007453  0.2923582   0.18767102 -0.15362704  0.02069548]\n [-0.4020421  -0.17549835  0.06835931  0.0975499   0.13885257]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08467481 -0.08323611  0.1002568   0.4177184   0.4190093 ]\n [-0.01309746  0.10255304 -0.02330186  0.02544807  0.2377884 ]\n [ 0.03557578  0.43156618  0.10225574 -0.3121384  -0.02002394]\n [-0.14878829  0.2938976   0.18935898 -0.1517148   0.02235389]\n [-0.4007167  -0.17395815  0.0699949   0.09939975  0.14060675]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 113] train=0.556851 val=0.659000 loss=62472.620056 time: 28.557581\n0\n\n[[-0.08673819 -0.08542541  0.09844913  0.41656783  0.41801432]\n [-0.01497985  0.10044625 -0.02528968  0.02425275  0.2370504 ]\n [ 0.03421443  0.4301086   0.1007385  -0.31305364 -0.02051055]\n [-0.14951058  0.29335883  0.18903582 -0.15158054  0.02264933]\n [-0.40054667 -0.17365155  0.0704329   0.10007012  0.14123471]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08877753 -0.08700648  0.09707081  0.41514266  0.41635174]\n [-0.01627033  0.09939893 -0.02639305  0.02317853  0.23586105]\n [ 0.03312572  0.4291886   0.09980649 -0.31396934 -0.02144019]\n [-0.15076792  0.29226092  0.18826494 -0.15215917  0.0220069 ]\n [-0.4015903  -0.17457077  0.06977375  0.09955499  0.14057264]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08834956 -0.08645883  0.09817845  0.41647106  0.41721994]\n [-0.01655977  0.0993507  -0.02597434  0.02380863  0.23608732]\n [ 0.0321996   0.42835385  0.09924746 -0.31458378 -0.02203477]\n [-0.15239652  0.2906385   0.18701175 -0.15340559  0.02104045]\n [-0.40399644 -0.17684157  0.06810489  0.09807091  0.139318  ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08674328 -0.08505137  0.09945554  0.41791075  0.41910613]\n [-0.0154838   0.1004097  -0.02472224  0.02542949  0.23813127]\n [ 0.03308963  0.42927387  0.10031959 -0.31333056 -0.02025496]\n [-0.15116481  0.29211253  0.18856607 -0.15166166  0.02314522]\n [-0.40228307 -0.1746941   0.07027838  0.10058191  0.14195065]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 114] train=0.556731 val=0.659900 loss=62403.158997 time: 28.929607\n0\n\n[[-0.08665556 -0.0848942   0.09997954  0.4189771   0.4202764 ]\n [-0.01566474  0.10020103 -0.0247604   0.02585235  0.23855752]\n [ 0.032379    0.4286665   0.09996904 -0.31361616 -0.02070608]\n [-0.15245013  0.29101238  0.1877342  -0.15243098  0.02217805]\n [-0.40409547 -0.17627461  0.06893681  0.09956419  0.14089584]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08612582 -0.08487111  0.09938668  0.41788194  0.4192578 ]\n [-0.01485755  0.10060199 -0.02482986  0.02537532  0.23814222]\n [ 0.03359409  0.42996278  0.10080024 -0.31345153 -0.02062777]\n [-0.15106697  0.29251367  0.18886247 -0.15196046  0.0226464 ]\n [-0.40266806 -0.17492169  0.06999328  0.10016072  0.1415567 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08810043 -0.08656608  0.09820873  0.4170055   0.41821492]\n [-0.01687139  0.09889662 -0.02587609  0.02473918  0.23774554]\n [ 0.03145871  0.42827502  0.09975442 -0.31401014 -0.02057826]\n [-0.15321676  0.29065254  0.1874296  -0.15288687  0.02244891]\n [-0.40478343 -0.17695959  0.0681631   0.09862377  0.14057969]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.0871684  -0.08595333  0.09883843  0.41759026  0.4188239 ]\n [-0.01654119  0.09862369 -0.02630608  0.02435018  0.2378124 ]\n [ 0.03139025  0.4275272   0.09868839 -0.31517667 -0.02106936]\n [-0.15338455  0.28989282  0.18627174 -0.15436453  0.02134659]\n [-0.40527633 -0.17790902  0.06687783  0.09710649  0.1390958 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 115] train=0.557051 val=0.663300 loss=62223.052612 time: 29.623029\n0\n\n[[-0.08709393 -0.08548123  0.09925737  0.41754588  0.41804346]\n [-0.01668317  0.09879609 -0.02603336  0.02427584  0.23727007]\n [ 0.03117025  0.42764103  0.09886677 -0.31545535 -0.02168988]\n [-0.15328988  0.29021233  0.18677105 -0.15425669  0.0212715 ]\n [-0.40510407 -0.17754205  0.06746387  0.09766541  0.13971923]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08634733 -0.08466078  0.10036349  0.41822654  0.41818237]\n [-0.01538042  0.09988738 -0.02471138  0.0252506   0.23777127]\n [ 0.03259309  0.42887354  0.10030717 -0.31440213 -0.02100234]\n [-0.15149802  0.29166186  0.18837474 -0.15286855  0.0224788 ]\n [-0.40313834 -0.17606436  0.06909953  0.09941117  0.14145522]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08371902 -0.082273    0.10218754  0.41941062  0.41890773]\n [-0.01360487  0.10117126 -0.02403929  0.02528231  0.23724146]\n [ 0.03335755  0.42917457  0.09986319 -0.31530967 -0.02226571]\n [-0.15131833  0.29139328  0.18741207 -0.15427154  0.02086352]\n [-0.40318823 -0.17640115  0.06811178  0.09795735  0.13981044]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08463549 -0.08296575  0.10140378  0.41868213  0.41827574]\n [-0.01481555  0.10016525 -0.02498365  0.02445789  0.2368406 ]\n [ 0.03186401  0.42822742  0.09920742 -0.31589425 -0.0223095 ]\n [-0.15284836  0.29054412  0.1869961  -0.1547701   0.02073807]\n [-0.40422967 -0.17706352  0.06752637  0.09730084  0.13937482]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 116] train=0.556290 val=0.664500 loss=62255.748627 time: 27.426114\n0\n\n[[-0.08289318 -0.08143347  0.10293832  0.41982096  0.418501  ]\n [-0.01361167  0.10120181 -0.02389792  0.02529869  0.23701712]\n [ 0.03247641  0.4287607   0.09970398 -0.31558314 -0.02238961]\n [-0.15275943  0.29066616  0.18700558 -0.15484539  0.02036284]\n [-0.40405416 -0.17703842  0.06740303  0.09730371  0.13913448]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0844833  -0.08296587  0.10134909  0.4182014   0.41654998]\n [-0.01558766  0.09943972 -0.02536093  0.02412754  0.23577252]\n [ 0.03024705  0.4271433   0.098584   -0.31617326 -0.02292644]\n [-0.15406957  0.29004627  0.186875   -0.1548718   0.01998404]\n [-0.40409184 -0.17666799  0.06790864  0.0976598   0.13893609]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08454755 -0.08318016  0.10139306  0.41823316  0.41646254]\n [-0.01538093  0.09936548 -0.02529247  0.02432662  0.23612544]\n [ 0.03080667  0.42767504  0.09930695 -0.31555048 -0.0221831 ]\n [-0.15321937  0.29088625  0.187881   -0.15402547  0.0208061 ]\n [-0.4030516  -0.17571649  0.068854    0.09864025  0.13981463]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08629375 -0.0849071   0.09971796  0.416645    0.41475996]\n [-0.01702473  0.0977203  -0.027132    0.0224566   0.2342763 ]\n [ 0.02945708  0.42650953  0.09785132 -0.31727004 -0.02367561]\n [-0.15411593  0.2899293   0.18647416 -0.15571426  0.01943283]\n [-0.40389264 -0.17675334  0.06728899  0.09694045  0.13828197]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 117] train=0.554367 val=0.667200 loss=62486.307709 time: 26.930146\n0\n\n[[-0.08396431 -0.08255506  0.10213598  0.41909933  0.41729808]\n [-0.01471793  0.10018365 -0.0248457   0.02452774  0.23639028]\n [ 0.03147219  0.42875773  0.09967425 -0.316042   -0.02234283]\n [-0.15263854  0.29144925  0.18760262 -0.15529467  0.01998647]\n [-0.40330562 -0.17616324  0.06779218  0.09719023  0.13868378]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08596519 -0.08474926  0.10029235  0.41763338  0.41596678]\n [-0.01681482  0.0980104  -0.0265331   0.02356401  0.23574518]\n [ 0.02915374  0.42637503  0.0976634  -0.31739935 -0.02336768]\n [-0.15499851  0.28888792  0.18541579 -0.15694517  0.01822636]\n [-0.4053542  -0.17852996  0.06584353  0.09567614  0.13667722]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08511132 -0.0839084   0.10135809  0.4189241   0.4172113 ]\n [-0.01601338  0.09891576 -0.02500239  0.02548296  0.23770458]\n [ 0.03018728  0.42744407  0.09933643 -0.31528625 -0.02079073]\n [-0.15353443  0.290024    0.18666014 -0.15509993  0.02081564]\n [-0.40369138 -0.17742206  0.06688391  0.09726842  0.13879502]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08577339 -0.0844873   0.10121088  0.41916236  0.41765705]\n [-0.01674521  0.09840524 -0.0250901   0.02581473  0.23839988]\n [ 0.03006482  0.42755443  0.09981044 -0.31459874 -0.02000773]\n [-0.15339388  0.2904639   0.1874218  -0.15411335  0.02173693]\n [-0.4037891  -0.17719814  0.067358    0.09817623  0.13978696]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 118] train=0.558293 val=0.666400 loss=62281.943756 time: 25.218778\n0\n\n[[-0.08650163 -0.0858349   0.09990861  0.41746712  0.41517672]\n [-0.01759959  0.09687894 -0.02652737  0.02423588  0.23642434]\n [ 0.02914282  0.42618024  0.09849362 -0.31597784 -0.02167439]\n [-0.15388262  0.28946838  0.18642287 -0.15509589  0.02030961]\n [-0.40394348 -0.17803761  0.06637815  0.0973686   0.13843317]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08690815 -0.08638033  0.09960543  0.4173853   0.41470987]\n [-0.01711099  0.09745591 -0.02609208  0.02458177  0.23647961]\n [ 0.0307681   0.42796218  0.09974943 -0.31526518 -0.02118281]\n [-0.15141807  0.2918038   0.18834461 -0.15373468  0.02115732]\n [-0.40104288 -0.17540503  0.06876181  0.09931726  0.13959792]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.0854843  -0.08498269  0.10134695  0.41943157  0.41665298]\n [-0.01601462  0.09840606 -0.0250432   0.02591756  0.23796998]\n [ 0.03171401  0.4289555   0.10066272 -0.31436008 -0.02006589]\n [-0.15059137  0.29273066  0.18919565 -0.15311836  0.02185478]\n [-0.40037048 -0.17475405  0.06923579  0.09979174  0.14009854]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08438257 -0.0841996   0.10195006  0.42020315  0.4179577 ]\n [-0.01522926  0.09893543 -0.02476418  0.02623887  0.23869523]\n [ 0.03190548  0.42870694  0.10029254 -0.31458572 -0.02001853]\n [-0.15096322  0.29179475  0.18833134 -0.15377931  0.02129279]\n [-0.4006517  -0.1752885   0.06854187  0.09898287  0.1391957 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 119] train=0.555669 val=0.658100 loss=62433.364502 time: 26.046590\n0\n\n[[-0.08583821 -0.08519462  0.10070527  0.41893274  0.41690835]\n [-0.01642323  0.09830609 -0.02560433  0.02528487  0.23798077]\n [ 0.03121535  0.42843816  0.0997401  -0.3153241  -0.02047939]\n [-0.15113607  0.2917015   0.18785764 -0.15442576  0.02085857]\n [-0.4004413  -0.17519768  0.06821907  0.09837395  0.13858573]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08682609 -0.08646271  0.09895319  0.4169958   0.41468436]\n [-0.01738203  0.09705765 -0.0271      0.02381583  0.23631038]\n [ 0.03017564  0.4272322   0.09838154 -0.31661206 -0.02215186]\n [-0.15232356  0.29056805  0.1866253  -0.15567116  0.01911032]\n [-0.40116936 -0.17595871  0.06720512  0.09728177  0.13705185]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08569207 -0.08455094  0.1009194   0.4189141   0.4164618 ]\n [-0.01699739  0.09807263 -0.02566244  0.02545989  0.23786628]\n [ 0.0302303   0.4278357   0.09932582 -0.31540903 -0.02108252]\n [-0.15265049  0.29067403  0.18701334 -0.15510727  0.01947844]\n [-0.40160137 -0.17628048  0.06695776  0.09731641  0.13711375]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08450474 -0.0833963   0.10186309  0.41966692  0.41715607]\n [-0.01553755  0.09968192 -0.02435725  0.0263391   0.2386011 ]\n [ 0.03208763  0.429655    0.10062841 -0.3149027  -0.0207576 ]\n [-0.1507178   0.29261506  0.18846872 -0.15456137  0.01972801]\n [-0.3994506  -0.1742999   0.06847958  0.09830403  0.137807  ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 120] train=0.553305 val=0.660500 loss=62468.133423 time: 24.521818\n0\n\n[[-0.08589587 -0.08454514  0.10103621  0.4191982   0.41660163]\n [-0.01708784  0.09854326 -0.02525503  0.02557652  0.2379704 ]\n [ 0.03096352  0.429029    0.10016503 -0.3154717  -0.02107045]\n [-0.1511749   0.2923869   0.18849128 -0.15449244  0.02015837]\n [-0.39954782 -0.17431217  0.06874692  0.09894753  0.13878939]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08519172 -0.08409696  0.10174288  0.42004615  0.41710028]\n [-0.01724643  0.09812737 -0.0252188   0.02615646  0.23850647]\n [ 0.03035599  0.42834786  0.09984178 -0.3152044  -0.02078612]\n [-0.15225588  0.2913748   0.18782291 -0.15451397  0.02017281]\n [-0.40140286 -0.17602633  0.06740882  0.09819006  0.13787547]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08392904 -0.08272146  0.10284209  0.42106512  0.4180395 ]\n [-0.01666064  0.09870389 -0.02494438  0.02654838  0.23930855]\n [ 0.03033708  0.42855296  0.10002895 -0.31468984 -0.01962551]\n [-0.1527475   0.29137546  0.18811518 -0.15375602  0.02135474]\n [-0.40209654 -0.1762397   0.0674643   0.09867868  0.13860373]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08721504 -0.08560274  0.1002247   0.41871938  0.41551268]\n [-0.0203725   0.09567622 -0.02754756  0.02421514  0.23669085]\n [ 0.02646027  0.42557186  0.09739659 -0.31724346 -0.02222727]\n [-0.15638591  0.2883957   0.18545315 -0.15626906  0.01881449]\n [-0.40523148 -0.1788682   0.06525119  0.09663267  0.13641022]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 121] train=0.557652 val=0.661300 loss=62140.125336 time: 24.420758\n0\n\n[[-0.08549374 -0.08398529  0.10208622  0.42141366  0.41872954]\n [-0.01924714  0.09683705 -0.02591667  0.02687049  0.24008496]\n [ 0.02754968  0.42683658  0.09901661 -0.31503326 -0.01914646]\n [-0.15531258  0.28956378  0.18688007 -0.1545529   0.02133677]\n [-0.4043638  -0.17804566  0.06618796  0.09768689  0.13828339]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08565886 -0.08459517  0.10132708  0.42099005  0.4183787 ]\n [-0.01827249  0.09742338 -0.02566556  0.02731956  0.2406041 ]\n [ 0.02950973  0.42844746  0.10013428 -0.313827   -0.01786319]\n [-0.1529055   0.29148543  0.18853855 -0.15265991  0.02317985]\n [-0.40227008 -0.1764054   0.06750511  0.09922451  0.1397315 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08652527 -0.08588049  0.09964843  0.41907227  0.41503128]\n [-0.01923685  0.09644219 -0.02702663  0.02562908  0.23786071]\n [ 0.02863907  0.42768255  0.09889928 -0.3156818  -0.02042219]\n [-0.15377206  0.29038087  0.18693435 -0.154778    0.02064847]\n [-0.40354407 -0.17793669  0.06543937  0.09688622  0.13700652]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08507349 -0.08504775  0.10058274  0.4204403   0.41618565]\n [-0.01803089  0.09742014 -0.02580587  0.02722016  0.23923923]\n [ 0.02970685  0.4290072   0.10058903 -0.31380966 -0.01868708]\n [-0.15277739  0.2917112   0.18872082 -0.1527727   0.02248044]\n [-0.40275127 -0.1768271   0.06684425  0.0985639   0.13840318]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 122] train=0.554948 val=0.660100 loss=62501.047241 time: 51.069255\n0\n\n[[-0.08538912 -0.08544707  0.09990519  0.4200203   0.41578987]\n [-0.01845148  0.09689583 -0.02646009  0.02693794  0.23925437]\n [ 0.02948756  0.42866805  0.10025366 -0.31377062 -0.01829242]\n [-0.15263362  0.29193416  0.18929765 -0.15194824  0.02328842]\n [-0.40271106 -0.1764369   0.06776801  0.09959557  0.13907008]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08443291 -0.08487307  0.09991077  0.42020583  0.41590753]\n [-0.01757147  0.09736647 -0.02674192  0.02697655  0.23957732]\n [ 0.0303571   0.42912155  0.09998885 -0.31390584 -0.01779672]\n [-0.15171885  0.29260617  0.18944824 -0.15197599  0.02388381]\n [-0.4012353  -0.17528656  0.06840373  0.10008847  0.14014895]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08566745 -0.08597168  0.09895539  0.4193739   0.4152493 ]\n [-0.0186731   0.09647819 -0.02741119  0.02667772  0.23964271]\n [ 0.02931261  0.42835167  0.0995364  -0.3140832  -0.01747243]\n [-0.15259315  0.2920639   0.189112   -0.15223384  0.02403236]\n [-0.4018277  -0.17560086  0.06822784  0.10002846  0.14043897]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08553222 -0.08606567  0.09835198  0.41798216  0.41364598]\n [-0.01800708  0.09705666 -0.02727292  0.02609978  0.23881066]\n [ 0.03049454  0.42965484  0.10046476 -0.3140408  -0.01763172]\n [-0.15099439  0.29394507  0.1905443  -0.15169846  0.02416535]\n [-0.39953002 -0.1728899   0.07027847  0.10101187  0.1407191 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 123] train=0.555268 val=0.661700 loss=62194.547546 time: 45.640170\n0\n\n[[-0.08463769 -0.08492881  0.09917752  0.4183873   0.41374925]\n [-0.01817939  0.09712836 -0.02739446  0.02584964  0.23873636]\n [ 0.02959765  0.4290861   0.09970477 -0.3148781  -0.01796932]\n [-0.15248     0.29280204  0.18930629 -0.15288834  0.02355282]\n [-0.4014488  -0.17443845  0.06872434  0.09955806  0.1395962 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08336584 -0.08322157  0.10097571  0.42023733  0.41541845]\n [-0.01694606  0.09870669 -0.02568731  0.02761301  0.24083109]\n [ 0.03074149  0.43061078  0.10137389 -0.31326628 -0.01591782]\n [-0.15158473  0.2942126   0.19101237 -0.1511488   0.02549226]\n [-0.4011031  -0.17377333  0.06969909  0.10084651  0.14081407]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08574009 -0.08536291  0.09923159  0.41886967  0.41379607]\n [-0.01929649  0.0963705  -0.02785054  0.0259158   0.23906888]\n [ 0.02863487  0.4283627   0.09888379 -0.31555897 -0.0181194 ]\n [-0.15366766  0.29210308  0.188714   -0.15338933  0.02322505]\n [-0.40305465 -0.17570007  0.06801939  0.09948131  0.13923502]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08411331 -0.08408321  0.10041738  0.4200571   0.41482383]\n [-0.01762889  0.09751742 -0.02703848  0.02696899  0.2400329 ]\n [ 0.0303807   0.4293035   0.09943599 -0.31472975 -0.01738275]\n [-0.15188229  0.2928945   0.18882062 -0.15296584  0.02376416]\n [-0.40101564 -0.17487429  0.0678956   0.09958763  0.13968976]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 124] train=0.556390 val=0.663400 loss=62316.241089 time: 50.724644\n0\n\n[[-0.08346222 -0.08393338  0.10036623  0.42052084  0.4157092 ]\n [-0.01656584  0.09795089 -0.02704479  0.02732377  0.24103367]\n [ 0.03133017  0.42983562  0.09939402 -0.314633   -0.01667452]\n [-0.15139277  0.29318362  0.18878832 -0.15289913  0.02419164]\n [-0.4010946  -0.17514624  0.06760494  0.09980824  0.14016885]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08336858 -0.08400974  0.1003142   0.42042133  0.4150381 ]\n [-0.01659155  0.09785555 -0.02731634  0.02662751  0.23948792]\n [ 0.03132964  0.42982057  0.0990129  -0.3157954  -0.01886847]\n [-0.1512561   0.29307997  0.18832639 -0.15406728  0.02190601]\n [-0.4007025  -0.17542     0.06688914  0.09873687  0.1380661 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08194145 -0.08317133  0.10080854  0.42082578  0.41521323]\n [-0.01495085  0.09914806 -0.02660174  0.02698302  0.23977308]\n [ 0.0329638   0.4314193   0.10000032 -0.31543157 -0.01853429]\n [-0.14958383  0.29465836  0.18933032 -0.15367566  0.02213398]\n [-0.39913586 -0.17408688  0.06768791  0.09911849  0.13831569]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08141248 -0.08303259  0.1007212   0.42046174  0.41451785]\n [-0.01436232  0.0997953  -0.02616861  0.02717966  0.23985742]\n [ 0.03338494  0.4322965   0.10101759 -0.3145724  -0.01770125]\n [-0.14922659  0.2957739   0.19094515 -0.15206166  0.02343896]\n [-0.39839292 -0.17263767  0.06971079  0.10126323  0.14004429]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 125] train=0.558333 val=0.665500 loss=62264.076782 time: 30.335491\n0\n\n[[-0.08354937 -0.08535414  0.09836815  0.41839135  0.4126457 ]\n [-0.01687942  0.09724258 -0.0287648   0.0248533   0.23805593]\n [ 0.03051151  0.42950374  0.09824073 -0.31705534 -0.01947367]\n [-0.15246744  0.29266942  0.1880156  -0.15490972  0.02105073]\n [-0.40180665 -0.17598704  0.06646565  0.09789864  0.1369366 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08353119 -0.08538978  0.09869371  0.41872445  0.41267565]\n [-0.01745123  0.09682054 -0.02900413  0.02472796  0.2378677 ]\n [ 0.03012584  0.42912045  0.09753716 -0.31798506 -0.02040607]\n [-0.1528225   0.29225755  0.187205   -0.15610261  0.01975501]\n [-0.4019907  -0.17642957  0.0657015   0.09683256  0.13570398]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08415068 -0.08638339  0.09773466  0.41797706  0.41258514]\n [-0.01814831  0.09565155 -0.03015674  0.02397723  0.2377254 ]\n [ 0.02972868  0.42836902  0.09668807 -0.3187278  -0.0207257 ]\n [-0.15299314  0.29201874  0.18701392 -0.15649372  0.01956001]\n [-0.40216002 -0.17655908  0.06584154  0.09697167  0.13582973]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08236565 -0.08450586  0.09971797  0.4199452   0.41471785]\n [-0.01692982  0.09710406 -0.02844573  0.02582252  0.24009012]\n [ 0.03034393  0.42924422  0.09783818 -0.31725502 -0.01861043]\n [-0.15252131  0.29260343  0.18777251 -0.15551423  0.02099552]\n [-0.4017131  -0.17622854  0.06629463  0.09744896  0.13646793]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 126] train=0.554748 val=0.666400 loss=62567.897217 time: 29.005326\n0\n\n[[-0.08230038 -0.08424685  0.10034127  0.4205743   0.4147849 ]\n [-0.01633692  0.09774843 -0.02746301  0.02672687  0.24056305]\n [ 0.03133295  0.4302615   0.0990658  -0.31623748 -0.01809511]\n [-0.15100849  0.2941367   0.18943137 -0.1542525   0.0215331 ]\n [-0.39982736 -0.17454156  0.0680655   0.09885742  0.13704969]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08272555 -0.08526888  0.0992567   0.41928893  0.41344368]\n [-0.01658528  0.09720479 -0.02804102  0.02593588  0.23962139]\n [ 0.03126375  0.43010944  0.09884112 -0.3168936  -0.01900618]\n [-0.15115161  0.29382065  0.18897626 -0.15523505  0.02028304]\n [-0.4003051  -0.17546527  0.06697705  0.09729619  0.13509594]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08269967 -0.08543481  0.09874017  0.41872016  0.4129217 ]\n [-0.01585434  0.09739625 -0.02806889  0.0260926   0.23994657]\n [ 0.03214933  0.4307039   0.0993007  -0.3161243  -0.01809421]\n [-0.15052995  0.29456133  0.18982777 -0.1542064   0.02138789]\n [-0.39944473 -0.17431858  0.06821217  0.09855115  0.13628188]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08330529 -0.08637224  0.09731522  0.4169953   0.41098294]\n [-0.01611543  0.09704054 -0.0288421   0.02493884  0.23872726]\n [ 0.03223052  0.43090138  0.09902957 -0.3169734  -0.01887563]\n [-0.14995281  0.29511005  0.18978073 -0.1548417   0.0207843 ]\n [-0.3986042  -0.17330147  0.06872432  0.09823122  0.13570172]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 127] train=0.560617 val=0.664600 loss=62026.388916 time: 25.097403\n0\n\n[[-0.08490349 -0.08784812  0.09625193  0.4159706   0.40963858]\n [-0.01728316  0.09598494 -0.02956155  0.02446708  0.23799473]\n [ 0.03149056  0.43058512  0.0989859  -0.31680822 -0.01888949]\n [-0.15019599  0.29534817  0.1903191  -0.15408084  0.02133431]\n [-0.39873806 -0.17311454  0.06912424  0.09895122  0.13628437]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08452185 -0.08741014  0.09713206  0.4168445   0.41036084]\n [-0.01711631  0.09595378 -0.02922101  0.02511007  0.23862447]\n [ 0.03142833  0.4304084   0.09885456 -0.31678483 -0.0189187 ]\n [-0.1503729   0.29515752  0.19015136 -0.15431261  0.0209336 ]\n [-0.3991774  -0.1735401   0.06871597  0.09830575  0.13556021]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08334237 -0.08644273  0.0979088   0.41759336  0.41113156]\n [-0.01611486  0.09677116 -0.02856839  0.02602367  0.23973943]\n [ 0.03178061  0.43091482  0.09921522 -0.31628606 -0.01825195]\n [-0.15077253  0.2951726   0.1903414  -0.15415508  0.02108198]\n [-0.40023416 -0.1743727   0.06829139  0.09810268  0.13537218]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08341072 -0.08670005  0.09763979  0.41738304  0.4114168 ]\n [-0.01663064  0.0961775  -0.02893164  0.02584854  0.24011685]\n [ 0.03123805  0.4304606   0.09892182 -0.31633392 -0.01750843]\n [-0.15117662  0.29483458  0.18993738 -0.15438466  0.02180851]\n [-0.40005392 -0.17432737  0.06815132  0.09794479  0.13613923]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 128] train=0.555769 val=0.658100 loss=62283.600555 time: 23.895671\n0\n\n[[-0.08232528 -0.08495405  0.10003337  0.4200086   0.41408902]\n [-0.01574766  0.09773223 -0.02699437  0.02798579  0.2424608 ]\n [ 0.03190268  0.43179333  0.10060827 -0.31452486 -0.01552145]\n [-0.1508256   0.2956153   0.19128397 -0.15269563  0.02367488]\n [-0.40022892 -0.17411655  0.06902454  0.09940004  0.13794991]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08456524 -0.0872381   0.09773363  0.41759813  0.41131914]\n [-0.01785148  0.09546252 -0.02931125  0.02573362  0.24003914]\n [ 0.02969615  0.4297546   0.09877653 -0.31658024 -0.01763356]\n [-0.15250964  0.29418537  0.18988611 -0.15453325  0.02176731]\n [-0.4014841  -0.17518972  0.06785407  0.09776074  0.13614629]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.083265   -0.08632495  0.09859759  0.4184867   0.41201672]\n [-0.01702292  0.09598736 -0.02889591  0.02643384  0.2408862 ]\n [ 0.02985303  0.4298464   0.09873003 -0.3161915  -0.01675813]\n [-0.15292755  0.29360625  0.18925992 -0.15477182  0.02190545]\n [-0.4016537  -0.17592019  0.06678438  0.09677117  0.13552725]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08329611 -0.08631486  0.09873556  0.4190499   0.412697  ]\n [-0.01720801  0.09611639 -0.0287142   0.02691379  0.24168909]\n [ 0.02970528  0.4301491   0.0989112  -0.315794   -0.01601189]\n [-0.15262507  0.29437754  0.18985854 -0.15423526  0.02244885]\n [-0.40136567 -0.17535429  0.06718837  0.09705502  0.13586707]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 129] train=0.557432 val=0.666800 loss=62236.726105 time: 23.699360\n0\n\n[[-0.08337675 -0.08665313  0.09839877  0.41877362  0.41205612]\n [-0.01703283  0.09628252 -0.02857873  0.02699734  0.24141626]\n [ 0.03040096  0.43098947  0.09963173 -0.31524011 -0.01573523]\n [-0.15163864  0.29554427  0.19096929 -0.15340032  0.02295001]\n [-0.40015888 -0.1742254   0.06821001  0.09794617  0.13646762]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08305308 -0.08670637  0.09803707  0.41805062  0.41063198]\n [-0.01674225  0.09669723 -0.0281902   0.0271745   0.24091548]\n [ 0.03091566  0.43186545  0.1003978  -0.31477007 -0.01565626]\n [-0.15014192  0.29718745  0.19211228 -0.15276895  0.02311585]\n [-0.39773953 -0.1720089   0.06943898  0.09861602  0.13681723]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08392652 -0.08694287  0.09764418  0.4170608   0.40881673]\n [-0.01802508  0.09585633 -0.02926458  0.02566652  0.23898135]\n [ 0.02912851  0.43041036  0.09852957 -0.3169371  -0.01792451]\n [-0.15211017  0.29534075  0.18980788 -0.15529361  0.02049059]\n [-0.39954752 -0.17399128  0.06685596  0.09593026  0.13394955]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08401074 -0.08690634  0.09733417  0.41691703  0.40871406]\n [-0.0182762   0.09558918 -0.0295589   0.02588359  0.23945878]\n [ 0.02861751  0.4298021   0.09828665 -0.3163247  -0.0167168 ]\n [-0.15220231  0.29525235  0.1901429  -0.15414827  0.0222633 ]\n [-0.3988299  -0.17334746  0.06771444  0.09770614  0.13620077]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 130] train=0.558153 val=0.663900 loss=62140.045929 time: 23.736850\n0\n\n[[-0.08364444 -0.08706462  0.09709997  0.4166489   0.40798128]\n [-0.01769703  0.09585328 -0.02954171  0.02554161  0.23873077]\n [ 0.02913898  0.43052605  0.09875789 -0.3164997  -0.01712464]\n [-0.1516965   0.29620683  0.19097176 -0.1539765   0.02212235]\n [-0.39818138 -0.17231399  0.06881852  0.09836625  0.13642001]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08246066 -0.08561927  0.09825651  0.4174262   0.40873381]\n [-0.01695848  0.09665293 -0.02878191  0.02633644  0.23992139]\n [ 0.0296978   0.43103093  0.09921609 -0.31596097 -0.01600052]\n [-0.15153658  0.29608965  0.19070475 -0.1540867   0.02268616]\n [-0.39845356 -0.17303318  0.06787424  0.09760382  0.13639404]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08503164 -0.08764369  0.09616483  0.41573593  0.4070925 ]\n [-0.01905911  0.09506205 -0.0304747   0.02477097  0.23813018]\n [ 0.0280925   0.4299012   0.09814405 -0.317232   -0.01765658]\n [-0.15290686  0.29522586  0.19009708 -0.1547511   0.02128531]\n [-0.39935777 -0.17352666  0.06786003  0.0976764   0.13563116]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08384745 -0.08677992  0.09701394  0.4169354   0.4085921 ]\n [-0.01796092  0.09611827 -0.02937974  0.02602794  0.23949009]\n [ 0.02876201  0.43073112  0.09906753 -0.316422   -0.01687896]\n [-0.15255249  0.29568088  0.19070132 -0.15433848  0.02161108]\n [-0.39938813 -0.17356277  0.06784149  0.09767574  0.13578758]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 131] train=0.560337 val=0.665300 loss=61773.330933 time: 23.952735\n0\n\n[[-0.08314499 -0.08601032  0.0979182   0.41823822  0.41031474]\n [-0.01719339  0.09694315 -0.02835778  0.02718888  0.24086267]\n [ 0.02918709  0.4312655   0.09977333 -0.315903   -0.01633902]\n [-0.15275483  0.29564217  0.19086754 -0.15435612  0.0215457 ]\n [-0.40025157 -0.17416069  0.06744151  0.0973071   0.1354445 ]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08273753 -0.08607142  0.09750639  0.41754687  0.40933627]\n [-0.01675094  0.096788   -0.02909235  0.02621012  0.23977126]\n [ 0.03004065  0.43145156  0.09921236 -0.31694314 -0.01752997]\n [-0.15142806  0.29629537  0.19089955 -0.15476152  0.02086653]\n [-0.39887074 -0.17346337  0.06755817  0.09717359  0.13513687]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08335308 -0.08661193  0.09756655  0.4177233   0.4092913 ]\n [-0.01782439  0.09578901 -0.02925134  0.02634284  0.23980859]\n [ 0.02930824  0.430718    0.09906634 -0.3169243  -0.01734052]\n [-0.15171002  0.29584327  0.19078887 -0.15456022  0.02155215]\n [-0.39918828 -0.17406428  0.06722058  0.09745225  0.13610129]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08370472 -0.08693552  0.09780895  0.41826636  0.40968722]\n [-0.01826132  0.09522752 -0.02948375  0.02646956  0.23995396]\n [ 0.02840604  0.42967242  0.09832389 -0.31725535 -0.01749465]\n [-0.15280913  0.29442808  0.18965481 -0.15512766  0.02137598]\n [-0.40040743 -0.17589608  0.06558944  0.09651895  0.1355465 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 132] train=0.561619 val=0.663700 loss=61913.390137 time: 24.546182\n0\n\n[[-0.08412842 -0.087474    0.09719313  0.41754258  0.40851822]\n [-0.01810468  0.09555721 -0.02953057  0.02583034  0.23875402]\n [ 0.02821208  0.42988175  0.09808601 -0.31830624 -0.01919889]\n [-0.15363173  0.29393852  0.18886162 -0.15660231  0.019208  ]\n [-0.40154648 -0.17688967  0.06461069  0.09514759  0.13329579]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08312862 -0.08622882  0.09875948  0.4189825   0.4095948 ]\n [-0.01696229  0.09698001 -0.02766145  0.02757861  0.24015464]\n [ 0.02899886  0.43120757  0.09998434 -0.31631732 -0.01735523]\n [-0.15263835  0.29555175  0.19098793 -0.15419593  0.02152898]\n [-0.4001099  -0.17485398  0.06702417  0.0978349   0.1354968 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08446337 -0.08675136  0.09867059  0.41877687  0.40881503]\n [-0.01817057  0.09661788 -0.02780301  0.02735792  0.23957993]\n [ 0.0280473   0.4307944   0.09959015 -0.3167106  -0.01767583]\n [-0.15339181  0.29514733  0.19064626 -0.15460864  0.02115905]\n [-0.4006893  -0.17494367  0.06690697  0.0975623   0.13520801]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08483879 -0.08676564  0.09889222  0.4191859   0.40915492]\n [-0.01854891  0.09651525 -0.02778513  0.02764561  0.24009791]\n [ 0.0273325   0.43018115  0.09895483 -0.31712773 -0.01773126]\n [-0.15432324  0.2943561   0.18991968 -0.15520121  0.02065619]\n [-0.40152386 -0.17580748  0.06596019  0.09697232  0.13449445]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 133] train=0.559014 val=0.662600 loss=61804.054138 time: 29.119408\n0\n\n[[-0.08366778 -0.08558253  0.1008633   0.42175555  0.41200313]\n [-0.01682656  0.09830641 -0.02519328  0.0308997   0.24392278]\n [ 0.02936051  0.4325231   0.10180843 -0.31386834 -0.01370894]\n [-0.15199548  0.29690957  0.19286788 -0.15210858  0.0243205 ]\n [-0.39889553 -0.17319886  0.06891344  0.10014237  0.13778895]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08533312 -0.08735207  0.09933133  0.42024127  0.4108384 ]\n [-0.01873759  0.09626503 -0.02709574  0.02906451  0.24257994]\n [ 0.0275048   0.43063778  0.09990771 -0.31580013 -0.01511575]\n [-0.15418597  0.294635    0.19081742 -0.15414162  0.02254728]\n [-0.40168545 -0.17610276  0.06642616  0.09777637  0.13536157]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08521032 -0.08742628  0.09907339  0.4201731   0.4108856 ]\n [-0.01806768  0.09642822 -0.02742387  0.0288376   0.24239394]\n [ 0.02817243  0.43097454  0.09947719 -0.3163988  -0.01546771]\n [-0.15375464  0.2949101   0.19043286 -0.1546338   0.0223236 ]\n [-0.40153491 -0.1759628   0.06619874  0.09744413  0.13501044]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08373361 -0.08544795  0.1010621   0.42216617  0.41269493]\n [-0.01616415  0.09866262 -0.02528252  0.03117569  0.24455632]\n [ 0.03065316  0.43370718  0.10194541 -0.3138597  -0.01306956]\n [-0.15093738  0.29788932  0.19313285 -0.1518865   0.02497439]\n [-0.3989135  -0.1731248   0.06900021  0.10018951  0.13774465]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 134] train=0.561038 val=0.665100 loss=62006.383118 time: 27.533152\n0\n\n[[-0.08300712 -0.08497895  0.10069992  0.42166913  0.41231337]\n [-0.01591321  0.09862413 -0.02600748  0.03038154  0.24394266]\n [ 0.03049112  0.43324128  0.10076533 -0.31525657 -0.01436224]\n [-0.1516587   0.296994    0.19191343 -0.15331253  0.02319169]\n [-0.39949962 -0.1738105   0.06797035  0.09878237  0.13556038]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08441415 -0.0864455   0.09979708  0.42145452  0.4125922 ]\n [-0.01737376  0.09728155 -0.02668068  0.0302731   0.24434775]\n [ 0.02938702  0.43245503  0.10054509 -0.31526053 -0.01387405]\n [-0.15245001  0.296422    0.19184677 -0.1530689   0.02366725]\n [-0.3994584  -0.17372714  0.0683688   0.09946912  0.13602488]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08512387 -0.08786992  0.09834543  0.41953304  0.41040573]\n [-0.01793486  0.09613667 -0.02791718  0.02851193  0.2421871 ]\n [ 0.02923272  0.43189913  0.09979168 -0.3166116  -0.01565954]\n [-0.15247566  0.29606512  0.19146563 -0.15380265  0.0223    ]\n [-0.39924374 -0.17390518  0.06810012  0.09910819  0.13519889]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08197097 -0.08485263  0.10068257  0.4208919   0.41135684]\n [-0.0155194   0.09835406 -0.02642412  0.02942394  0.24289742]\n [ 0.03062484  0.43311375  0.10050473 -0.31605092 -0.01506594]\n [-0.15170053  0.29668894  0.1915609  -0.15372951  0.02239807]\n [-0.39811993 -0.17310101  0.06818214  0.09922581  0.1351066 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 135] train=0.556931 val=0.664700 loss=62194.230560 time: 23.851132\n0\n\n[[-0.08473255 -0.08763076  0.09806269  0.41863868  0.40925452]\n [-0.01776438  0.09613131 -0.02846231  0.02769144  0.2411346 ]\n [ 0.02887125  0.4315207   0.099191   -0.3171526  -0.01626717]\n [-0.15316163  0.29545906  0.19077513 -0.15409917  0.02202294]\n [-0.3994933  -0.17430887  0.06751852  0.09940643  0.13557772]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08451317 -0.08767342  0.09783563  0.41840854  0.40900278]\n [-0.01767419  0.09587494 -0.02872884  0.02749449  0.2407891 ]\n [ 0.0284674   0.43088394  0.09889346 -0.31718284 -0.0163423 ]\n [-0.15357919  0.29457664  0.19021277 -0.15422285  0.02220282]\n [-0.39949164 -0.17480575  0.06688865  0.09894999  0.13550925]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08397357 -0.08760621  0.09747907  0.41814566  0.40899467]\n [-0.01609598  0.09670779 -0.02860733  0.02751824  0.2409879 ]\n [ 0.03073696  0.43238842  0.09954966 -0.31679192 -0.01563974]\n [-0.15113148  0.2963097   0.19100155 -0.153631    0.02310359]\n [-0.3975252  -0.17358732  0.06711156  0.09926178  0.13623662]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08471442 -0.08888743  0.09654762  0.41815877  0.4094696 ]\n [-0.01751033  0.09492624 -0.02988291  0.02698262  0.24103361]\n [ 0.02910066  0.43057156  0.09824838 -0.31758636 -0.01587485]\n [-0.15274039  0.2946811   0.18995327 -0.15423548  0.02288905]\n [-0.39923087 -0.17515466  0.06622668  0.09882706  0.13583085]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 136] train=0.559395 val=0.664500 loss=61831.335419 time: 26.023248\n0\n\n[[-0.08453227 -0.0884352   0.09726534  0.41876954  0.40984854]\n [-0.01739584  0.0951776  -0.02945015  0.02739407  0.24108295]\n [ 0.02911484  0.4305949   0.09847222 -0.31714618 -0.01580912]\n [-0.15253776  0.29473436  0.19030407 -0.15380892  0.02280516]\n [-0.39873028 -0.17482445  0.06679264  0.09934054  0.13582769]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08467483 -0.08878345  0.09653188  0.41732907  0.40758955]\n [-0.01789316  0.09457423 -0.02993821  0.02659208  0.2394678 ]\n [ 0.02859561  0.429788    0.09787279 -0.31764102 -0.0167005 ]\n [-0.15248024  0.2943978   0.18994647 -0.15414654  0.02236706]\n [-0.39784005 -0.17454347  0.06660233  0.098782    0.13514093]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08450764 -0.08865957  0.09717789  0.41841528  0.4083144 ]\n [-0.01738095  0.09503745 -0.02912518  0.02752365  0.23991886]\n [ 0.02979504  0.43093154  0.09884347 -0.31717378 -0.01663688]\n [-0.1507789   0.29591116  0.19108726 -0.15373613  0.02235207]\n [-0.3960072  -0.17311312  0.06772578  0.09936781  0.1352821 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08471931 -0.0888      0.0972935   0.41883245  0.40834063]\n [-0.0188174   0.09388028 -0.02973622  0.02732407  0.23965397]\n [ 0.02735896  0.42888272  0.0972577  -0.31846434 -0.01780679]\n [-0.15404762  0.29289094  0.18834949 -0.15600598  0.0201358 ]\n [-0.39971894 -0.176728    0.06428975  0.09636015  0.1321224 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 137] train=0.558634 val=0.665900 loss=62026.172180 time: 28.370514\n0\n\n[[-0.08328446 -0.08719967  0.09915671  0.42065483  0.4097106 ]\n [-0.01728885  0.0957175  -0.02770315  0.02931371  0.24122451]\n [ 0.02983022  0.4313686   0.09969192 -0.3161478  -0.01579905]\n [-0.15073174  0.2959585   0.19123915 -0.15322587  0.02272843]\n [-0.3963655  -0.17361061  0.06717312  0.09935203  0.13515382]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08435567 -0.08828505  0.09799375  0.41914827  0.40811303]\n [-0.01851777  0.09449755 -0.02909442  0.02768312  0.23976608]\n [ 0.02868418  0.43025944  0.0985765  -0.3175212  -0.01712894]\n [-0.15143631  0.29530817  0.19077738 -0.15399456  0.0217977 ]\n [-0.3970294  -0.17414488  0.06694999  0.09899139  0.13455342]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08475509 -0.08835488  0.09811731  0.41913882  0.40812355]\n [-0.018693    0.0947945  -0.02854466  0.0281836   0.24016963]\n [ 0.02805538  0.43018582  0.09881397 -0.31705615 -0.01660228]\n [-0.15245515  0.2946822   0.19056454 -0.15379943  0.02239641]\n [-0.3982947  -0.17516282  0.06621428  0.09883289  0.13475892]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08500561 -0.08841657  0.098358    0.41922572  0.40763777]\n [-0.01872899  0.09503298 -0.02798462  0.02885333  0.2404924 ]\n [ 0.02849173  0.4310697   0.0998982  -0.31583664 -0.0157759 ]\n [-0.15146804  0.29625368  0.19234902 -0.15203494  0.02377017]\n [-0.39682007 -0.17323533  0.06827877  0.10093088  0.1365519 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 138] train=0.558694 val=0.663300 loss=61961.992249 time: 28.407798\n0\n\n[[-0.08524668 -0.08830206  0.09839697  0.4191504   0.40691406]\n [-0.01867303  0.0952576  -0.02793079  0.02887663  0.23998524]\n [ 0.02892687  0.4315411   0.10008895 -0.31592965 -0.01640981]\n [-0.15066381  0.29687548  0.19266336 -0.15218443  0.02298229]\n [-0.39597225 -0.17272064  0.0685358   0.10092027  0.13602091]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08532025 -0.08846273  0.09832859  0.4190155   0.40699798]\n [-0.01840327  0.09546432 -0.02747865  0.02948388  0.24072377]\n [ 0.02938891  0.4318862   0.10065023 -0.31531054 -0.0156184 ]\n [-0.15004225  0.29709145  0.19296786 -0.15193555  0.02310394]\n [-0.39525488 -0.17238215  0.06877676  0.10087485  0.13570447]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08498228 -0.08849925  0.09829586  0.4190889   0.40731302]\n [-0.01817343  0.09542372 -0.02750918  0.02971503  0.24136256]\n [ 0.02968968  0.43190596  0.10037631 -0.31532016 -0.01507591]\n [-0.14928356  0.29741257  0.19293709 -0.15165187  0.02378129]\n [-0.39436737 -0.1718081   0.06916589  0.10162447  0.13663891]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08635657 -0.09020701  0.09681083  0.4177127   0.4058833 ]\n [-0.01974122  0.09338652 -0.02952059  0.02787955  0.23960312]\n [ 0.02825799  0.4301791   0.0986354  -0.31711864 -0.016829  ]\n [-0.1500171   0.29641497  0.19180664 -0.15312505  0.02212913]\n [-0.39438874 -0.17209907  0.06874402  0.10094363  0.13560052]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 139] train=0.559275 val=0.664800 loss=62008.072906 time: 26.721061\n0\n\n[[-0.08664579 -0.09035517  0.09670223  0.4174364   0.40538687]\n [-0.02010562  0.0935763  -0.02918896  0.02793083  0.23936681]\n [ 0.02770573  0.4302417   0.09887715 -0.3172666  -0.01717207]\n [-0.15073432  0.2960504   0.19140029 -0.1540069   0.02127971]\n [-0.39481586 -0.17261145  0.06801053  0.10002434  0.13491882]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08781069 -0.09112652  0.09650277  0.41772008  0.40576184]\n [-0.0205087   0.09374341 -0.02850004  0.0288927   0.2405698 ]\n [ 0.02787579  0.43107423  0.10008544 -0.3161173  -0.01591652]\n [-0.15044855  0.29694438  0.1926525  -0.15271437  0.02253681]\n [-0.39449075 -0.17199327  0.06885721  0.10092998  0.13586095]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08778332 -0.0907979   0.09724586  0.41840538  0.40566942]\n [-0.02074889  0.09352832 -0.02854277  0.02871195  0.23990394]\n [ 0.02761508  0.4308907   0.09971716 -0.3170483  -0.01727989]\n [-0.150951    0.29669935  0.19241254 -0.15353195  0.02128587]\n [-0.39548022 -0.17281175  0.06813564  0.09994581  0.13463216]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08586136 -0.08912702  0.09835801  0.41911313  0.40647075]\n [-0.01925001  0.0950201  -0.02752535  0.02946142  0.24071662]\n [ 0.02885163  0.4323599   0.10079497 -0.3164655  -0.01677218]\n [-0.14955136  0.29848406  0.19376254 -0.1528254   0.02172517]\n [-0.39389884 -0.17092983  0.06952309  0.1007117   0.13501751]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 140] train=0.560657 val=0.663600 loss=61969.522125 time: 29.115745\n0\n\n[[-0.08632698 -0.08913022  0.09816153  0.41893366  0.40641013]\n [-0.01982086  0.09465118 -0.02808583  0.02903642  0.24056733]\n [ 0.02798409  0.43161958  0.09992336 -0.31727877 -0.01703946]\n [-0.1502583   0.29779118  0.19292787 -0.15369444  0.02128477]\n [-0.39446637 -0.17164777  0.06853171  0.09958351  0.13424635]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08645488 -0.08895828  0.09810058  0.4182886   0.40531528]\n [-0.0197373   0.09485867 -0.02816726  0.02848007  0.23973098]\n [ 0.02790095  0.43172184  0.099864   -0.31770334 -0.01762221]\n [-0.15035859  0.29771665  0.19262558 -0.15432984  0.02078138]\n [-0.39450333 -0.17190239  0.06779005  0.09853029  0.13347474]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08464785 -0.08766498  0.0990655   0.4194111   0.40595925]\n [-0.01822774  0.09607365 -0.02712364  0.02964672  0.2403207 ]\n [ 0.02902454  0.43301246  0.10108582 -0.3166132  -0.01700414]\n [-0.14942484  0.29880673  0.19372512 -0.15314318  0.02177974]\n [-0.39321613 -0.17095461  0.06874646  0.09978727  0.13462576]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08585514 -0.08879513  0.09821872  0.41891652  0.40548375]\n [-0.01985342  0.09455866 -0.02812896  0.02893649  0.23971556]\n [ 0.02739864  0.4316523   0.10020387 -0.31760496 -0.01799259]\n [-0.15079513  0.29771236  0.19282052 -0.15454996  0.02025061]\n [-0.39415887 -0.1719291   0.06769045  0.09839793  0.13316539]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 141] train=0.558373 val=0.668400 loss=61933.172913 time: 26.463548\n0\n\n[[-0.08691074 -0.09016618  0.09697565  0.4180257   0.40475786]\n [-0.02093177  0.09301186 -0.02974878  0.02762985  0.23888569]\n [ 0.02670037  0.43061846  0.0988333  -0.31908068 -0.01895468]\n [-0.15128578  0.29705557  0.19187802 -0.15582985  0.01921294]\n [-0.39450333 -0.17267816  0.06684206  0.09750916  0.13223511]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.0851654  -0.08858044  0.09905012  0.42040646  0.40725327]\n [-0.01993986  0.09382661 -0.02843208  0.02931192  0.2409714 ]\n [ 0.02709236  0.43100291  0.09962066 -0.31801385 -0.01730772]\n [-0.15142444  0.29704103  0.19224021 -0.15491223  0.02088044]\n [-0.39476755 -0.17314917  0.06668527  0.09837046  0.13389245]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08546789 -0.08972701  0.09777303  0.41920966  0.40580449]\n [-0.02022316  0.09294201 -0.02959423  0.02812677  0.23966433]\n [ 0.02700295  0.4305393   0.09869085 -0.319312   -0.01875546]\n [-0.15151261  0.29674533  0.19140503 -0.15646507  0.01902852]\n [-0.39467168 -0.17337422  0.06589259  0.09676352  0.13186401]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08595359 -0.09018309  0.09786455  0.41930196  0.40565416]\n [-0.02045847  0.09267773 -0.02976182  0.02794359  0.23945062]\n [ 0.02668443  0.4301632   0.09838742 -0.31951305 -0.01892057]\n [-0.15195905  0.29626614  0.19115116 -0.1565226   0.01888982]\n [-0.3951349  -0.17381871  0.06561441  0.09677757  0.1316933 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 142] train=0.558634 val=0.669900 loss=61950.904114 time: 24.571471\n0\n\n[[-0.08647675 -0.09054453  0.09753596  0.4187415   0.40469244]\n [-0.02031363  0.09275512 -0.02965335  0.02813574  0.23956862]\n [ 0.02716411  0.43053615  0.09873209 -0.31892806 -0.01819545]\n [-0.15119034  0.29698166  0.19194569 -0.15551831  0.01970999]\n [-0.39406636 -0.17295612  0.06645622  0.09782412  0.13232262]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08733607 -0.09080143  0.09759474  0.41791853  0.40342632]\n [-0.02080177  0.0931728  -0.02907832  0.02772437  0.23841746]\n [ 0.02697433  0.43103796  0.09932058 -0.31916702 -0.01892424]\n [-0.15117948  0.29726565  0.1923085  -0.15549083  0.01977964]\n [-0.39416513 -0.17279766  0.06682053  0.09820527  0.13305423]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08653204 -0.09005618  0.0985356   0.41854835  0.4039501 ]\n [-0.01944005  0.09441362 -0.02778968  0.02880778  0.23965575]\n [ 0.02854265  0.43243378  0.10052662 -0.31802425 -0.01726207]\n [-0.14939855  0.29839683  0.19295385 -0.15460038  0.02150908]\n [-0.3923917  -0.17196988  0.06709041  0.09893352  0.13467012]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08506018 -0.08871901  0.09950527  0.41903004  0.40397927]\n [-0.01884053  0.0951968  -0.0272457   0.02918697  0.23971494]\n [ 0.02858578  0.43278375  0.1008419  -0.3175148  -0.01672586]\n [-0.14916682  0.29876512  0.1933253  -0.15398127  0.02216538]\n [-0.39182684 -0.17149329  0.06722771  0.09929857  0.1349845 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 143] train=0.562760 val=0.665200 loss=61597.523560 time: 25.553104\n0\n\n[[-0.0855986  -0.08949044  0.09876812  0.41839987  0.40290546]\n [-0.01909979  0.09472855 -0.02777203  0.02869574  0.23870711]\n [ 0.02923632  0.43315208  0.10098118 -0.3176709  -0.01751652]\n [-0.14790475  0.29968116  0.19416668 -0.15357047  0.02195211]\n [-0.39029622 -0.1701375   0.0686226   0.10049509  0.13552685]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08672225 -0.09011849  0.09828437  0.41778097  0.40207475]\n [-0.02007657  0.09426412 -0.0279753   0.02845247  0.23835127]\n [ 0.02820993  0.43251452  0.10055018 -0.31816095 -0.01800448]\n [-0.14937843  0.298235    0.19285792 -0.15472518  0.02091626]\n [-0.39183316 -0.17206977  0.06663441  0.09878471  0.13393195]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08554413 -0.0890994   0.09920491  0.41870087  0.40321803]\n [-0.01890125  0.09564974 -0.02658726  0.02967962  0.23973279]\n [ 0.02930598  0.4340701   0.10218134 -0.31693363 -0.01669141]\n [-0.1483767   0.29960743  0.1941659  -0.15394905  0.02152041]\n [-0.39116988 -0.17123683  0.067205    0.09893599  0.13382834]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08714253 -0.09073134  0.09758954  0.4175239   0.4025734 ]\n [-0.02032389  0.09410969 -0.02805028  0.02850586  0.23923609]\n [ 0.02794413  0.43291923  0.10115014 -0.31778294 -0.01687043]\n [-0.15011017  0.29848835  0.19332902 -0.15454204  0.0216272 ]\n [-0.39275944 -0.17215917  0.06662942  0.0987817   0.13420732]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 144] train=0.559255 val=0.668700 loss=61853.376587 time: 23.460190\n0\n\n[[-0.08652429 -0.09061294  0.09748007  0.41713995  0.40184656]\n [-0.02005174  0.09388895 -0.02852781  0.02782281  0.23837274]\n [ 0.02821403  0.4329882   0.10098062 -0.3184542  -0.01789441]\n [-0.14978245  0.2988064   0.19352324 -0.15479842  0.02108429]\n [-0.3921155  -0.1716161   0.06705464  0.09905805  0.13424289]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08509351 -0.08858334  0.09978571  0.41946125  0.40420052]\n [-0.01930237  0.09514251 -0.0268487   0.029836    0.24055761]\n [ 0.02854059  0.43402857  0.10253605 -0.31635538 -0.01543967]\n [-0.14914887  0.30005893  0.19516908 -0.15242563  0.02386943]\n [-0.39102092 -0.17021155  0.06870323  0.10115068  0.1367737 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08689767 -0.09081978  0.09706881  0.4161339   0.4001579 ]\n [-0.02091914  0.09297733 -0.02945222  0.02686129  0.23714218]\n [ 0.02701019  0.43187997  0.09974698 -0.3195019  -0.01861011]\n [-0.15033875  0.2980601   0.19258909 -0.15514423  0.02106661]\n [-0.39232036 -0.17237473  0.06628646  0.09891397  0.13422656]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08600184 -0.09027444  0.09782272  0.41706452  0.4011353 ]\n [-0.01951306  0.09398153 -0.02824485  0.02824211  0.2385328 ]\n [ 0.02841708  0.43307474  0.10113131 -0.31812808 -0.01727086]\n [-0.1489712   0.29951823  0.19441587 -0.15335536  0.02246697]\n [-0.39059353 -0.17037727  0.06854029  0.10092411  0.13550895]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 145] train=0.559856 val=0.667500 loss=61815.884338 time: 22.943514\n0\n\n[[-0.0847559  -0.08883534  0.09922704  0.41869617  0.4025415 ]\n [-0.01855973  0.09500685 -0.02750308  0.02916636  0.23928888]\n [ 0.02906782  0.4338708   0.101431   -0.3181231  -0.01739701]\n [-0.14885494  0.29981884  0.19421297 -0.15391831  0.02174558]\n [-0.39115626 -0.1710809   0.06758656  0.09999797  0.13448142]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08623014 -0.0900212   0.09890147  0.41932693  0.40327626]\n [-0.01990476  0.09418719 -0.02745344  0.03001514  0.24018177]\n [ 0.0272856   0.4328049   0.10105239 -0.31797853 -0.01710658]\n [-0.15093936  0.29825735  0.1931595  -0.15469569  0.02091794]\n [-0.3933246  -0.17271698  0.06632955  0.09874189  0.1328577 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08691119 -0.09075873  0.09818923  0.4190209   0.403336  ]\n [-0.02075225  0.09321561 -0.0281406   0.02963835  0.24034382]\n [ 0.02673981  0.431927    0.10026548 -0.318595   -0.01707473]\n [-0.15129581  0.29735416  0.19196719 -0.15568742  0.02061374]\n [-0.39348254 -0.1738108   0.06455734  0.09732269  0.13206069]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08647563 -0.09032781  0.09839372  0.4193729   0.40355483]\n [-0.02071379  0.09330191 -0.02814264  0.02995793  0.24054953]\n [ 0.02673787  0.43219316  0.10045481 -0.3180965  -0.01670158]\n [-0.1507956   0.29823944  0.19272366 -0.15490682  0.02113782]\n [-0.39248866 -0.17234436  0.06585238  0.09870587  0.1331954 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 146] train=0.562240 val=0.664300 loss=61751.220886 time: 23.211770\n0\n\n[[-0.08696206 -0.0909692   0.09779002  0.4188501   0.40303984]\n [-0.02108147  0.09282694 -0.02836733  0.02986823  0.24022265]\n [ 0.02644395  0.4318802   0.10021712 -0.3185196  -0.01741411]\n [-0.15113246  0.29792652  0.19233443 -0.15559046  0.02006989]\n [-0.39269423 -0.1724791   0.06548877  0.09808996  0.13189606]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08783199 -0.0914294   0.09796291  0.41912732  0.40281036]\n [-0.02189868  0.09242271 -0.02830333  0.02995714  0.23998241]\n [ 0.02553368  0.4311122   0.09977339 -0.31887594 -0.01775506]\n [-0.15206812  0.29687485  0.19154714 -0.15602353  0.01972803]\n [-0.3939092  -0.1740239   0.06418993  0.09728029  0.13110545]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08865888 -0.09216392  0.09747363  0.41884676  0.40233254]\n [-0.02249338  0.0917221  -0.02879162  0.02977706  0.23981342]\n [ 0.02534664  0.43088466  0.09979488 -0.3186021  -0.01721981]\n [-0.15178579  0.29708079  0.19221774 -0.15490693  0.02112957]\n [-0.39324456 -0.1735067   0.06540248  0.09940042  0.1335505 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08734341 -0.09101485  0.0988169   0.42028973  0.40311173]\n [-0.02128975  0.09272476 -0.02769713  0.03105767  0.24068397]\n [ 0.02606442  0.43163243  0.10069756 -0.31741795 -0.01626053]\n [-0.15148503  0.29726684  0.19261394 -0.15427896  0.02140745]\n [-0.39298826 -0.17368051  0.0655347   0.09993363  0.1337012 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 147] train=0.556290 val=0.664100 loss=62193.249084 time: 23.286508\n0\n\n[[-0.08557041 -0.08993106  0.09936047  0.42038155  0.40303025]\n [-0.01966894  0.09369041 -0.02714423  0.03118748  0.24065754]\n [ 0.02752108  0.43273246  0.10158293 -0.31692043 -0.01619728]\n [-0.14996974  0.29862413  0.19377068 -0.153649    0.02151866]\n [-0.39098915 -0.17170689  0.06712292  0.10104489  0.13438444]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08584248 -0.09006939  0.09948245  0.42093724  0.40369314]\n [-0.02074138  0.09292836 -0.02754812  0.03120575  0.241063  ]\n [ 0.02574482  0.43133384  0.10050113 -0.31796047 -0.016866  ]\n [-0.15219231  0.2966526   0.1920526  -0.15547334  0.0198452 ]\n [-0.39357024 -0.17419758  0.06484848  0.09896478  0.1325395 ]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08657137 -0.09138776  0.0981837   0.42009023  0.40273586]\n [-0.02101337  0.09211543 -0.02851324  0.03062283  0.24031846]\n [ 0.02603844  0.43145046  0.10015537 -0.3184252  -0.01737331]\n [-0.1510109   0.29788148  0.1927461  -0.15530065  0.01969274]\n [-0.39179173 -0.17251189  0.06617709  0.09966575  0.1325107 ]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08784245 -0.09265825  0.0973815   0.419178    0.40187627]\n [-0.02186671  0.09099191 -0.02941893  0.02970114  0.23951514]\n [ 0.02565871  0.4309278   0.09978712 -0.31893095 -0.01769886]\n [-0.151143    0.2978956   0.19313543 -0.15492828  0.02028893]\n [-0.39189327 -0.17268042  0.06647463  0.10040887  0.13374235]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 148] train=0.560156 val=0.667800 loss=61941.725616 time: 22.801399\n0\n\n[[-0.08705185 -0.09204177  0.09809407  0.41984907  0.4025335 ]\n [-0.02132713  0.0913574  -0.02914556  0.02999111  0.24014868]\n [ 0.02623565  0.4314183   0.10018976 -0.31862137 -0.01703477]\n [-0.15023722  0.29878026  0.19410828 -0.15409523  0.02107362]\n [-0.39091498 -0.17173769  0.06741179  0.10125571  0.13422264]]\n<NDArray 5x5 @cpu(0)>\n50\n\n[[-0.08629967 -0.09160759  0.09817566  0.42012513  0.40285495]\n [-0.02113536  0.09146863 -0.02946378  0.0298938   0.24046357]\n [ 0.02614472  0.43135557  0.09965729 -0.31911078 -0.016987  ]\n [-0.15067373  0.29836497  0.1934034  -0.15478404  0.02065998]\n [-0.39148086 -0.1725387   0.06655616  0.10050172  0.13349766]]\n<NDArray 5x5 @cpu(0)>\n100\n\n[[-0.08529637 -0.09089452  0.09821095  0.4199793   0.4024314 ]\n [-0.02020149  0.09239704 -0.02913204  0.03005271  0.24054796]\n [ 0.02733487  0.4323026   0.09986491 -0.31907094 -0.01656052]\n [-0.14937536  0.2991478   0.19351201 -0.15485413  0.02095977]\n [-0.39056566 -0.17224708  0.06631883  0.10031514  0.13347517]]\n<NDArray 5x5 @cpu(0)>\n150\n\n[[-0.08433833 -0.08992733  0.09921802  0.4217409   0.4045174 ]\n [-0.01972021  0.09296168 -0.02811348  0.03222807  0.24327303]\n [ 0.0270582   0.43212485  0.10051265 -0.31709272 -0.01376781]\n [-0.1497434   0.29887465  0.19405323 -0.1530733   0.02365823]\n [-0.39071012 -0.17233633  0.06682131  0.10191976  0.13594992]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 149] train=0.560557 val=0.666200 loss=61921.236908 time: 23.909120\n"
 }
]
```

```{.python .input  n=120}
print net[0].weight.data()[0][0]
```

```{.json .output n=120}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[-0.08655212 -0.09304737  0.09627281  0.4197063   0.40260616]\n [-0.02112781  0.0905636  -0.03056227  0.03058454  0.24193658]\n [ 0.02636526  0.43079594  0.09913642 -0.31822014 -0.01492882]\n [-0.15010849  0.2983658   0.19384784 -0.15322928  0.02273046]\n [-0.39082694 -0.17250638  0.06688458  0.1019552   0.13519903]]\n<NDArray 5x5 @cpu(0)>\n"
 }
]
```

```{.python .input  n=132}
print train_history.plot()
```

```{.json .output n=132}
[
 {
  "data": {
   "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VNX5+PHPyWTfd7ICISxJ2CEEEJFdwQXrgmK17li3+tX2Z8VatVrbamut1qJ11ypVEa2l7oIgoogQQAiEJWwhCWQl+zqZ8/vjDCGEhIQwySTD83698krm3jN3nrlJnnvm3HOfq7TWCCGEcC1uzg5ACCGE40lyF0IIFyTJXQghXJAkdyGEcEGS3IUQwgVJchdCCBfUbnJXSr2qlCpQSmW0sV4ppf6ulMpSSm1RSo1xfJhCCCFORUd67q8Ds0+yfg4wyP51C/D86YclhBDidLSb3LXWq4GSkzS5GPiXNr4HgpVS0Y4KUAghxKlzd8A2YoGDzR7n2JcdatlQKXULpnePn5/f2KSkJAe8vBBCnDnS09OLtNYR7bVzRHLvMK31i8CLAKmpqXrDhg3d+fJCCNHrKaUOdKSdI2bL5ALxzR7H2ZcJIYRwEkck92XAtfZZMxOAMq31CUMyQgghuk+7wzJKqbeBqUC4UioHeBjwANBa/xP4BDgfyAKqgRu6KlghhBAd025y11pf1c56DdzhsIiEEKetoaGBnJwcamtrnR2K6CRvb2/i4uLw8PDo1PO79YSqEKJ75OTkEBAQQP/+/VFKOTsccYq01hQXF5OTk0NCQkKntiHlB4RwQbW1tYSFhUli76WUUoSFhZ3WJy9J7kK4KEnsvdvp/v4kuQshhAuS5C6E6BKlpaU899xzp/y8888/n9LS0pO2eeihh1i+fHlnQzsjSHIXQnSJtpK71Wo96fM++eQTgoODT9rm0UcfZebMmacV36loGXN77+EorTU2m60rQmqXJHchRJdYuHAhe/bsYdSoUYwbN47Jkyczd+5cUlJSAPjJT37C2LFjGTp0KC+++GLT8/r3709RURH79+8nOTmZBQsWMHToUM4991xqamoAuP7661m6dGlT+4cffpgxY8YwfPhwduzYAUBhYSGzZs1i6NCh3HzzzfTr14+ioqIT4qyqquLGG28kLS2N0aNH89///heA119/nblz5zJ9+nRmzJjBqlWrTngPTz31FMOGDWPYsGE8/fTTAOzfv58hQ4Zw7bXXMmzYMA4ePHjCa3YHmQophIt75H/b2J5X7tBtpsQE8vBFQ0/a5vHHHycjI4PNmzezatUqLrjgAjIyMpqm9r366quEhoZSU1PDuHHjuOyyywgLCztuG7t37+btt9/mpZde4oorruD999/nmmuuOeG1wsPD2bhxI8899xxPPvkkL7/8Mo888gjTp0/n/vvv57PPPuOVV15pNc4//OEPTJ8+nVdffZXS0lLS0tKaPhVs3LiRLVu2EBoayqpVq9i4cWPTe0hPT+e1115j3bp1aK0ZP348U6ZMISQkhN27d/PGG28wYcKEzuxeh5CeuxCiW6SlpR03Z/vvf/87I0eOZMKECRw8eJDdu3ef8JyEhARGjRoFwNixY9m/f3+r27700ktPaLNmzRrmz58PwOzZswkJCWn1uV988QWPP/44o0aNYurUqdTW1pKdnQ3ArFmzCA0NbfU9rFmzhksuuQQ/Pz/8/f259NJL+eabbwDo16+fUxM7SM9dCJfXXg+7u/j5+TX9vGrVKpYvX87atWvx9fVtSqoteXl5Nf1ssViahmXaamexWNodD1+0aBEvvfQSYMb3tda8//77DBky5Lh269atOy7mlu/hZDraritJz10I0SUCAgKoqKhodV1ZWRkhISH4+vqyY8cOvv/+e4e//qRJk1iyZAlgeudHjhwB4I477mDz5s1s3ryZmJgYzjvvPJ599llMJRXYtGlTh7Y/efJkPvzwQ6qrq6mqquI///kPkydPdvj76CxJ7kKILhEWFsakSZMYNmwY995773HrZs+ejdVqJTk5mYULF3bJEMbDDz/MF198wbBhw3jvvfeIiooiICDghHYPPvggDQ0NjBgxgqFDh/Lggw92aPtjxozh+uuvJy0tjfHjx3PzzTczevRoR7+NTlNHj1bdTW7WIUTXyczMJDk52dlhOFVdXR0WiwV3d3fWrl3LbbfdxubNm50d1ilp7feolErXWqe291wZcxdCuKTs7GyuuOIKbDYbnp6eTePsZ4pel9z3FVWxr6iSUD8vQnw98PGw4OluRpds2lw0oAGb1qDty9BoDY02TU1DIzX1jcSH+hLq5+ncNyOE6DKDBg3q8Pi5K+p1yT1z1RLqf1xCObD/NLazD/DxsODuduy0g1IQ5ONBdLAPYdN/gVf/8acZrRBCOEevS+5TYmxYsvOw2TSN2vTItdY01U9rUUmtZV01N6VQChoabdRbbTQ/46A11JU3El5WyMo3y3G//CVmJPfpwncjhBBdo9cld7+zboKzbuqy7VsbbZS9PJfE/GxmvrGBB85PZsE5A7rs9YQQoivIVMgW3C1uhCWMJlHlMibWj4+3yr2+hRC9jyT31vQZhmqs54LYajJyy6ipb3R2REK4PH9/fwDy8vK4/PLLW20zdepU2ptC/fTTT1NdXd30uCMlhF2RJPfW9DEV3yb4HcZq02w+eOb9YQjhLDExMU0VHzujZXLvSAlhR2psbDzp47Z0tIxwR0lyb034YFAWEnU2SsGG/SXOjkiIXmfhwoUsWrSo6fHvfvc7HnvsMWbMmNFUnvdoed3m9u/fz7BhwwCoqalh/vz5JCcnc8kllxxXW+a2224jNTWVoUOH8vDDDwOmGFleXh7Tpk1j2rRpwLESwtB2id62Sgu39NZbb5GWlsaoUaP4+c9/3pS4/f39+dWvfsXIkSNZu3Yt/fv357777mPMmDG89957bN68mQkTJjBixAguueSSplIIU6dO5e677yY1NZVnnnnmtPZ3S73uhGq3cPeC8EF4l+xgcOR01h844uyIhOi8TxfC4a2O3WbUcJjz+EmbXHnlldx9993ccccdACxZsoTPP/+cu+66i8DAQIqKipgwYQJz585t836hzz//PL6+vmRmZrJlyxbGjBnTtO4Pf/gDoaGhNDY2MmPGDLZs2cJdd93FU089xcqVKwkPDz9uW+2V6G2vtHBmZibvvvsu3377LR4eHtx+++0sXryYa6+9lqqqKsaPH89f//rXpvZhYWFs3LgRgBEjRvDss88yZcoUHnroIR555JGmg0t9fX27Q02dIcm9LX2GwsH1pPYP4b+b82i0aSxucsNhITpq9OjRFBQUkJeXR2FhISEhIURFRXHPPfewevVq3NzcyM3NJT8/n6ioqFa3sXr1au666y7AJMgRI0Y0rVuyZAkvvvgiVquVQ4cOsX379uPWt9S8RC/QVKJ37ty5HSotvGLFCtLT0xk3bhxgPlVERkYCphrlZZdddlz7K6+8EjBF0kpLS5kyZQoA1113HfPmzTuhnaNJcm9LZApkvM/EWE8Wr7Oy43A5Q2OCnB2VEKeunR52V5o3bx5Lly7l8OHDXHnllSxevJjCwkLS09Px8PCgf//+rZb6bc++fft48sknWb9+PSEhIVx//fWd2s5RrZUWPnjwIBdddBEAt956K1prrrvuOv70pz+d8Hxvb28sFstxy5xdHljG3NvSx9TATvM7DMCG/TI0I8SpuvLKK3nnnXdYunQp8+bNo6ysjMjISDw8PFi5ciUHDhw46fPPOecc/v3vfwOQkZHBli1bACgvL8fPz4+goCDy8/P59NNPm57TVqnhUy3RGx8f31Qa+NZbb2XGjBksXbqUgoICAEpKStqNHyAoKIiQkJCmG3m8+eabTb34riQ997ZEmhkzEdV7iA7qx7p9xVx3Vn/nxiRELzN06FAqKiqIjY0lOjqaq6++mosuuojhw4eTmppKUlLSSZ9/2223ccMNN5CcnExycjJjx44FYOTIkYwePZqkpCTi4+OZNGlS03NuueUWZs+eTUxMDCtXrmxa3rxEL9BUoretuzu1lJKSwmOPPca5556LzWbDw8ODRYsW0a9fv3af+8Ybb3DrrbdSXV3NgAEDeO211zr0mqdDSv62RWt4vC8Mu4xf193AZxmH2fjgLNwt8mFH9HxS8tc1nE7JX8lUbVEK+p0Fe1cyZVAE5bVWNsl8dyFELyHJ/WQGzoQj+zknvByLm2LVzgJnRySEEB0iyf1kBs4AIODgKsb0DWbVzkInByRExzlryFU4xun+/iS5n0zoAAhNhKzlTB0Syba8cgoqOj/dSoju4u3tTXFxsST4XkprTXFxMd7e3p3ehsyWac/AmbDxX0ydHMBfgNW7irh8bJyzoxLipOLi4sjJyaGwUD5t9lbe3t7ExXU+10hyb8+gWfDDC6TUbyUiwIvPMg5Jchc9noeHBwkJCc4OQziRDMu0p98ksHihdn3GVWl9WZ5ZwLa8MmdHJYQQJ9Wh5K6Umq2U2qmUylJKLWxlfV+l1Eql1Cal1Bal1PmOD9VJPH0h5WLYvJibR/sT4O3OM8t3OzsqIYQ4qXaTu1LKAiwC5gApwFVKqZQWzX4LLNFajwbmA885OlCnmnIfWGsJTF/EzWcP4Ivt+WTkSu9dCNFzdaTnngZkaa33aq3rgXeAi1u00UCg/ecgIM9xIfYA4QNhxHxY/zI3jvImyMeDp5fvcnZUQgjRpo4k91jgYLPHOfZlzf0OuEYplQN8AvyitQ0ppW5RSm1QSm3odWfxp9wLjQ0ErF/EgskJLM8s4Ee5YlUI0UM56oTqVcDrWus44HzgTaXUCdvWWr+otU7VWqdGREQ46KW7SegAGH45bHqL61PDCfaV3rsQoufqSHLPBeKbPY6zL2vuJmAJgNZ6LeANhONqxi2A+gr8d37Az89JZOXOQjZmSylgIUTP05Hkvh4YpJRKUEp5Yk6YLmvRJhuYAaCUSsYk91427tIBcakQNQLWv8K1E/oS6ufJ45/uoNEmVwEKIXqWdpO71toK3Al8DmRiZsVsU0o9qpSaa2/2K2CBUupH4G3geu2K1z0rBeNuhoJt+OWvZ+GcJH7YV8ILq/c4OzIhhDiO1HM/VfXV8NckiB2Dvnopd767hc8zDvPerRMZ3TfE2dEJIVyc1HPvKp6+MONB2LsS9emv+eNPhtEn0Jvffpjh7MiEEKKJJPfOSFsAZ90FG14hKP1Zbjo7gW155ewrqnJ2ZEIIAUhy77yZj8DwebDi91zstxWATzMOOTkoIYQwJLl3lpsbzH0WooYR9vmdzI6p5tOth50dlRBCAJLcT4+HD1z5Fig3fqtfYmtuGQdLqp0dlRBCSHI/bSH9IelCouv2AfBZhvTehRDOJ8ndEQJjsVQXMjLah4+3yri7EML5JLk7QmAMoLkyyYPNB0vJKqh0dkRCiDOcJHdHCDRFMmf3A4ub4r30g+08QQghupYkd0cIjAEg1FrItCGRvJ+eS0OjzclBCSHOZJLcHSEw2nwvz+OK1DiKKuv4eqfr1U0TQvQektwdwTsYPHyhPI9pSZGE+3vy7gYZmhFCOI8kd0dQygzNVOThYXFjXmo8KzLz2VsoJ1aFEM4hyd1RAmOg3Nw69sZJCXi6u/GPlVlODkoIcaaS5O4oAceSe0SAF1eP78d/N+exX4qJCSGcQJK7owTGQMUhsDUC8PNzBuDuplgkvXchhBNIcneUwBiwWaHKzJKJDPTmqrS+fLApV+rNCCG6nSR3R7FfyHR0aAbgtqmJWKT3LoRwAknujtJsrvtRfQK9uWpcPEvTc8g5Ir13IUT3keTuKK303AFunZqIm1I8t0puoi2E6D6S3B3FNxzcPKA897jF0UE+XDkunnfXH2T9/hInBSeEONNIcncUNzczNFNxYsnfe2cPIT7EhzsWb6Swos4JwQkhzjSS3B0pMBbKck9c7O3B89eMpby2gbve3oTNpp0QnBDiTCLJ3ZGC+8GBNfDKebDri+NWJUcH8vBFQ1m7t5hP5EbaQoguJsndkc77I8x42AzNLL2h6YKmo65IjWdInwD++sUuKQkshOhSktwdyS8MJv8Spv0G6iuhcOdxqy1uinvPG8K+oiqWSNVIIUQXkuTeFWLHmu+56SesmpEcSWq/EJ5ZvpuqOms3ByaEOFNIcu8KoYngFdRqcldKcf/5SRRU1EnVSCFEl5Hk3hXc3CB2dKvJHWBsv1AuHRPLy9/sZY/UfBdCdAFJ7l0ldizkb4OGmlZX3z8nGW93C79btg2tZWqkEMKxJLl3ldixoBvh0JZWV0cEeHHPrMF8s7uIz7fld3NwQghXJ8m9q5zkpOpR107sx5A+Afz+o+3U1De22U4IIU6VJPeuEhBlrljN29hmE3eLG49ePJTc0hqeWyUnV4UQjiPJvSvFjoGsFbD6SSjIbLXJ+AFhXDwqhhe+3svmg6XdHKAQwlV1KLkrpWYrpXYqpbKUUgvbaHOFUmq7UmqbUurfjg2zl0q9CQKi4avfwz8nw4HvWm324IUp9Any4sbX18vsGSGEQ7Sb3JVSFmARMAdIAa5SSqW0aDMIuB+YpLUeCtzdBbH2PonT4Pbv4Fc7IaQfvPszKD3xytRwfy/+deN4FHDtKz9I5UghxGnrSM89DcjSWu/VWtcD7wAXt2izAFiktT4CoLUucGyYvVxAFFz1DjTWw7vXgO3EujIJ4X68dsM4iqvquGPxRuqtUntGCNF5HUnusUDz7maOfVlzg4HBSqlvlVLfK6Vmt7YhpdQtSqkNSqkNhYWFnYu4twofBDMegkOboWRvq01GxAXzxGUj+GF/CY99vL2bAxRCuBJHnVB1BwYBU4GrgJeUUsEtG2mtX9Rap2qtUyMiIhz00r1I3wnme96mNptcPCqWBZMT+NfaA9y3dAvV9VJ/Rghx6jqS3HOB+GaP4+zLmssBlmmtG7TW+4BdmGQvmotIBnefk06PBFg4J5nbpyayJP0gFz67hm92n2GfcoQQp60jyX09MEgplaCU8gTmA8tatPkQ02tHKRWOGaZpfezhTGZxh+gRkHvy5G5xU/x6dhKLbx5PvdXGz175gZ+9so5d+RXdFKgQordrN7lrra3AncDnQCawRGu9TSn1qFJqrr3Z50CxUmo7sBK4V2td3FVB92oxo+HwFmhsf7jlrMRwVvxqCg9dmMLW3DLOf+Yb/vzZDuqscjWrEOLklLOKVqWmpuoNGzY45bWd6sd34T+3wG3fQZ+hHX5aSVU9f/wkk6XpOVwwPJpnrxqNm5vqwkCFED2RUipda53aXju5QrW7xY4x39sZmmkp1M+TJ+eN5DfnJ/Hx1kM88dmOLghOCOEqJLl3t9BE8Ao86YyZk1kweQA/m9CPF1bv5abX17N8ez6NNikZLIQ4nruzAzjjuLlB9Mh2Z8y0RSnFwxelEOrnyb9/yGbFvzYwMi6IP146nKExQQ4OVgjRW0nP3Rlix8DhDKiv7tTT3S1u3DNrMN8tnM5TV4wkt7SGuf/4lvuWbpHaNEIIQJK7cwyeDbYG+PFt8zhvM7x1OdSd2lRHD4sbl46JY/kvp3DN+L58uDmXmU99zcL3t1Be29AFgQsheguZLeMMWsNL00wyv30dvDLTjMH/dAkMPq/Tmy2qrOOFr/fwypp99An05iejYwnz8+TsQeEkRQU68A0IIZxFZsv0ZErBxDuhOAs+WHDs5OpJ7trUEeH+XjxwQQof3D6JMH9PXlq9l8c+zmTOM99w39ItHCpr/X6uQgjXIz13Z2lsgGdGQXkOxKZCQ7Wp/f6zDxz2Elpriirr+efXe/jX2v002jRTBkdw/vBo0hJC6Rvqi1IyV16I3qSjPXeZLeMsFg+YeAd88QDMeQI2vgHbl5khGwclXKUUEQFePHhhCtef1Z931x9kaXoOK3eaWjUDI/1ZMDmBSQPDKaioIzLAi7gQX4e8thDCuaTn7kw2m+m5B/eF9Nfhf/8Hv9gIYYld+JKarMJK1u0r4e112Ww/VN60ztPixi/PHcyFI6J5+4dsquoauWfWYIJ8PLosHiHEqeloz12Se09xOAP+OQkufQlGXNEtL6m15rs9xRworqZPoBdL03P4NOMwAG7K9PzjQnxY9NMxDIuVOfRC9AQyLNPbRCSBh685qdpNyV0pxaSB4UwaaB5PT4rk462HyCqo5LIxcRRU1HL74o1c+OwakqICmJ4UyYzkPoyKD8YidW2E6NGk596TvDrH3IpvwQpnR9KkpKqe99NzWLEjn/X7j9Bo04T6eTJ1SASD+wSwZncROUeqeeCCFGal9HF2uEK4PBmW6Y2++C2sewHuzwV3T2dHc4Ky6ga+3l3IV5n5rNxZSFlNA4kRfljcFLvyK7n+rP5EBHhR19DIRSNjGNQnwNkhC+FyJLn3RpkfwbtXw/Ar4IK/gnfPvfDI2mijpKqeyEBvahsa+d2ybbyz3txqVykz6eesxDD8vNyps9qYOCCMi0ZGy2wcIU6TJPfeyGaD1X+Gr5+AoHi46QsIiHJ2VB1WUlWPr6eF6vpG3vr+AMt+zMPdPja/47AprTBtSATXndUfD4sbuaU1nJUYJglfiFMgyb03O7AW3rgQxt1s5sC7gOziat7fmMPidQcoqqxvWu5pceOqtHiCfDzYfqgCD4uZm59dUs3mg6XEBvswf1w8Zw+KIDLACz+vY3MAGm26aVaPEGcKSe693Yd3QMZSuHsr+Ec6OxqHqW1o5JvdRfh5Wgjx8+SN7/azZIMZzkmM8MemNQXldUQFeTMqPpiMvHIym83F9/dyJzLAiwabjUOltfQN9eXxy0aQlhBKbUMjm7JL+TariNqGRualxjMkSsb9hWuR5N7bFWXBonFw1i9g1qPOjqZLlVTV4+NhwcfTcsI6rTXbD5Wz83AFBRV15JfXUlBeh8VNERPsw8db88g5UsOgSH/2FlZhtWksbgqLUtQ32hgZF8RZA8OZlBjOhAGhuFuOL6dU29BoKmhqiAz07q63LESnSXJ3BUtvgl2fwQ2fmBt8iBNU1Vl5evkuduZXMjw2kNHxIYwfEIq1UdsvyjrElpwyrDZNuL8X4xNCySurIedIDWU1DdRbbU3bmjwonJ+m9aWyzsrhslr8vd0J9fMk1M+TiAAvBkUGyPx+4XSS3F1B4U54eSbUlUO/s+GiZyB8IDTUwNp/wIDpEDfW2VH2eNX1VlbvKuQ/m3LZlldO31Bf4kN8CfbzINDbg0AfD45U1fPm9wcorKhrczuRAV7MSO5DYUUtmYcquG9OEnNHxnTjOxFCkrvrqDkCG9+ENX8DN3e44g1Y8ShkrzXrR/8Mzv09+IQ4N04XUGdtZHN2KZGB3kQHeVNd30hJVR3FlfXkHKnhy+35fL2rkJhgb2wa8str+egXZzMgwt/ZoYsziCR3V1O4C/51MVTkgZuH6cUXZsL3z0P0KLhuGXj6OTvKM8ahshrmPPMNscE+fHD7WXi5n3i+QIiuIDfrcDURg+HGzyDpQrj6PRh9NZz7GMx7w9xse8m1pka86BbRQT48eflItuWV8/inO5wdjhAnkOTem4T0g/mLIXHasWXJF8KFT0PWcvj3lVBT6rz4zjAzU/pw/Vn9ee3b/Szfnu/scIQ4jlSFdAVjrzPfP/4lvDwD+k6A/O0w7FIzlVJ0mfvPT2L9/hL+39IfmTsyhvX7jxDu78nExDAmJYYzLDYIi5uiss5KdnE12SXVlFTVU1VnJSHcj+lJkbi5KbTWaA1u9tk4h8tqqa43beQiLdEZMubuSg58B0tvBGudKVtQsB3O+xP0Pxu++j0MmGru/iQcam9hJXP/8S02rRnbL4SC8jp25ptyCwHe7nha3Ciuqm/1uQMj/UmJDmTt3mJKquqJCvSmodFGgX3WTlyID8NjgyiuqqfRphkZF8yIuCBC/Dzx9bRgbdTkldaw7Mc8tuaWMXdkDLdOSSQqqPU5+3XWRtbuKeZgSTUlVQ2cMzic0X3bPxlf29BI+oEjeHtYGB0f3HQQcpR6qw1PdxlI6Ag5oXqmstlM5S5bIyy9ATKXgXIDbYOAGPjldofdxk8cU1bTgK+nBQ/7RVKFFXWs3VvM2j3FgCY+1Jd+oX70DfUlIsALH08LX+8q5J+r9lBUWcfExDBig304XFaLBkbEBeHp7sbKHYXsLaok3N8LrTVbcsqoazY3/6jYYB+GxgTy1Y4CGrWZ0x8Z4EWgtwd+Xu4EeJsP6V/tKKCs5vhzM+cMjsDT4saPOaX0CfTi7IER+HtZKKyoo6iynoKKWjJyy6lpaATMjdhjQ3w4XFaDRSkSI/1JjPAnMcKPiAAvGho1VpvNfLf/XFRZT0ZuGeU1DfxkdCzTkiLZnF3Kmqwivs0yZaPnDI9m3tg4Mg9VkH6gBFD4eFqICjS3f0yODmRAhB+rdhby1Y58pgyOYN7YeCrqrLy34SDuboohUYHEBvsQ4ueBv5d7u596cktrOFJVT3J0YJdcw1Db0MiKzAJGxgc5rIaSJHdhevAf/RJ8giEoDj5bCLesgpjRzo5MdFK91UZ2SRVlNVaq661Y3BSB3h6kRAfi5qY4WFLNfzblkldaQ0FFHRW1DVTWNVJZ10Btg6nOecnoWIbGBOLlYWHxugO89u1+ArzdGRkXTM6RajZll2K1aQK83YkI8CLc34vkqADOGRxBVX0jX27Pp7T62KeMvUVV7CmopKq+sc243ZT5lOKmVFMROYAAL3cmJIYRFejNh5tyqaizAqath8WN6npzQVnLA1qQjwdlNQ2MiAviQHH1CQcsgD6BXqT2CyUlJpD4UF/ySmtYs7vIfAKKD2ZfUSVfbs/Hps32+ob6UlJVT3W9FTel8LC4EehjLmTrH+aHt4eF9ftLyCqoxN1NEervybyx8cxIjmRFZgFrdhfh5eFGmJ8no+KDiQz05onPdnCguBqlYOrgCAZE+OPraWFmch9Gxgd36m9Akrs4XlUxPDkQzrkXpv0GynLM8qA458Ylepya+kaUAm+Pjk/v1FqTX15HSVU9HhaFu8UNdzeTIN0tCn8vd7w9LGit2ZhdyqbsI4zpF8KI2KCmkhDltQ38sLeE4XFB9GlWCkJrTWFFHVtzy9iVX0lq/xDG9g1haXoOz6zYTVIb8kruAAASxElEQVRUAPfMGkxkgBc78yvIL6+juLKO7YfK2bD/CLmlNU3bSooKwNPdje155QR4u/PT8X0ZFBnA2j3FHC6vJczPEz8vdzSaugYbFbVWCivr2F9URVW9lTF9QxgaE4hNw+6CSlbvKmza9si4IJRS5JfXcqisFoD+Yb4snJPEtrxyPtycS0llPdUNjfzxkuFclda3U78fSe7iRK/OgfoKuO5/8I808PKHO9PBTcY6heuqrreSXVJNiK9n00GjtqERi/3g01Fa6xOGebIKKli3r4RzBkUQH3ps2OVgSTVZBZVMTAw74SDZ8uT5qZJ7qIoTDZkDXz4I798MVQXmK+tLGHyesyMTosv4erqTFHX8jW9O5VPJUa2N3w+MDGBg5ImVR+NDfY9L9i230x2nvaTLdiYZcr75nrUcJt4JAdGw7p8nf05BJuz6vOtjE0I4lPTczyThAyEiGWwNMP235kTrV49BTjrs/hzqq8xyDx+oyIeVj8Gmt8xMmyv+BSkXO/sdCCE6SMbczzRluWDxMDcAqSqCp1KgsR6w/x30GQ5DZsPa58zytAVw8AdTofKWlRA+yKnhC3Gmc2htGaXUbKXUTqVUllJq4UnaXaaU0kqpdl9YOElQ7LE7O/mFw9l3w8AZsOAr+OkSKDsIq/8CA6fDHetg9p9MJUp3T3j3GqgtM89tbIACqakiRE/Vbs9dKWUBdgGzgBxgPXCV1np7i3YBwMeAJ3Cn1vqk3XLpufdQ5YegMh9iRh2/fO/X8NalkHAOXPYKvHcd7FsNA6bBrEeO3UykZC9seBVGX2uKnTVXetBMvZSLqIToNEf23NOALK31Xq11PfAO0Nrg6++BJ4DaU4pU9CyB0ScmdoABU+DCv8Ger+DpEabUwbgFcOhHeOEcMwNn02J4YSp89yw8PxG+fAis9svu96yEp4fBikeObbPRCs07F9b64x8LITqtIydUY4GDzR7nAOObN1BKjQHitdYfK6XubWtDSqlbgFsA+vbt3AR+4URjrjUXP/3wElz5pqlOOeNBWPO0qSu/9T3Tg7/gb6b3/u0zJoGf+3v4/AFAmbYJU6C6GP53t5lr33eC6dXnbYIpv4ap9pG/wp3gEwr+EU5920L0Rqc9W0Yp5QY8BVzfXlut9YvAi2CGZU73tYUTTPsNTFl47MIn7yCY+bA58Zq1HIbPM7Nt4saam4d8v8jMpy/YBj953iT3d66GhiqIHw+BseaEbVAsRA03B4TUm6C+El6YYu4wde2HEDHEue/7wFpz4PJ0TH0QIbpaR5J7LhDf7HGcfdlRAcAwYJV9kn8UsEwpNbe9cXfRS7V2RWtgjOnZNzfrUcj+zvTo48fDyKsgagS8caH9ZiN/MCdqjyrKgkVpsOYpM7/ezR1sVnh1Npz3B4hMhj7DzGyf7rRnJbz5E4hMMVNCZcaQ6AU6ckLVHXNCdQYmqa8Hfqq13tZG+1XA/5MTqgKAot3w0T1w3h8heoRZZrO1XfLgv3eYsXs0nP8kJE43J3KP7Dfro0bAzz4EvzAzP1/bIH6cWbfjEyjNhvE/d+xJ23d/Bvu+NgebhlrzSSI+zXHbF+IUOKz8gNbaqpS6E/gcsACvaq23KaUeBTZorZedfrjCZYUPgus/On7ZyWrZnPNr+PFdU7ky9SbT9o71UJwFOT/AJ7+GNy6CuFTY+AagYPoDJvEu/53ZxuGt5h6zFndzMVbuBnMQSLqw9aSvtbkRuW+oeVxZAN88BZPuAmWBnZ/A+Fthwu3w6nnwv/+Dn6/u/k8QQpwCuYhJ9DyHt5opkz6t3ERi7yr493xorDPJtjLfDPsADL0UwhLNPP2gvlBXDrXNbjs4eDZc/Jzp9deWm3vP7v0aMt438/t/ugQGzTLz+TP/Z8bYB51rtndnurnCN/N/Zv15f4KJt3fL7hCiOSkcJnqvqOFtrxsw1VwpqzX0STHfE84x946deKfp6Qf3gx0fm5O0oYkQOxYObYYvfgt/HQwoU4IBTM98wBRzEviDBTDlPpPAky40PfZDP0L/ySaxg1meOANW/QmGXQYBfbp4ZwjROdJzF2eOQ1sgY6m5M5VXAESPgtgx5hNC8R54carp7fcZbg4gP7wIn//mxLo6xXvMid/xt5oTvUJ0I6nnLsSp2vExfHqfmcN/9G5VZTmt39Bk6Y2we7m5baGXf/fGKc5oDq0tI8QZIekCuHvr8bchbOtOVeNvg7oy+PHt7olNiFMkY+5CNNfRKZRxqRAzBta9AKk3QuEOU3GzutjM7CnaZebkpy04NgsHoLrEzMzx8AW/CDOjR4guIH9ZQnSGUjDhNnMS9okE04tvWmeB4HjIXAbfPm0OBN5BULIP8jOOtfOLgLE3mBO+RTtNuYWCTGioNieCI5Oh30RzEPEOMs+pzDfTOtu7963WZgZQwQ44ss889vAxs38Co6GuEnZ+aq78PXr9welqtJori72DOnaQtNabmVERg805kNORv82cSJchsiYy5i5EZ1nrYekNJjElTIGwgaaXHhQH7l4mUX//nEnaNaUQEGVm9gTFm3vZ7v7Sfpcr+/+gf5RJtp7+prpm8W5zhW4Tdaxt/8kw4gpTerksB7LXmtcL6W+uFs7bBBWHTozZzcMk+OzvzCcIMMNQZ/0CUi45dg1CwQ5Y/Wdz0Vb0CFMmwsPHfLl7m9cIG2TKSGQtNweK3V+aqacWLwgfDMMuheS5ENLvxGsCakpNGYoDa8zB8OhrePpDRR5UFprnRaZAn6HmK3wwuLW4PV59lZkFteFV80npmvfNfrbZzAHm6EHGZjPbLdlrfl9RI07c1nHbrYaaEgiIMfukscF89YDyE3JCVYje4MgBqDhseq8t5/XXVZoLtwp2QF0F6Ebw72MS6IbXTM8cTHKMGWWSW2k2lOea6aR9J5plYYnmIq/KAkh/7Vg5iPG3mgPChlfsw0jDzXaqS2DXpybR+vcxw0y0kifcfcyUUpsVfMNg0Hnm00ZVIRxcZ77AzE4KHWCuNo4da64x2PCq2e7Mh02iz91gEnpduUnOfhHmk07zA5xnAMSONgcwn1Bz1XL29+bTzMj5sH2ZKTIXM8YccLQ2772x3mzLWnMsdp8QCEkAa51Zbq0zB62wgeYaigNrzXcPX9O24pA5MKbeCGffY6bANjaYcy67v4SBMyH5InO/g4LtsO1DE0N9lXm9QbNM+Q1rrVmfdIHZF50gyV0IV2ZrNFMyvQLMpwV3r9PbVsb7sOZvx84HDJplrhb2CzMJqrrY9OIbqs1XabaZWuruaS4Oixt3Yk+4ZC/s/9be9kdT//9ogvUJNTeBSTjn5LFZ60wJi/wMyNkAuenm4FVVZD4hRY+EtFsgYbJZ//Z887zB55n3UZxlknboAPMVlmieu3eluXfB0U8i7t7m01TxXjPsNWAqhA0w+7jmiPm0VZ4LP75jDrJBfc338lzzXmpKjo/bO9jckN4/0uy/bR9CdZFZpyxwwV8h9YZO/bokuQshepaGWig9YHr5PiEnHxZpj9atj+vbbIA+vW2fTPEeyPjAnECvq4BxN5sDYe5G2LcK/CIhNAHi0o4vimetN0NnfuHm08FpHIwluQshhAuSee5CCHEGk+QuhBAuSJK7EEK4IEnuQgjhgiS5CyGEC5LkLoQQLkiSuxBCuCBJ7kII4YIkuQshhAuS5C6EEC5IkrsQQrggSe5CCOGCJLkLIYQLkuQuhBAuSJK7EEK4IEnuQgjhgiS5CyGEC5LkLoQQLkiSuxBCuCBJ7kII4YIkuQshhAuS5C6EEC5IkrsQQrigDiV3pdRspdROpVSWUmphK+t/qZTarpTaopRaoZTq5/hQhRBCdFS7yV0pZQEWAXOAFOAqpVRKi2abgFSt9QhgKfBnRwcqhBCi4zrSc08DsrTWe7XW9cA7wMXNG2itV2qtq+0PvwfiHBumEEKIU9GR5B4LHGz2OMe+rC03AZ+2tkIpdYtSaoNSakNhYWHHoxRCCHFKHHpCVSl1DZAK/KW19VrrF7XWqVrr1IiICEe+tBBCiGbcO9AmF4hv9jjOvuw4SqmZwAPAFK11nWPCE0II0Rkd6bmvBwYppRKUUp7AfGBZ8wZKqdHAC8BcrXWB48MUQghxKtpN7lprK3An8DmQCSzRWm9TSj2qlJprb/YXwB94Tym1WSm1rI3NCSGE6AYdGZZBa/0J8EmLZQ81+3mmg+MSQghxGuQKVSGEcEGS3IUQwgVJchdCCBckyV0IIVyQJHchhHBBktyFEMIFSXIXQggXJMldCCFckCR3IYRwQZLchRDCBUlyF0IIFyTJXQghXJAkdyGEcEGS3IUQwgVJchdCCBckyV0IIVyQJHchhHBBktyFEMIFSXIXQggXJMldCCFckCR3IYRwQZLchRDCBUlyF0IIFyTJXQghXJAkdyGEcEGS3IUQwgVJchdCCBckyV0IIVyQJHchhHBBktyFEMIFSXIXQggXJMldCCFckCR3IYRwQZLchRDCBUlyF0IIF9Sh5K6Umq2U2qmUylJKLWxlvZdS6l37+nVKqf6ODlQIIUTHtZvclVIWYBEwB0gBrlJKpbRodhNwRGs9EPgb8ISjAxVCCNFxHem5pwFZWuu9Wut64B3g4hZtLgbesP+8FJihlFKOC1MIIcSpcO9Am1jgYLPHOcD4ttpora1KqTIgDChq3kgpdQtwi/1hpVJqZ2eCBsJbbrsHkhgdQ2J0jJ4eY0+PD3pOjP060qgjyd1htNYvAi+e7naUUhu01qkOCKnLSIyOITE6Rk+PsafHB70jxuY6MiyTC8Q3exxnX9ZqG6WUOxAEFDsiQCGEEKeuI8l9PTBIKZWglPIE5gPLWrRZBlxn//ly4CuttXZcmEIIIU5Fu8My9jH0O4HPAQvwqtZ6m1LqUWCD1noZ8ArwplIqCyjBHAC60mkP7XQDidExJEbH6Okx9vT4oHfE2ERJB1sIIVyPXKEqhBAuSJK7EEK4oF6X3NsrheAMSql4pdRKpdR2pdQ2pdT/2ZeHKqW+VErttn8PcXKcFqXUJqXUR/bHCfZyEVn28hGeTo4vWCm1VCm1QymVqZSa2AP34T3233GGUuptpZS3s/ejUupVpVSBUiqj2bJW95sy/m6PdYtSaowTY/yL/Xe9RSn1H6VUcLN199tj3KmUOs9ZMTZb9yullFZKhdsfO2U/nopeldw7WArBGazAr7TWKcAE4A57XAuBFVrrQcAK+2Nn+j8gs9njJ4C/2ctGHMGUkXCmZ4DPtNZJwEhMrD1mHyqlYoG7gFSt9TDMBIP5OH8/vg7MbrGsrf02Bxhk/7oFeN6JMX4JDNNajwB2AfcD2P935gND7c95zv6/74wYUUrFA+cC2c0WO2s/dpzWutd8AROBz5s9vh+439lxtRLnf4FZwE4g2r4sGtjpxJjiMP/k04GPAIW52s69tX3rhPiCgH3YT/I3W96T9uHRK7FDMTPNPgLO6wn7EegPZLS334AXgKtaa9fdMbZYdwmw2P7zcf/XmJl6E50VI6akykhgPxDu7P3Y0a9e1XOn9VIIsU6KpVX2ipijgXVAH631Ifuqw0AfJ4UF8DTwa8BmfxwGlGqtrfbHzt6XCUAh8Jp96OhlpZQfPWgfaq1zgScxPbhDQBmQTs/aj0e1td966v/QjcCn9p97TIxKqYuBXK31jy1W9ZgY29LbknuPppTyB94H7tZalzdfp83h3SnzTpVSFwIFWut0Z7x+B7kDY4DntdajgSpaDME4cx8C2MetL8YciGIAP1r5GN/TOHu/tUcp9QBmaHOxs2NpTinlC/wGeMjZsXRGb0vuHSmF4BRKKQ9MYl+stf7AvjhfKRVtXx8NFDgpvEnAXKXUfkxVz+mY8e1ge7kIcP6+zAFytNbr7I+XYpJ9T9mHADOBfVrrQq11A/ABZt/2pP14VFv7rUf9DymlrgcuBK62H4Sg58SYiDmQ/2j/34kDNiqloug5MbaptyX3jpRC6HZKKYW5SjdTa/1Us1XNyzJchxmL73Za6/u11nFa6/6YffaV1vpqYCWmXIRT4wPQWh8GDiqlhtgXzQC200P2oV02MEEp5Wv/nR+Nscfsx2ba2m/LgGvtsz0mAGXNhm+6lVJqNmaocK7WurrZqmXAfGVuApSAOWn5Q3fHp7XeqrWO1Fr3t//v5ABj7H+rPWY/tsnZg/6dOOFxPubM+h7gAWfHY4/pbMzH3i3AZvvX+Zhx7RXAbmA5ENoDYp0KfGT/eQDmnyYLeA/wcnJso4AN9v34IRDS0/Yh8AiwA8gA3gS8nL0fgbcx5wAaMAnoprb2G+ZE+iL7/89WzMwfZ8WYhRm3Pvo/889m7R+wx7gTmOOsGFus38+xE6pO2Y+n8iXlB4QQwgX1tmEZIYQQHSDJXQghXJAkdyGEcEGS3IUQwgVJchdCCBckyV0IIVyQJHchhHBB/x/0MIJs81zg9AAAAABJRU5ErkJggg==\n",
   "text/plain": "<matplotlib.figure.Figure at 0x1283a9ed0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "None\n"
 }
]
```

```{.python .input  n=133}
###############################################################
# your code here to save parameters and visualize the ﬁlters  #
file_name = "net-params_std0_01_lr10"
net.save_parameters(file_name)
```

```{.python .input  n=173}
print net[0].weight.shape
import matplotlib.pyplot as plt
# %matplotlib inline
for i in range(net[0].weight.data().shape[0]):
    for j in range(net[0].weight.data().shape[1]):
        plt.matshow(net[0].weight.data()[i][j].asnumpy())

```

```{.json .output n=173}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "(6L, 3L, 5L, 5L)\n"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x128246a90>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12ab6ac90>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12643efd0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126999310>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x1269073d0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12644ad10>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x1282c3090>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126709ad0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126e00550>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAACfdJREFUeJzt3c+LXfUdxvHncTLTSZOKRlOaZlJjQYTUUoUhCOkqIMQf6FZBV0I2FSIIVqEb/wFx4yaoKCiKVBeSWiRgxEr9NWoUk2hJJSUR27HVqKNJJrl+upi7SNXOPUfO937n+Hm/YGDu5HB8Msw7586d6x1HhADkck7tAQDGj/CBhAgfSIjwgYQIH0iI8IGEehG+7R2237d92PZdtfeMYvsh2/O23629pSnbm2zvs33Q9gHbu2pvWo7taduv2X57uPee2puasj1h+y3be2ptWPHh256QdL+kqyVtkXST7S11V430sKQdtUe0dEbSHRGxRdKVkn63wj/PpyRtj4jfSLpc0g7bV1be1NQuSYdqDljx4UvaKulwRHwQEYuSnpB0Q+VNy4qIFyV9UntHGxHxUUS8OXz/Cy19YW6su+r/iyULw5uTw7cV/2w02zOSrpX0QM0dfQh/o6SjZ90+phX8BflDYHuzpCskvVp3yfKGd5n3S5qXtDciVvTeofsk3Snp65oj+hA+xsj2WklPSbo9Ij6vvWc5ETGIiMslzUjaavuy2puWY/s6SfMR8UbtLX0I/0NJm866PTP8GDpme1JL0T8WEU/X3tNURByXtE8r/3GVbZKut31ES9+ybrf9aI0hfQj/dUmX2L7Y9pSkGyU9U3nTD45tS3pQ0qGIuLf2nlFsr7d93vD91ZKukvRe3VXLi4i7I2ImIjZr6ev4+Yi4ucaWFR9+RJyRdJuk57T0gNOTEXGg7qrl2X5c0suSLrV9zPattTc1sE3SLVq6Cu0fvl1Te9QyNkjaZ/sdLV0c9kZEtR+P9Y3533KBfFb8FR9A9wgfSIjwgYQIH0iI8IGEehW+7Z21N7TVt8192yv1b/NK2Nur8CVV/4R9D33b3Le9Uv82V9/bt/ABdKDIE3gm1q6JVevWdX7ewcKCJtau7fy8kjT1WZknMp1e/FKTU2s6P+85Cyc7P6ckLcZJTXm6yLlLPVnsdJzUZKHNnprq/JyLg680NfHjzs8rSSdOf6bFwVceddyqEv/xVevWacPvV/QLuHzLRXsGtSe0Mv2Xg7UntBaDfn2OJemci2ZqT2jl5SOPNDqOu/pAQoQPJET4QEKEDyRE+EBChA8kRPhAQoQPJET4QEKEDyRE+EBChA8kRPhAQoQPJET4QEKEDyRE+EBCjcK3vcP2+7YP276r9CgAZY0M3/aEpPslXS1pi6SbbG8pPQxAOU2u+FslHY6IDyJiUdITkm4oOwtASU3C3yjp6Fm3jw0/9j9s77Q9Z3tusLDQ1T4ABXT24F5E7I6I2YiYLfUS2AC60ST8DyVtOuv2zPBjAHqqSfivS7rE9sW2pyTdKOmZsrMAlDTyF2pExBnbt0l6TtKEpIci4kDxZQCKafSbdCLiWUnPFt4CYEx45h6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwk1eiGOttauOaHfzh4qcepi3j34q9oTWll95Oe1J7Tmj+ZrT2ht8Le/157QSsSpRsdxxQcSInwgIcIHEiJ8ICHCBxIifCAhwgcSInwgIcIHEiJ8ICHCBxIifCAhwgcSInwgIcIHEiJ8ICHCBxIifCChkeHbfsj2vO13xzEIQHlNrvgPS9pReAeAMRoZfkS8KOmTMWwBMCZ8jw8k1Fn4tnfanrM9d/L4ya5OC6CAzsKPiN0RMRsRs9PnTXd1WgAFcFcfSKjJj/Mel/SypEttH7N9a/lZAEoa+Su0IuKmcQwBMD7c1QcSInwgIcIHEiJ8ICHCBxIifCAhwgcSInwgIcIHEiJ8ICHCBxIifCAhwgcSInwgIcIHEiJ8ICHCBxIa+Qo838eEQ+euOlXi1MUs/KL2gnb+uX197QmtDaZ/WntCaxMnovaEVs788ZVGx3HFBxIifCAhwgcSInwgIcIHEiJ8ICHCBxIifCAhwgcSInwgIcIHEiJ8ICHCBxIifCAhwgcSInwgIcIHEiJ8IKGR4dveZHuf7YO2D9jeNY5hAMpp8pp7ZyTdERFv2v6JpDds742Ig4W3AShk5BU/Ij6KiDeH738h6ZCkjaWHASin1ff4tjdLukLSqyXGABiPxuHbXivpKUm3R8Tn3/HnO23P2Z478enJLjcC6Fij8G1Pain6xyLi6e86JiJ2R8RsRMyuPn+6y40AOtbkUX1LelDSoYi4t/wkAKU1ueJvk3SLpO229w/frim8C0BBI3+cFxEvSfIYtgAYE565ByRE+EBChA8kRPhAQoQPJET4QEKEDyRE+EBChA8kRPhAQoQPJET4QEKEDyRE+EBChA8kRPhAQk1eV7+1SQ/0sx99VuLUxZy+4EztCa0M/jVZe0JrHtRe0N7xX39de0Irgz81O44rPpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwmNDN/2tO3XbL9t+4Dte8YxDEA5TV5z75Sk7RGxYHtS0ku2/xwRrxTeBqCQkeFHREhaGN6cHL5FyVEAymr0Pb7tCdv7Jc1L2hsRr5adBaCkRuFHxCAiLpc0I2mr7cu+eYztnbbnbM99+eli1zsBdKjVo/oRcVzSPkk7vuPPdkfEbETMrjl/qqt9AApo8qj+etvnDd9fLekqSe+VHgagnCaP6m+Q9IjtCS39Q/FkROwpOwtASU0e1X9H0hVj2AJgTHjmHpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kFCTl95qbcOqk/rDhf16Wb6/XvLL2hNaOXJ0c+0JrZ37wde1J7S26q1+XRvnv2p2XL/+VgA6QfhAQoQPJET4QEKEDyRE+EBChA8kRPhAQoQPJET4QEKEDyRE+EBChA8kRPhAQoQPJET4QEKEDyRE+EBCjcO3PWH7Ldt7Sg4CUF6bK/4uSYdKDQEwPo3Ctz0j6VpJD5SdA2Acml7x75N0p6T+vUwqgG8ZGb7t6yTNR8QbI47baXvO9tzH/xl0NhBA95pc8bdJut72EUlPSNpu+9FvHhQRuyNiNiJm118w0fFMAF0aGX5E3B0RMxGxWdKNkp6PiJuLLwNQDD/HBxJq9Su0IuIFSS8UWQJgbLjiAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCRE+kBDhAwkRPpAQ4QMJET6QEOEDCTkiuj+p/bGkf3R+YulCSf8ucN6S+ra5b3ul/m0uufeiiFg/6qAi4Zdiey4iZmvvaKNvm/u2V+rf5pWwl7v6QEKEDyTUt/B31x7wPfRtc9/2Sv3bXH1vr77HB9CNvl3xAXSA8IGECB9IiPCBhAgfSOi/qOki6KVDq5AAAAAASUVORK5CYII=\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12a829f90>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126f05a10>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126fdbf90>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12700ded0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126f92950>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12711e3d0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x1267e4890>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12a894d90>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x126b04050>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 }
]
```

```{.python .input  n=11}
net2 = nn.Sequential()
with net2.name_scope():
    net2.add(nn.Conv2D(channels=6, kernel_size=5,activation='relu')) #,weight_initializer=mx.init.Normal(0.01)  in_channels=3,
    net2.add(nn.MaxPool2D(pool_size=2, strides=2))
    net2.add(nn.Conv2D(channels=16,kernel_size=5, activation='relu')) #,weight_initializer=mx.init.Normal(0.01) in_channels=6,
    net2.add(nn.MaxPool2D(pool_size=2, strides=2))
    # The Flatten layer collapses all axis, except the first one, into one axis.
    net2.add(nn.Flatten())
    net2.add(nn.Dense(120, activation="relu")) #,weight_initializer=mx.init.Normal(0.01)
    net2.add(nn.Dense(84,activation="relu")) #weight_initializer=mx.init.Normal(0.01), 
    net2.add(nn.Dense(num_output)) #,weight_initializer=mx.init.Normal(0.01)

print net2
```

```{.json .output n=11}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (2): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (4): Flatten\n  (5): Dense(None -> 120, Activation(relu))\n  (6): Dense(None -> 84, Activation(relu))\n  (7): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=13}
file_name = "net-params_std0_01_lr10"
net2.load_parameters(file_name, ctx=mx.cpu(1))
```

```{.python .input  n=14}
print net2[0].weight.data()[0][0]
```

```{.json .output n=14}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[-0.08655212 -0.09304737  0.09627281  0.4197063   0.40260616]\n [-0.02112781  0.0905636  -0.03056227  0.03058454  0.24193658]\n [ 0.02636526  0.43079594  0.09913642 -0.31822014 -0.01492882]\n [-0.15010849  0.2983658   0.19384784 -0.15322928  0.02273046]\n [-0.39082694 -0.17250638  0.06688458  0.1019552   0.13519903]]\n<NDArray 5x5 @cpu(1)>\n"
 }
]
```

```{.python .input  n=15}
net3 = nn.Sequential()
with net3.name_scope():
    net3.add(nn.Conv2D(channels=6, kernel_size=5,activation='relu')) #,weight_initializer=mx.init.Normal(0.01)  in_channels=3,
    net3.add(nn.MaxPool2D(pool_size=2, strides=2))
    net3.add(nn.Conv2D(channels=16,kernel_size=5, activation='relu')) #,weight_initializer=mx.init.Normal(0.01) in_channels=6,
    net3.add(nn.MaxPool2D(pool_size=2, strides=2))
    # The Flatten layer collapses all axis, except the first one, into one axis.
    net3.add(nn.Flatten())
    net3.add(nn.Dense(120, activation="relu")) #,weight_initializer=mx.init.Normal(0.01)
    net3.add(nn.Dense(84,activation="relu")) #weight_initializer=mx.init.Normal(0.01), 
    net3.add(nn.Dense(num_output)) #,weight_initializer=mx.init.Normal(0.01)

print net3
net3.initialize(MyInit2(),ctx=ctx)
```

```{.json .output n=15}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (2): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (4): Flatten\n  (5): Dense(None -> 120, Activation(relu))\n  (6): Dense(None -> 84, Activation(relu))\n  (7): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=16}
trainer2 = gluon.Trainer(net3.collect_params(), optimizer, optimizer_params)
loss_fn2 = gluon.loss.SoftmaxCrossEntropyLoss()
train_metric2 = mx.metric.Accuracy()
train_history2 = TrainingHistory(['training-error', 'validation-error'])
```

```{.python .input  n=42}
# print net3[0].weight.data()[0][0]
epochs=30
for epoch in range(epochs):
    tic = time.time()
    train_metric2.reset()
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        trainer2.set_learning_rate(trainer2.learning_rate*lr_decay)
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net3(X) for X in data]
            loss = [loss_fn2(yhat, y) for yhat, y in zip(output, label)]

        # Backpropagation
        for l in loss:
            l.backward()

        # Optimize
        trainer2.step(batch_size)

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric2.update(label, output)

    name, acc = train_metric2.get()
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net3)

    # Update history and print metrics
    train_history2.update([1-acc, 1-val_acc])
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net3[0].weight.data()[0][0]
```

```{.json .output n=42}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "[Epoch 0] train=0.557111 val=0.665000 loss=62485.402863 time: 33.105658\n\n[[-0.28012198 -0.28049514  0.03176591  0.27127913 -0.1576477 ]\n [ 0.05755473 -0.03012206  0.08639891  0.32393545  0.13857824]\n [ 0.34771413  0.20154168  0.02182794  0.15500084  0.25743258]\n [ 0.2058441   0.19256409 -0.05597633  0.23662518  0.26805988]\n [-0.12883423 -0.17175834  0.02110011  0.25660884  0.07500188]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 1] train=0.558694 val=0.664500 loss=62311.639740 time: 30.425709\n\n[[-0.28115368 -0.28359514  0.02867969  0.26569092 -0.16246831]\n [ 0.06614937 -0.02281154  0.08717692  0.3179772   0.1329466 ]\n [ 0.35619935  0.20966597  0.0212827   0.14966984  0.25657287]\n [ 0.2035635   0.1930513  -0.05783888  0.23662306  0.27139094]\n [-0.13302974 -0.17251658  0.01916518  0.2634915   0.08228994]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 2] train=0.558654 val=0.657800 loss=62292.810608 time: 29.774356\n\n[[-0.27740666 -0.27729142  0.03595233  0.2751665  -0.15486708]\n [ 0.0669934  -0.01570714  0.09683951  0.3256422   0.13851887]\n [ 0.35672587  0.2224691   0.03701313  0.1618787   0.27075917]\n [ 0.20627776  0.20483623 -0.04144888  0.24759044  0.2829257 ]\n [-0.1264657  -0.1621131   0.03308206  0.27195418  0.08795016]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 3] train=0.554507 val=0.657800 loss=62468.932617 time: 30.775024\n\n[[-0.26928937 -0.2687936   0.03867165  0.26245072 -0.16556159]\n [ 0.06523704 -0.0155359   0.09714998  0.31388682  0.13104169]\n [ 0.3494679   0.21916349  0.03435507  0.15019758  0.26251456]\n [ 0.19821042  0.20338292 -0.03995476  0.24010488  0.2781096 ]\n [-0.1292095  -0.16326205  0.03168167  0.26377445  0.07364501]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 4] train=0.560156 val=0.670200 loss=61804.140472 time: 30.032503\n\n[[-0.26997864 -0.27225107  0.03527432  0.27298138 -0.14690568]\n [ 0.06424735 -0.0177046   0.09695317  0.32112524  0.13524024]\n [ 0.35785416  0.22508259  0.03240693  0.14433463  0.25500125]\n [ 0.20291197  0.20599824 -0.04977802  0.2200879   0.26299712]\n [-0.1394199  -0.17237581  0.01697195  0.24814254  0.06161664]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 5] train=0.556591 val=0.676100 loss=62496.619415 time: 29.849272\n\n[[-0.27101222 -0.28317848  0.01903116  0.26636243 -0.15409276]\n [ 0.06190878 -0.02780803  0.08331057  0.31511214  0.12579109]\n [ 0.3608591   0.21861693  0.02241944  0.14029343  0.24749616]\n [ 0.2162031   0.20916882 -0.04612752  0.22811715  0.26506647]\n [-0.12376144 -0.1616731   0.02950426  0.26892188  0.07406423]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 6] train=0.554307 val=0.669600 loss=62703.616852 time: 27.568468\n\n[[-0.27949047 -0.2846172   0.02347009  0.26217148 -0.16488726]\n [ 0.05872894 -0.01950638  0.09670337  0.32120162  0.12578902]\n [ 0.35901585  0.22771095  0.03252863  0.14600582  0.25169924]\n [ 0.21461053  0.20968021 -0.04473719  0.22273777  0.26256657]\n [-0.13078639 -0.17026024  0.02430775  0.26019916  0.07045674]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 7] train=0.555909 val=0.662800 loss=62369.636047 time: 23.491582\n\n[[-0.2658269  -0.2602861   0.05202235  0.2788413  -0.16222437]\n [ 0.05760399 -0.01084523  0.11563616  0.3289939   0.12026785]\n [ 0.35447997  0.2320822   0.0479234   0.15138656  0.24722135]\n [ 0.20935965  0.21012239 -0.0363997   0.22558829  0.25916407]\n [-0.13951893 -0.17521212  0.02401487  0.2605176   0.06514986]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 8] train=0.557953 val=0.667600 loss=62090.227570 time: 30.920452\n\n[[-0.27454016 -0.27053612  0.0352411   0.27164295 -0.15289952]\n [ 0.0513993  -0.02561846  0.09798291  0.32263318  0.13042381]\n [ 0.35334587  0.22035085  0.03254204  0.1501458   0.2597864 ]\n [ 0.21504445  0.21346322 -0.04148395  0.22009888  0.26041505]\n [-0.13818008 -0.1675268   0.02573263  0.26046738  0.06475057]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 9] train=0.554367 val=0.655200 loss=62513.777435 time: 28.233413\n\n[[-0.28849655 -0.29143664  0.02372752  0.2616288  -0.16176209]\n [ 0.04035306 -0.043368    0.09212819  0.3231377   0.13618065]\n [ 0.3406562   0.20425206  0.02718336  0.15184112  0.26814052]\n [ 0.20147358  0.20149766 -0.04190374  0.22076026  0.26354617]\n [-0.1415805  -0.17117403  0.03309737  0.2668806   0.06447937]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 10] train=0.556510 val=0.670400 loss=62561.630829 time: 26.596426\n\n[[-0.27358383 -0.2797203   0.03082885  0.26593548 -0.15457052]\n [ 0.05303717 -0.03135575  0.10045968  0.3231226   0.13403533]\n [ 0.35192126  0.21305928  0.03234671  0.14776133  0.26396972]\n [ 0.21253653  0.20641929 -0.0355699   0.22354662  0.27114338]\n [-0.12668547 -0.16485417  0.03779796  0.27281636  0.07523148]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 11] train=0.555509 val=0.666200 loss=62321.857391 time: 29.750058\n\n[[-0.28969443 -0.29300165  0.03594923  0.28074834 -0.13511626]\n [ 0.0419839  -0.04168946  0.1071995   0.33222562  0.14560473]\n [ 0.34953672  0.20928381  0.03380632  0.14068913  0.2598686 ]\n [ 0.21249725  0.20207289 -0.03848758  0.21340843  0.2664054 ]\n [-0.12908794 -0.17646913  0.0249475   0.25592697  0.06278232]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 12] train=0.555909 val=0.670600 loss=62699.729034 time: 31.463000\n\n[[-0.28300217 -0.28540102  0.03427858  0.27378708 -0.13949841]\n [ 0.0425656  -0.04224854  0.09849127  0.31769437  0.1353434 ]\n [ 0.35200167  0.21041323  0.03050642  0.13429607  0.25780174]\n [ 0.2195125   0.20607471 -0.03746914  0.20724122  0.2603825 ]\n [-0.12826853 -0.18161777  0.01582433  0.2465151   0.05815215]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 13] train=0.554908 val=0.664500 loss=62309.886871 time: 26.497772\n\n[[-0.28581834 -0.28365755  0.03732105  0.277953   -0.14210778]\n [ 0.03745092 -0.03959646  0.10404336  0.32340837  0.13506173]\n [ 0.34533396  0.21318169  0.0384006   0.1409502   0.25804526]\n [ 0.22049627  0.21666606 -0.02400625  0.21565849  0.26443723]\n [-0.11965417 -0.16068172  0.0347315   0.25742757  0.062068  ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 14] train=0.553305 val=0.664100 loss=62769.634521 time: 24.151046\n\n[[-0.27778524 -0.26957253  0.04909719  0.28002933 -0.14885002]\n [ 0.03293878 -0.03113325  0.11583778  0.32584968  0.12797779]\n [ 0.3392814   0.22067007  0.05230414  0.14754252  0.25206545]\n [ 0.22097078  0.22896236 -0.00552687  0.22531414  0.26162136]\n [-0.12538584 -0.15865734  0.04217903  0.26105577  0.05772696]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 15] train=0.559515 val=0.673900 loss=62123.343658 time: 22.955817\n\n[[-0.26598698 -0.2581408   0.05548961  0.28267434 -0.15042062]\n [ 0.04519612 -0.0258026   0.1206985   0.32957056  0.12453239]\n [ 0.3476226   0.21880391  0.0491234   0.14438248  0.24107413]\n [ 0.22827123  0.22832938 -0.00556933  0.22687645  0.25870636]\n [-0.12359203 -0.15828441  0.04774938  0.2718636   0.06037777]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 16] train=0.556490 val=0.671600 loss=62683.583740 time: 22.841767\n\n[[-0.2742124  -0.27786502  0.03729871  0.2649628  -0.16605142]\n [ 0.03835058 -0.04275504  0.10860305  0.32082468  0.12028866]\n [ 0.34875557  0.21124363  0.03645133  0.13663393  0.24024083]\n [ 0.2276328   0.22219725 -0.01899773  0.21587163  0.25612825]\n [-0.14317936 -0.17772843  0.03064279  0.25750753  0.05203315]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 17] train=0.558534 val=0.670900 loss=61963.134735 time: 23.009586\n\n[[-0.28205195 -0.28248838  0.03026302  0.26131034 -0.16331775]\n [ 0.03834607 -0.03713581  0.11352015  0.32558084  0.12613529]\n [ 0.3528155   0.21957684  0.04588383  0.1440026   0.25076985]\n [ 0.23703451  0.23248373 -0.008886    0.21883161  0.26158923]\n [-0.11759096 -0.15599653  0.04449734  0.26778722  0.06385111]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 18] train=0.557632 val=0.672200 loss=62337.145477 time: 23.022816\n\n[[-0.28627893 -0.29982173  0.01887834  0.26852456 -0.15105075]\n [ 0.03111067 -0.05622562  0.09967927  0.32951054  0.13364795]\n [ 0.3488673   0.20493516  0.02948962  0.13904603  0.25163907]\n [ 0.2380752   0.22702144 -0.01594587  0.21778458  0.2667784 ]\n [-0.11613017 -0.15405402  0.04559068  0.27173966  0.06899724]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 19] train=0.558053 val=0.670400 loss=62236.480286 time: 22.873063\n\n[[-0.28272572 -0.2892811   0.02455754  0.2646026  -0.16067137]\n [ 0.03437545 -0.04931183  0.10819778  0.32840186  0.12490321]\n [ 0.34905875  0.20491162  0.03179474  0.13304289  0.23753235]\n [ 0.23512575  0.22614984 -0.01603962  0.21013474  0.25646245]\n [-0.13794664 -0.16957705  0.03266971  0.25677407  0.06213854]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 20] train=0.557492 val=0.665400 loss=62286.892120 time: 23.038421\n\n[[-0.2891191  -0.28754407  0.02640321  0.25722137 -0.16653581]\n [ 0.03488749 -0.04134787  0.1124783   0.3217102   0.12063687]\n [ 0.35219115  0.21777253  0.04165397  0.13185579  0.23556559]\n [ 0.24514265  0.25039616  0.00437943  0.21664922  0.25634286]\n [-0.13234901 -0.15124115  0.04915062  0.2657021   0.0609645 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 21] train=0.557993 val=0.666600 loss=62434.093048 time: 28.986100\n\n[[-0.28873253 -0.28262916  0.04026832  0.27093485 -0.15361813]\n [ 0.03609584 -0.03744965  0.12191962  0.32934463  0.12681386]\n [ 0.35196614  0.21623708  0.0394544   0.13087839  0.23332644]\n [ 0.23999472  0.23924437 -0.00765577  0.20774937  0.2489094 ]\n [-0.14125557 -0.16988653  0.02705481  0.2534271   0.04938733]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 22] train=0.556911 val=0.663600 loss=62191.664062 time: 27.411190\n\n[[-0.27838743 -0.28092214  0.02993084  0.26545542 -0.14822207]\n [ 0.04305542 -0.03818475  0.11337322  0.327836    0.13357377]\n [ 0.3647925   0.22330901  0.03631314  0.13067232  0.24540007]\n [ 0.2442486   0.24285278 -0.01019676  0.20369805  0.25475496]\n [-0.13600174 -0.16180421  0.03114405  0.2599005   0.06237568]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 23] train=0.556170 val=0.662300 loss=62422.536499 time: 24.796829\n\n[[-0.2904683  -0.2839256   0.03209689  0.25830722 -0.16977689]\n [ 0.02887814 -0.04294988  0.11963838  0.32902747  0.12238767]\n [ 0.3496116   0.21926287  0.03934995  0.12833647  0.23460102]\n [ 0.2290431   0.24366231 -0.00661551  0.19697355  0.24164695]\n [-0.15317146 -0.16155532  0.03301593  0.26067436  0.05704248]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 24] train=0.559275 val=0.672000 loss=62268.204376 time: 24.918147\n\n[[-0.28060627 -0.26612258  0.04072005  0.2510207  -0.17306809]\n [ 0.03885572 -0.02763877  0.13162972  0.32872486  0.12079697]\n [ 0.34499666  0.22286706  0.04300898  0.12653679  0.23073807]\n [ 0.21085005  0.24165806 -0.00292073  0.19571972  0.23424178]\n [-0.16842492 -0.16183124  0.03265744  0.2548844   0.04756089]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 25] train=0.556671 val=0.673700 loss=62156.657898 time: 24.880295\n\n[[-0.29721653 -0.2780059   0.04236655  0.25537893 -0.17529722]\n [ 0.02678096 -0.03786765  0.13270566  0.33189085  0.11995383]\n [ 0.33773065  0.21559364  0.04701165  0.13011982  0.23177487]\n [ 0.20521973  0.23523615  0.00076199  0.19677578  0.23620765]\n [-0.17524967 -0.16863693  0.03338162  0.25747716  0.04966914]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 26] train=0.554087 val=0.668900 loss=62681.016388 time: 24.202300\n\n[[-0.2904371  -0.27719593  0.03476217  0.24498981 -0.18325992]\n [ 0.03168149 -0.0377581   0.12815428  0.32883802  0.11649186]\n [ 0.34416172  0.22208327  0.05003913  0.13378634  0.23656327]\n [ 0.21686484  0.24904537  0.0105599   0.20243801  0.24273789]\n [-0.15727966 -0.14854677  0.04726344  0.26808485  0.06183146]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 27] train=0.560958 val=0.653800 loss=62068.912781 time: 25.785144\n\n[[-0.28372744 -0.26713756  0.04514474  0.25385377 -0.17947008]\n [ 0.03027725 -0.03161024  0.13448773  0.33338565  0.11779306]\n [ 0.3398648   0.22046576  0.04724123  0.1360649   0.239919  ]\n [ 0.21439232  0.248629    0.00985016  0.20708682  0.24740095]\n [-0.14901915 -0.13622116  0.0553591   0.2807348   0.06649189]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 28] train=0.557352 val=0.660700 loss=62250.372101 time: 25.941135\n\n[[-0.28203067 -0.2725419   0.03537716  0.24471526 -0.18591665]\n [ 0.02367717 -0.04396972  0.12063107  0.3198789   0.10828631]\n [ 0.33773658  0.21554078  0.04107483  0.13067378  0.2401039 ]\n [ 0.21170145  0.24459411  0.005699    0.20112401  0.24775039]\n [-0.15468822 -0.14067088  0.05475953  0.2791353   0.06504548]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 29] train=0.558213 val=0.665900 loss=62089.063049 time: 26.482250\n\n[[-0.2892432  -0.27655122  0.04814598  0.26625052 -0.17052294]\n [ 0.02967626 -0.04312751  0.1285892   0.33365837  0.11356345]\n [ 0.3481558   0.22051953  0.04539069  0.13170332  0.23741324]\n [ 0.21692169  0.24885571  0.00970042  0.19537261  0.24488153]\n [-0.15312676 -0.13947918  0.05812686  0.2790097   0.06574617]]\n<NDArray 5x5 @cpu(0)>\n"
 }
]
```

```{.python .input  n=45}
print net3[0].weight.data()[0][0] #70
print train_history2.plot()
file_name = "net-params_std0_01_lr10_newinitializaiton"
net3.save_parameters(file_name)
```

```{.json .output n=45}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[-0.2892432  -0.27655122  0.04814598  0.26625052 -0.17052294]\n [ 0.02967626 -0.04312751  0.1285892   0.33365837  0.11356345]\n [ 0.3481558   0.22051953  0.04539069  0.13170332  0.23741324]\n [ 0.21692169  0.24885571  0.00970042  0.19537261  0.24488153]\n [-0.15312676 -0.13947918  0.05812686  0.2790097   0.06574617]]\n<NDArray 5x5 @cpu(0)>\n"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x1238f8cd0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 },
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "None\n"
 }
]
```

## Problem2

### b）
From the acc, we can know that after using a better
initialization method, our model converge faster and yield the some acc as the
original one.

```{.python .input  n=44}
ctx_list=[mx.cpu(1)]
ctx=mx.cpu(1)
net4 = nn.Sequential()
with net4.name_scope():
    net4.add(nn.Conv2D(channels=6, kernel_size=5)) 
    net4.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net4.add(gluon.nn.Activation(activation='relu'))
    net4.add(nn.MaxPool2D(pool_size=2, strides=2))
    net4.add(nn.Conv2D(channels=16,kernel_size=5))
    net4.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net4.add(gluon.nn.Activation(activation='relu'))
    net4.add(nn.MaxPool2D(pool_size=2, strides=2))
    net4.add(nn.Flatten())
    net4.add(nn.Dense(120)) 
    net4.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net4.add(gluon.nn.Activation(activation='relu'))
    net4.add(nn.Dense(84)) 
    net4.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net4.add(gluon.nn.Activation(activation='relu'))
    net4.add(nn.Dense(num_output))

print net4
net4.initialize(MyInit(),ctx=ctx)
trainer4 = gluon.Trainer(net4.collect_params(), optimizer, optimizer_params)
loss_fn4 = gluon.loss.SoftmaxCrossEntropyLoss()
train_metric4 = mx.metric.Accuracy()
train_history4 = TrainingHistory(['training-error', 'validation-error'])
```

```{.json .output n=44}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n  (2): Activation(relu)\n  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (4): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (5): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n  (6): Activation(relu)\n  (7): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (8): Flatten\n  (9): Dense(None -> 120, linear)\n  (10): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n  (11): Activation(relu)\n  (12): Dense(None -> 84, linear)\n  (13): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n  (14): Activation(relu)\n  (15): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=50}
epochs=20
for epoch in range(epochs):
    tic = time.time()
    train_metric4.reset() #1
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        trainer4.set_learning_rate(trainer4.learning_rate*lr_decay) #2
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net4(X) for X in data] #1
            loss = [loss_fn4(yhat, y) for yhat, y in zip(output, label)] #1

        # Backpropagation
        for l in loss:
            l.backward()

        # Optimize
        trainer4.step(batch_size) #1

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric4.update(label, output) #1

    name, acc = train_metric4.get() #1
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net4) #1

    # Update history and print metrics
    train_history4.update([1-acc, 1-val_acc]) #1
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net4[0].weight.data()[0][0] #1
    
file_name = "net-params_std0_01_lr10_bn"
net4.save_parameters(file_name) #1
```

```{.json .output n=50}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "[Epoch 0] train=0.572196 val=0.682800 loss=59942.433838 time: 25.775940\n\n[[-0.14603357 -0.15902223 -0.19037658 -0.16968198 -0.14279298]\n [ 0.03265335  0.00303119 -0.00554689  0.00390562 -0.07119174]\n [ 0.15423922  0.11852524  0.1588346   0.1799965   0.12207413]\n [ 0.15388788  0.12306624  0.16944128  0.20776372  0.23413529]\n [ 0.13705221  0.11893569  0.12270483  0.12856992  0.14039668]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 1] train=0.573057 val=0.685800 loss=60028.215271 time: 25.842599\n\n[[-0.14980765 -0.16475108 -0.19566196 -0.17602988 -0.14763165]\n [ 0.02584381 -0.00394333 -0.01277554 -0.00557943 -0.07874657]\n [ 0.14242445  0.10759469  0.1478635   0.16823542  0.11311716]\n [ 0.14857832  0.11840232  0.16554078  0.20595305  0.23559067]\n [ 0.13496926  0.11459446  0.1182195   0.12894337  0.14482653]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 2] train=0.577304 val=0.691100 loss=59823.591431 time: 25.802029\n\n[[-0.15023696 -0.16143394 -0.1905968  -0.1692661  -0.14000282]\n [ 0.03017083  0.00258266 -0.00659837  0.00124291 -0.07050102]\n [ 0.14721195  0.11355701  0.15305868  0.17567103  0.1204539 ]\n [ 0.14771484  0.11662954  0.16154745  0.20271271  0.23121284]\n [ 0.13569257  0.11071847  0.11094093  0.11891516  0.13097928]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 3] train=0.578666 val=0.677500 loss=59435.328308 time: 25.244419\n\n[[-1.65615499e-01 -1.76713809e-01 -2.02746809e-01 -1.79776400e-01\n  -1.46962777e-01]\n [ 3.19483280e-02  1.65583938e-03 -8.14595819e-03 -1.05481755e-04\n  -6.79925010e-02]\n [ 1.48976222e-01  1.13004208e-01  1.51438653e-01  1.77155271e-01\n   1.28409043e-01]\n [ 1.48513570e-01  1.17209867e-01  1.61867052e-01  2.06069306e-01\n   2.44076520e-01]\n [ 1.35120898e-01  1.08881548e-01  1.15286045e-01  1.29130468e-01\n   1.50029168e-01]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 4] train=0.577003 val=0.674900 loss=59482.947250 time: 25.478456\n\n[[-0.15975429 -0.16480708 -0.18910374 -0.17438695 -0.14704765]\n [ 0.03358617  0.0106415   0.00355848  0.00239696 -0.07384628]\n [ 0.14492594  0.11644163  0.15971094  0.18074913  0.12580957]\n [ 0.13915254  0.11307843  0.16267699  0.20462035  0.23816392]\n [ 0.1245413   0.10078286  0.11120375  0.12271666  0.13680707]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 5] train=0.576542 val=0.673100 loss=59806.709473 time: 25.969602\n\n[[-1.61143571e-01 -1.67945415e-01 -1.91726655e-01 -1.73211798e-01\n  -1.35849833e-01]\n [ 3.66257057e-02  1.06777623e-02  9.58971679e-04 -1.49941377e-04\n  -6.89434186e-02]\n [ 1.49540976e-01  1.19972564e-01  1.61128044e-01  1.81707203e-01\n   1.29839346e-01]\n [ 1.51875228e-01  1.24837615e-01  1.69054657e-01  2.10521132e-01\n   2.41336390e-01]\n [ 1.40314296e-01  1.14841811e-01  1.19515710e-01  1.28676668e-01\n   1.36817157e-01]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 6] train=0.579547 val=0.686900 loss=59444.286041 time: 35.152516\n\n[[-0.16656427 -0.1737427  -0.19839144 -0.17848213 -0.14609233]\n [ 0.03047175  0.00717662 -0.00171374 -0.00226455 -0.07607027]\n [ 0.13552314  0.11378001  0.15977715  0.18277766  0.12791336]\n [ 0.13261892  0.11509117  0.16549487  0.2091479   0.23849873]\n [ 0.11923721  0.10301196  0.11557136  0.1276504   0.13328226]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 7] train=0.575861 val=0.680000 loss=60059.066772 time: 33.763183\n\n[[-0.16103406 -0.17136526 -0.20003009 -0.1828948  -0.15073091]\n [ 0.04118047  0.0144246   0.00155862 -0.0031238  -0.07802594]\n [ 0.15287235  0.12496293  0.1673149   0.18561603  0.12775457]\n [ 0.14894679  0.12182466  0.16591603  0.20140287  0.2261577 ]\n [ 0.13072848  0.10408875  0.11057837  0.1113198   0.11115663]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 8] train=0.573858 val=0.684900 loss=59944.517120 time: 43.434770\n\n[[-0.16951782 -0.17654346 -0.20338504 -0.17934404 -0.14196673]\n [ 0.0302026   0.00653995 -0.00049484  0.00262982 -0.06645069]\n [ 0.14004886  0.1116266   0.1625507   0.18913832  0.13715959]\n [ 0.13879149  0.10921058  0.16069579  0.20342341  0.23457685]\n [ 0.12782654  0.09639381  0.1106948   0.11775742  0.12305462]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 9] train=0.573778 val=0.688800 loss=59833.418243 time: 37.367102\n\n[[-0.16599983 -0.17582639 -0.20282584 -0.17892425 -0.1405261 ]\n [ 0.03897914  0.01018827 -0.00116011 -0.00168494 -0.07333556]\n [ 0.14504562  0.11175001  0.15817158  0.18103759  0.1249421 ]\n [ 0.14435306  0.10888285  0.1580839   0.19955258  0.22560932]\n [ 0.137473    0.0990483   0.11207953  0.11896841  0.11979318]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 10] train=0.579287 val=0.678900 loss=59408.051300 time: 33.115305\n\n[[-0.16649164 -0.1739001  -0.20106056 -0.17627427 -0.13305841]\n [ 0.03852123  0.01427809  0.00393554  0.00457015 -0.06268063]\n [ 0.14479226  0.11538289  0.16091143  0.18731546  0.13792117]\n [ 0.13830794  0.10723888  0.1580086   0.20447868  0.23819073]\n [ 0.13051753  0.09485524  0.11024724  0.11988611  0.12415724]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 11] train=0.576282 val=0.687400 loss=59912.562164 time: 28.519298\n\n[[-0.16087258 -0.16969173 -0.20046991 -0.18098335 -0.13398051]\n [ 0.03731424  0.01003666 -0.00367219 -0.00562297 -0.0710078 ]\n [ 0.14304268  0.11089251  0.15519859  0.18079522  0.1306759 ]\n [ 0.14077742  0.1083906   0.15871917  0.2051735   0.24120569]\n [ 0.12870444  0.0958952   0.11418615  0.12760822  0.13512062]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 12] train=0.575721 val=0.682900 loss=59593.997940 time: 31.057483\n\n[[-0.16655102 -0.17334245 -0.20217334 -0.17824389 -0.1272242 ]\n [ 0.0365286   0.01132799  0.00044389  0.00086157 -0.06713447]\n [ 0.13788728  0.10681612  0.15304717  0.18372506  0.12917373]\n [ 0.13029978  0.10044602  0.15259008  0.20359674  0.23646092]\n [ 0.11154339  0.08202014  0.10119025  0.11949691  0.12726898]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 13] train=0.578405 val=0.659800 loss=59557.668488 time: 28.995410\n\n[[-0.1714038  -0.18308541 -0.21566041 -0.19093093 -0.13752955]\n [ 0.03663842  0.01131826 -0.00338281 -0.00200352 -0.07177765]\n [ 0.13784891  0.10939788  0.15401435  0.1881399   0.1274679 ]\n [ 0.12991071  0.10371479  0.15837725  0.21342067  0.24051496]\n [ 0.10496151  0.08049718  0.10510632  0.12784557  0.13051976]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 14] train=0.577825 val=0.673700 loss=59300.294098 time: 33.955170\n\n[[-0.17231679 -0.19138202 -0.22430786 -0.19681987 -0.13303547]\n [ 0.03644122  0.00631517 -0.00584476 -0.00439877 -0.06845437]\n [ 0.1350602   0.1073266   0.15556702  0.1871241   0.12481459]\n [ 0.1269493   0.10322319  0.16157186  0.21215451  0.23886035]\n [ 0.10901254  0.08669118  0.11145823  0.12625569  0.12749344]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 15] train=0.577845 val=0.688600 loss=59395.131622 time: 36.954657\n\n[[-0.17616077 -0.19427238 -0.22421992 -0.19537123 -0.12816778]\n [ 0.03457471  0.00406361 -0.00639198 -0.00392792 -0.06565206]\n [ 0.13542241  0.10690977  0.15522459  0.18578205  0.12311471]\n [ 0.13384104  0.10636263  0.16254298  0.21000104  0.23420566]\n [ 0.11971261  0.09179319  0.11393203  0.12198903  0.11589731]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 16] train=0.576843 val=0.686700 loss=59635.329132 time: 35.685588\n\n[[-1.75689608e-01 -1.90885425e-01 -2.18577147e-01 -1.89169466e-01\n  -1.23008035e-01]\n [ 3.70582230e-02  9.32091195e-03 -4.06101055e-04  1.55038142e-04\n  -6.27334565e-02]\n [ 1.38707802e-01  1.12021737e-01  1.60366297e-01  1.90089002e-01\n   1.23961858e-01]\n [ 1.34800583e-01  1.08386733e-01  1.63885236e-01  2.10166737e-01\n   2.29639664e-01]\n [ 1.28961235e-01  9.82643440e-02  1.13924772e-01  1.15583725e-01\n   1.03194356e-01]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 17] train=0.578566 val=0.684200 loss=59215.537506 time: 33.313664\n\n[[-0.17026198 -0.18541665 -0.21841364 -0.19672522 -0.13266793]\n [ 0.04318311  0.01562892 -0.00080302 -0.00970185 -0.07766514]\n [ 0.14506559  0.11814235  0.16198888  0.18703109  0.11591807]\n [ 0.13439612  0.10762313  0.16177829  0.21024181  0.23083507]\n [ 0.11850895  0.08775908  0.10640366  0.1140784   0.10458827]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 18] train=0.578926 val=0.691100 loss=59234.735352 time: 32.669286\n\n[[-0.16256796 -0.17916492 -0.21268581 -0.1918196  -0.1288787 ]\n [ 0.04942797  0.01970537  0.00248761 -0.00516303 -0.07306468]\n [ 0.1461603   0.11790451  0.16190036  0.1934794   0.12584834]\n [ 0.13461095  0.11054029  0.1662505   0.22118525  0.24611418]\n [ 0.1178081   0.09067946  0.11287567  0.12531805  0.1193085 ]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 19] train=0.577544 val=0.698700 loss=59441.259247 time: 33.644681\n\n[[-1.70281723e-01 -1.82739586e-01 -2.17906982e-01 -1.97007284e-01\n  -1.32131770e-01]\n [ 4.60177101e-02  1.89062599e-02 -6.91305977e-05 -8.86073615e-03\n  -7.84535035e-02]\n [ 1.48991033e-01  1.21090829e-01  1.65142611e-01  1.95340529e-01\n   1.23841412e-01]\n [ 1.42480940e-01  1.14862509e-01  1.69106126e-01  2.24065140e-01\n   2.45200694e-01]\n [ 1.30430967e-01  9.93597358e-02  1.16564654e-01  1.29038006e-01\n   1.19374909e-01]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 20] train=0.582692 val=0.653700 loss=59136.272888 time: 31.537186\n\n[[-1.69831723e-01 -1.82269499e-01 -2.14499310e-01 -1.95514232e-01\n  -1.35699362e-01]\n [ 4.29987051e-02  1.66225582e-02  2.38765700e-04 -8.25920887e-03\n  -8.27663466e-02]\n [ 1.40934780e-01  1.15803272e-01  1.62506595e-01  1.98038831e-01\n   1.24112077e-01]\n [ 1.27703413e-01  1.03348665e-01  1.60688907e-01  2.22512037e-01\n   2.45951906e-01]\n [ 1.12845533e-01  8.69638920e-02  1.07011005e-01  1.25962019e-01\n   1.16105549e-01]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 21] train=0.578305 val=0.685700 loss=59152.966461 time: 37.463324\n\n[[-0.17199732 -0.18500262 -0.2166177  -0.19333501 -0.13289672]\n [ 0.03944644  0.01538428  0.0034442  -0.00221349 -0.07786304]\n [ 0.14170298  0.11944303  0.1709452   0.2088368   0.13410157]\n [ 0.13482998  0.11275672  0.17184779  0.23438291  0.2550407 ]\n [ 0.11765416  0.09108867  0.11076175  0.13027981  0.11681315]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 22] train=0.579107 val=0.688000 loss=59419.486832 time: 42.111918\n\n[[-0.16174199 -0.17799415 -0.21436855 -0.19751036 -0.13824174]\n [ 0.04372929  0.01734335  0.00478039 -0.00660962 -0.08587242]\n [ 0.13934864  0.11601837  0.1697847   0.20419165  0.1278893 ]\n [ 0.12749875  0.10619985  0.16728541  0.22836283  0.25197017]\n [ 0.10830218  0.08355837  0.10399048  0.12162377  0.11237418]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 23] train=0.576522 val=0.686700 loss=59636.112885 time: 31.269955\n\n[[-0.17375486 -0.18676393 -0.22400782 -0.20479763 -0.14203928]\n [ 0.03735369  0.01543336  0.0014424  -0.00832862 -0.08597984]\n [ 0.13670486  0.12052278  0.17126803  0.20432758  0.12916973]\n [ 0.12548472  0.11249762  0.17183347  0.22839653  0.24972306]\n [ 0.11049876  0.09334987  0.11427981  0.12820938  0.11443011]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 24] train=0.578546 val=0.693000 loss=59629.311401 time: 33.854581\n\n[[-0.17352968 -0.19115868 -0.23120536 -0.2092984  -0.1435989 ]\n [ 0.03264135  0.00831295 -0.00436953 -0.00769594 -0.08089508]\n [ 0.12859987  0.11131147  0.16611972  0.20642254  0.136163  ]\n [ 0.11544757  0.10235596  0.16817492  0.2320177   0.25771165]\n [ 0.09886631  0.0836473   0.1109328   0.13175304  0.12328836]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 25] train=0.580549 val=0.680200 loss=59090.213318 time: 36.233436\n\n[[-0.16365497 -0.17736459 -0.22327554 -0.20257002 -0.13829984]\n [ 0.04177576  0.01972325  0.00133284 -0.0040708  -0.08131443]\n [ 0.12998024  0.11734407  0.16735609  0.20468834  0.13001065]\n [ 0.11051003  0.10352883  0.16664647  0.22886503  0.2508228 ]\n [ 0.09111648  0.08252432  0.10892108  0.12736966  0.11455177]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 26] train=0.582412 val=0.684800 loss=59080.484955 time: 37.419441\n\n[[-0.15633363 -0.17048325 -0.22012575 -0.19875924 -0.12792882]\n [ 0.0443303   0.02229997  0.00140215 -0.00475982 -0.07670794]\n [ 0.12804824  0.11745776  0.1697754   0.20925422  0.13580282]\n [ 0.11088592  0.10397342  0.17374076  0.23974384  0.25980076]\n [ 0.09305421  0.08289891  0.1145269   0.13609314  0.12099022]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 27] train=0.580589 val=0.686400 loss=59413.411377 time: 47.779390\n\n[[-0.16569166 -0.1800561  -0.2250572  -0.2006978  -0.12343753]\n [ 0.03936747  0.01764565  0.00086251 -0.00335562 -0.07066235]\n [ 0.13062605  0.11582614  0.1669256   0.2099506   0.14314438]\n [ 0.11382161  0.10023431  0.16755211  0.23936714  0.26629415]\n [ 0.09507544  0.07743923  0.10574533  0.13416943  0.126025  ]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 28] train=0.579828 val=0.687100 loss=59276.129456 time: 41.521000\n\n[[-0.17135035 -0.18122718 -0.2237964  -0.19655997 -0.1191086 ]\n [ 0.03167868  0.01369391 -0.0016239  -0.00389209 -0.07008369]\n [ 0.12344395  0.11164552  0.16434662  0.209721    0.14264959]\n [ 0.11054862  0.0987943   0.16576152  0.24098067  0.26628762]\n [ 0.09588125  0.0789414   0.10452372  0.13598418  0.12779881]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 29] train=0.576843 val=0.694000 loss=59162.226898 time: 52.345280\n\n[[-0.1799171  -0.18765739 -0.22837117 -0.20360355 -0.12735492]\n [ 0.03019067  0.01626788  0.001916   -0.00637746 -0.07648005]\n [ 0.12326834  0.11421783  0.16861796  0.20805596  0.13645715]\n [ 0.11090425  0.1014345   0.17152983  0.2421178   0.26292905]\n [ 0.09798618  0.08551268  0.11272787  0.14090852  0.12799765]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 30] train=0.581530 val=0.690500 loss=59265.128326 time: 41.321410\n\n[[-0.17702211 -0.18125258 -0.22375603 -0.20356862 -0.13242264]\n [ 0.03235339  0.01809664  0.00113873 -0.0137603  -0.08723657]\n [ 0.12978764  0.11746547  0.17091067  0.20571649  0.13168003]\n [ 0.11983538  0.10792149  0.17895031  0.24607024  0.26319057]\n [ 0.10680535  0.0948737   0.12350615  0.1477565   0.13217802]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 31] train=0.582632 val=0.678300 loss=59109.229553 time: 30.616057\n\n[[-0.18257867 -0.18399648 -0.21948044 -0.197826   -0.12484485]\n [ 0.0305292   0.01841536  0.00579379 -0.00962239 -0.08211487]\n [ 0.12450079  0.11429762  0.17197073  0.20774843  0.13331299]\n [ 0.10779452  0.1000407   0.17689528  0.24522606  0.26542735]\n [ 0.08880439  0.08201305  0.11675279  0.14343943  0.13150981]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 32] train=0.586558 val=0.694400 loss=58762.480408 time: 31.992529\n\n[[-0.17427292 -0.17339492 -0.21426351 -0.19955449 -0.13694037]\n [ 0.04423474  0.03351778  0.0168952  -0.00457414 -0.08608097]\n [ 0.1354123   0.12566695  0.18022867  0.21043667  0.12840821]\n [ 0.11668097  0.10896061  0.18009372  0.23999661  0.25569594]\n [ 0.09567918  0.0882896   0.11518077  0.12988272  0.11523227]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 33] train=0.580649 val=0.688200 loss=59026.749313 time: 32.009527\n\n[[-0.16877553 -0.17415607 -0.2195227  -0.20779878 -0.14353575]\n [ 0.04498834  0.03036996  0.01040046 -0.01340658 -0.09423714]\n [ 0.13395089  0.12240621  0.17677823  0.20904753  0.12763265]\n [ 0.11789089  0.1085202   0.17988187  0.24303077  0.25866798]\n [ 0.10135834  0.09329896  0.11930907  0.1343511   0.11583976]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 34] train=0.582672 val=0.665000 loss=58895.035553 time: 33.382202\n\n[[-0.17253567 -0.17440787 -0.21485172 -0.20048603 -0.13718638]\n [ 0.04423035  0.02992486  0.01362607 -0.00827666 -0.09034697]\n [ 0.1299631   0.11847649  0.17656867  0.2098329   0.12702817]\n [ 0.1131565   0.10450494  0.18109548  0.2442418   0.25480586]\n [ 0.10182001  0.09529042  0.12649639  0.1397929   0.11111032]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 35] train=0.576222 val=0.684900 loss=59602.777313 time: 33.648511\n\n[[-0.17363894 -0.17569646 -0.21441054 -0.1965637  -0.12927294]\n [ 0.05258465  0.03715964  0.02246697  0.00165948 -0.07753225]\n [ 0.13624395  0.12453087  0.18418847  0.217189    0.1360777 ]\n [ 0.11151516  0.10367586  0.18421414  0.2504564   0.2625168 ]\n [ 0.10139994  0.09647132  0.13392332  0.15483461  0.12785926]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 36] train=0.580268 val=0.687800 loss=59011.336807 time: 32.944070\n\n[[-0.17676635 -0.17730506 -0.21589528 -0.19682607 -0.13339634]\n [ 0.04433761  0.02922551  0.01542197 -0.00186306 -0.08336669]\n [ 0.12928443  0.11744218  0.1773222   0.21134861  0.12735067]\n [ 0.10800607  0.09972946  0.1777551   0.2443133   0.2562287 ]\n [ 0.09199292  0.08861659  0.12017376  0.13971469  0.11289055]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 37] train=0.585076 val=0.681200 loss=58603.900208 time: 40.569249\n\n[[-0.17954038 -0.17854133 -0.2167686  -0.19543783 -0.12435585]\n [ 0.04914516  0.03310022  0.0182412   0.00080644 -0.07497116]\n [ 0.14108181  0.12771916  0.1875778   0.22051802  0.1394734 ]\n [ 0.12160756  0.10791124  0.18305355  0.24893732  0.2637714 ]\n [ 0.09698243  0.08861227  0.11849998  0.14006627  0.11500613]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 38] train=0.581751 val=0.687000 loss=59073.572937 time: 35.497103\n\n[[-0.19000997 -0.19165346 -0.22710544 -0.2000538  -0.12045868]\n [ 0.04406615  0.02378416  0.00910595 -0.00412396 -0.07429879]\n [ 0.13238956  0.11674318  0.1801126   0.21967228  0.13678505]\n [ 0.10651188  0.09104799  0.17160864  0.24629958  0.2580018 ]\n [ 0.08286452  0.072797    0.10756749  0.13746883  0.10655832]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 39] train=0.581230 val=0.684900 loss=59032.091507 time: 33.223423\n\n[[-0.19476622 -0.19523679 -0.22719689 -0.19464621 -0.11485132]\n [ 0.04680486  0.02449597  0.0118648   0.0010873  -0.0706819 ]\n [ 0.13499026  0.1173152   0.18348771  0.2235332   0.1366994 ]\n [ 0.10364197  0.0854711   0.1689414   0.24415205  0.25296617]\n [ 0.07489649  0.06644145  0.10746961  0.13591535  0.10101354]]\n<NDArray 5x5 @cpu(1)>\n"
 }
]
```

```{.python .input  n=52}
epochs=20
for epoch in range(epochs):
    tic = time.time()
    train_metric4.reset() #1
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        trainer4.set_learning_rate(trainer4.learning_rate*lr_decay) #2
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net4(X) for X in data] #1
            loss = [loss_fn4(yhat, y) for yhat, y in zip(output, label)] #1

        # Backpropagation
        for l in loss:
            l.backward()

        # Optimize
        trainer4.step(batch_size) #1

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric4.update(label, output) #1

    name, acc = train_metric4.get() #1
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net4) #1

    # Update history and print metrics
    train_history4.update([1-acc, 1-val_acc]) #1
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch+40, acc, val_acc, train_loss, time.time()-tic))
    print net4[0].weight.data()[0][0] #1
    
file_name = "net-params_std0_01_lr10_bn"
net4.save_parameters(file_name) #1
```

```{.json .output n=52}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "[Epoch 40] train=0.580088 val=0.679200 loss=59139.045197 time: 44.841745\n\n[[-0.1790886  -0.17884654 -0.2172374  -0.19460548 -0.11834127]\n [ 0.06721558  0.04515892  0.02790034  0.00722044 -0.06920212]\n [ 0.15759805  0.13678305  0.20101212  0.23280568  0.14038725]\n [ 0.12012667  0.0934955   0.1740543   0.24445806  0.24965353]\n [ 0.08826998  0.07320988  0.11066172  0.13647076  0.09969378]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 41] train=0.581831 val=0.687500 loss=59004.880585 time: 41.788028\n\n[[-0.18276434 -0.1805806  -0.22168306 -0.20416518 -0.12853472]\n [ 0.06297252  0.04364337  0.02492625 -0.00307466 -0.08345082]\n [ 0.14706962  0.1321489   0.19999203  0.2277136   0.13016486]\n [ 0.1020696   0.08357994  0.17235593  0.24444501  0.24801166]\n [ 0.06985969  0.06455009  0.10944167  0.13905105  0.10256851]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 42] train=0.578225 val=0.679100 loss=59338.291321 time: 32.685989\n\n[[-0.19194591 -0.19186433 -0.2311637  -0.20895848 -0.12940572]\n [ 0.06153415  0.03901345  0.02095994 -0.00683334 -0.08647829]\n [ 0.14413904  0.12667346  0.19513854  0.22428817  0.12754725]\n [ 0.10439311  0.08341224  0.1711209   0.244766    0.25347826]\n [ 0.07705936  0.06792881  0.10865861  0.14018658  0.11029688]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 43] train=0.580268 val=0.688400 loss=59070.194855 time: 37.436530\n\n[[-0.19918174 -0.1974945  -0.23819952 -0.21289305 -0.12818636]\n [ 0.05582479  0.03700092  0.02047893 -0.00451306 -0.07776318]\n [ 0.13936093  0.12222403  0.19109134  0.22470562  0.13637999]\n [ 0.10506971  0.08100583  0.16805786  0.24766657  0.26432818]\n [ 0.08469875  0.0712426   0.10815807  0.14266478  0.11869229]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 44] train=0.585116 val=0.687400 loss=58647.617249 time: 28.152512\n\n[[-0.1851941  -0.19232935 -0.24222058 -0.22299716 -0.13400951]\n [ 0.06998768  0.04616332  0.02260301 -0.00881539 -0.08124708]\n [ 0.14946502  0.12832966  0.1938577   0.22592962  0.13831341]\n [ 0.11105789  0.08244252  0.16596857  0.24639292  0.26433563]\n [ 0.07982354  0.06527867  0.10228682  0.13998118  0.11854877]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 45] train=0.581530 val=0.681600 loss=59266.337555 time: 45.249719\n\n[[-0.17233956 -0.17762344 -0.22702488 -0.21651584 -0.13837798]\n [ 0.06982299  0.04810291  0.02808489 -0.01282221 -0.094183  ]\n [ 0.14429925  0.12751868  0.19627696  0.2227265   0.12541586]\n [ 0.11034207  0.0841517   0.16642462  0.2410431   0.25080487]\n [ 0.07690236  0.06298842  0.09344687  0.12686965  0.10043882]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 46] train=0.581530 val=0.679200 loss=58857.748260 time: 41.023381\n\n[[-0.17999643 -0.1828349  -0.23102278 -0.21577987 -0.1336315 ]\n [ 0.06089196  0.04346     0.02430607 -0.01430162 -0.09334124]\n [ 0.13538016  0.12266342  0.19377266  0.22158471  0.12431511]\n [ 0.10830482  0.08552659  0.17162901  0.24694732  0.2551974 ]\n [ 0.079241    0.06902549  0.1067135   0.14058836  0.11186092]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 47] train=0.586298 val=0.683800 loss=58574.258179 time: 42.335174\n\n[[-0.18014108 -0.18328477 -0.22582419 -0.20532224 -0.11946803]\n [ 0.06074428  0.04139329  0.02574621 -0.00463015 -0.07838305]\n [ 0.13430725  0.12023299  0.19283809  0.22858286  0.13800718]\n [ 0.11145908  0.08653425  0.1741167   0.25368765  0.2644596 ]\n [ 0.08217791  0.06986663  0.11048938  0.14772768  0.11830717]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 48] train=0.585938 val=0.680700 loss=58448.302887 time: 41.368094\n\n[[-0.18260257 -0.18553752 -0.22774455 -0.20762463 -0.12242928]\n [ 0.06210427  0.04391622  0.02810233 -0.00273716 -0.07909006]\n [ 0.13731705  0.1221277   0.19310121  0.22980158  0.13947353]\n [ 0.11307589  0.08364832  0.1670387   0.24718802  0.25798085]\n [ 0.08196829  0.0676424   0.10170569  0.1397685   0.11003623]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 49] train=0.583574 val=0.687200 loss=59006.816559 time: 46.871425\n\n[[-1.75500333e-01 -1.84100643e-01 -2.27960616e-01 -2.04175964e-01\n  -1.17377117e-01]\n [ 6.65397942e-02  4.26781699e-02  2.71220095e-02  2.35007086e-04\n  -7.59154260e-02]\n [ 1.36424929e-01  1.16591282e-01  1.90601259e-01  2.32122451e-01\n   1.42210200e-01]\n [ 1.09139785e-01  7.71139935e-02  1.63951173e-01  2.48862013e-01\n   2.61121392e-01]\n [ 7.68813193e-02  6.29257113e-02  9.84314680e-02  1.39053613e-01\n   1.07748799e-01]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 50] train=0.582873 val=0.680600 loss=58777.533630 time: 36.258275\n\n[[-0.17957227 -0.1862358  -0.23303498 -0.21155186 -0.12103805]\n [ 0.06578708  0.04422416  0.02802755 -0.00687517 -0.08768228]\n [ 0.13603246  0.11785696  0.19238663  0.2222912   0.1228366 ]\n [ 0.11363978  0.08146659  0.16531256  0.24098924  0.24648383]\n [ 0.08785848  0.0722438   0.09945031  0.13423917  0.09770932]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 51] train=0.582692 val=0.693200 loss=58960.193542 time: 37.622875\n\n[[-0.1796857  -0.1841046  -0.23056813 -0.2087258  -0.1168156 ]\n [ 0.06595982  0.04814826  0.03328755 -0.00297994 -0.08115961]\n [ 0.1318405   0.11625762  0.19553156  0.22674434  0.13082175]\n [ 0.10756749  0.07687851  0.16632596  0.24290515  0.25347102]\n [ 0.07505126  0.06240199  0.09680945  0.13234077  0.10051187]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 52] train=0.585998 val=0.692900 loss=58552.248795 time: 31.212325\n\n[[-0.18842785 -0.19373505 -0.24326335 -0.2131348  -0.11627273]\n [ 0.07090037  0.05175283  0.02937771 -0.00493683 -0.08074248]\n [ 0.14004113  0.12155429  0.19237874  0.22713648  0.1313893 ]\n [ 0.11697946  0.08322131  0.16550544  0.24610959  0.25887686]\n [ 0.08184858  0.06749109  0.09796888  0.1394737   0.11269466]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 53] train=0.581470 val=0.696800 loss=59027.997040 time: 29.898568\n\n[[-0.17895198 -0.18402217 -0.23349889 -0.20238186 -0.10994849]\n [ 0.07331367  0.05580177  0.03468047  0.00065177 -0.07921654]\n [ 0.13803576  0.12292401  0.196778    0.23091318  0.13052742]\n [ 0.11500677  0.0849814   0.16909197  0.2514252   0.26451975]\n [ 0.07593243  0.06731295  0.10039212  0.14547892  0.12139428]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 54] train=0.584295 val=0.695100 loss=58766.149338 time: 33.171573\n\n[[-0.18334582 -0.18730044 -0.24186264 -0.21479581 -0.12529708]\n [ 0.07687394  0.05870764  0.03321519 -0.00771131 -0.09117539]\n [ 0.14107776  0.12579432  0.19585153  0.225266    0.12383442]\n [ 0.11637757  0.08373088  0.16488546  0.24669535  0.26158288]\n [ 0.07464445  0.06245169  0.09260658  0.1393357   0.11890575]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 55] train=0.585116 val=0.674000 loss=58603.019135 time: 27.317622\n\n[[-0.19607335 -0.19978814 -0.24904291 -0.22183207 -0.13714187]\n [ 0.06299591  0.04622075  0.02116521 -0.01834257 -0.10755336]\n [ 0.13265017  0.11801464  0.1876073   0.22005534  0.1124924 ]\n [ 0.12086349  0.08429191  0.16103797  0.2415071   0.25234702]\n [ 0.08275688  0.06562136  0.09058003  0.13125266  0.10461842]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 56] train=0.585377 val=0.692800 loss=58670.271515 time: 32.397149\n\n[[-0.17437471 -0.17977469 -0.2412012  -0.21701892 -0.13318232]\n [ 0.08459105  0.06759146  0.03213788 -0.00834505 -0.10151673]\n [ 0.14409392  0.12891145  0.19320682  0.23154438  0.12077013]\n [ 0.12048903  0.08776107  0.16152641  0.24786767  0.2581285 ]\n [ 0.07637005  0.06545349  0.08997201  0.13244219  0.10125396]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 57] train=0.588862 val=0.685400 loss=58353.744141 time: 33.792955\n\n[[-0.18504886 -0.18525212 -0.247283   -0.22635302 -0.13928962]\n [ 0.07280789  0.05966436  0.02593324 -0.01676029 -0.10425422]\n [ 0.13562141  0.12427322  0.18901847  0.22550912  0.12074079]\n [ 0.12033396  0.09083539  0.16460778  0.24691947  0.2593363 ]\n [ 0.0772868   0.06714167  0.0903132   0.12863861  0.09467382]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 58] train=0.583153 val=0.695400 loss=58632.242508 time: 35.382978\n\n[[-0.18628033 -0.1796817  -0.23699711 -0.21209578 -0.12393643]\n [ 0.07130613  0.06231136  0.03250101 -0.00797529 -0.09786021]\n [ 0.13498726  0.12596737  0.19491515  0.23705204  0.12907171]\n [ 0.12199218  0.09194071  0.16673863  0.2555852   0.2681322 ]\n [ 0.08267186  0.06810296  0.09281397  0.138528    0.10693632]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 59] train=0.582672 val=0.677600 loss=58739.587677 time: 38.371675\n\n[[-0.20011641 -0.18963383 -0.24784385 -0.22051585 -0.12908196]\n [ 0.06649036  0.06196351  0.03320714 -0.00710864 -0.09746651]\n [ 0.13354197  0.12507433  0.19431074  0.23748823  0.12989612]\n [ 0.11715592  0.08708142  0.16239806  0.2535791   0.26552498]\n [ 0.07826377  0.05952589  0.08183444  0.12917022  0.09709397]]\n<NDArray 5x5 @cpu(1)>\n"
 }
]
```

```{.python .input}
print net4[0].weight.data()[0][0] #1
train_history4.plot() #1
```

## Problem2

### c）
From the acc, we can know that after adding BN layer, our
model yield 2% higher acc than the original one.

```{.python .input  n=138}
global states
global t
t=1
v1=nd.zeros((6,3,5,5))
vb1=nd.zeros((6,))
v2=nd.zeros((16,6,5,5))
vb2=nd.zeros((16,))
v3=nd.zeros((120,400))
vb3=nd.zeros((120,))
v4=nd.zeros((84,120))
vb4=nd.zeros((84,))
v5=nd.zeros((10,84))
vb5=nd.zeros((10,))
s1=nd.zeros((6,3,5,5))
sb1=nd.zeros((6,))
s2=nd.zeros((16,6,5,5))
sb2=nd.zeros((16,))
s3=nd.zeros((120,400))
sb3=nd.zeros((120,))
s4=nd.zeros((84,120))
sb4=nd.zeros((84,))
s5=nd.zeros((10,84))
sb5=nd.zeros((10,))
states=[[v1,s1],[vb1,sb1],[v2,s2],[vb2,sb2],[v3,s3],[vb3,sb3],[v4,s4],[vb4,sb4],[v5,s5],[vb5,sb5]]
```

```{.python .input  n=139}
def adam(net,  lr): 
    global t
    global states
    beta1, beta2, eps = 0.9, 0.999, 1e-6 
    params=[]
    for layer in net:
        try:
            params.append(layer.weight)
            params.append(layer.bias)
        except:
            pass
    for p, (v, s) in zip(params, states):
        v[:] = beta1 * v + (1 - beta1) * p.grad()
        s[:] = beta2 * s + (1 - beta2) * p.grad().square()
        v_bias_corr = v / (1 - beta1 ** t)
        s_bias_corr = s / (1 - beta2 ** t)
        p.data()[:] -= lr * v_bias_corr / (s_bias_corr.sqrt() + eps)
    t += 1

```

```{.python .input  n=140}
ctx_list=[mx.cpu(0)]
ctx=mx.cpu(0)
net5 = nn.Sequential()
with net5.name_scope():
    net5.add(nn.Conv2D(channels=6, kernel_size=5)) 
    net5.add(gluon.nn.Activation(activation='relu'))
    net5.add(nn.MaxPool2D(pool_size=2, strides=2))
    net5.add(nn.Conv2D(channels=16,kernel_size=5))
    net5.add(gluon.nn.Activation(activation='relu'))
    net5.add(nn.MaxPool2D(pool_size=2, strides=2))
    net5.add(nn.Flatten())
    net5.add(nn.Dense(120)) 
    net5.add(gluon.nn.Activation(activation='relu'))
    net5.add(nn.Dense(84)) 
    net5.add(gluon.nn.Activation(activation='relu'))
    net5.add(nn.Dense(num_output))
    
class MyInit(mx.init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.normal(loc=0, scale=0.1, shape=data.shape )
        
print net5
net5.initialize(MyInit(),ctx=ctx)
```

```{.json .output n=140}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): Activation(relu)\n  (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (3): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (4): Activation(relu)\n  (5): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (6): Flatten\n  (7): Dense(None -> 120, linear)\n  (8): Activation(relu)\n  (9): Dense(None -> 84, linear)\n  (10): Activation(relu)\n  (11): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=141}
lr_decay = 0.01
lr_decay_epoch = [80, 160, np.inf]
optimizer = 'adam'
optimizer_params = {'learning_rate': 0.01, 'wd': 0.0005}
trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)
lr_decay_count = 0
```

```{.python .input  n=142}
trainer5 = gluon.Trainer(net5.collect_params(), optimizer, optimizer_params)  #2
loss_fn5 = gluon.loss.SoftmaxCrossEntropyLoss()
train_metric5 = mx.metric.Accuracy()
train_history5 = TrainingHistory(['training-error', 'validation-error'])
lr=0.01
```

```{.python .input  n=147}
epochs=30
for epoch in range(epochs):
    tic = time.time()
    train_metric5.reset() #1
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        #trainer5.set_learning_rate(trainer5.learning_rate*lr_decay) #2
        lr=lr*lr_decay
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net5(X) for X in data] #1
            loss = [loss_fn5(yhat, y) for yhat, y in zip(output, label)] #1

        # Backpropagation
        for l in loss:
            l.backward()
            
#         if i==0:
#             for layer in net5:
#                 try:
#                     print layer.weight.grad().shape
#                     print layer.bias.grad().shape
#                 except:
#                     pass
        adam(net5,lr)
        # Optimize
        #trainer5.step(batch_size) #1

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric5.update(label, output) #1

    name, acc = train_metric5.get() #1
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net5) #1

    # Update history and print metrics
    train_history5.update([1-acc, 1-val_acc]) #1
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net5[0].weight.data()[0][0] #1

    
file_name = "net-params_std0_01_lr10_last"
net5.save_parameters(file_name) #1
```

```{.json .output n=147}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "[Epoch 0] train=0.446655 val=0.558300 loss=78230.908295 time: 30.074366\n\n[[ 0.43593833 -0.2615098   0.22728083  0.14165498  0.24457178]\n [ 0.16941495  0.12840007  0.25741246  0.15103744  0.04822544]\n [ 0.00670256  0.26417762  0.7004164   0.88867754  0.5611713 ]\n [-0.13844611 -0.33274978 -0.19355474 -0.26605767 -0.15852594]\n [-0.16369076 -0.4872328  -0.98856634 -0.84301925 -0.6663806 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 1] train=0.445733 val=0.552600 loss=77911.799957 time: 27.530869\n\n[[ 0.4218921  -0.2872853   0.22022502  0.15412354  0.282124  ]\n [ 0.1524091   0.08347309  0.22505727  0.13364834  0.06317094]\n [-0.02718347  0.21588631  0.66896844  0.869814    0.5758323 ]\n [-0.19927295 -0.38353133 -0.22368428 -0.2777981  -0.14574268]\n [-0.21426941 -0.52455723 -1.0168583  -0.85500777 -0.6674238 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 2] train=0.449259 val=0.545900 loss=77683.129059 time: 24.797333\n\n[[ 0.44788852 -0.2850156   0.22656864  0.1497603   0.26267374]\n [ 0.17465028  0.10305067  0.24570253  0.12776361  0.04143757]\n [ 0.00306256  0.2669029   0.71867347  0.8702319   0.55795294]\n [-0.16491295 -0.32542008 -0.16536999 -0.27296695 -0.15921344]\n [-0.15537792 -0.46282065 -0.962313   -0.84477997 -0.67691606]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 3] train=0.444912 val=0.544700 loss=78175.342285 time: 24.517344\n\n[[ 0.4078627  -0.31644344  0.22461103  0.16060886  0.27816066]\n [ 0.15616429  0.09331085  0.25562695  0.1404584   0.05503519]\n [-0.01373289  0.24977535  0.7203245   0.8859901   0.57694423]\n [-0.1886357  -0.3568792  -0.17905256 -0.25599226 -0.12515569]\n [-0.19245973 -0.5075041  -0.99343914 -0.8419337  -0.6473537 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 4] train=0.445232 val=0.529800 loss=78242.828583 time: 24.436477\n\n[[ 0.39160132 -0.34564966  0.21599562  0.17932251  0.3098116 ]\n [ 0.1528073   0.07629641  0.24944909  0.15705366  0.09222568]\n [-0.01121814  0.24543431  0.72715896  0.9143261   0.6120363 ]\n [-0.16897877 -0.34633934 -0.17224999 -0.24602932 -0.10685191]\n [-0.16998889 -0.49949637 -0.9929791  -0.8480304  -0.6528089 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 5] train=0.448077 val=0.546200 loss=77890.716583 time: 23.710026\n\n[[ 0.40824407 -0.33447152  0.21967667  0.17639837  0.29938474]\n [ 0.1680032   0.090546    0.2600981   0.1604519   0.0788117 ]\n [-0.01240994  0.24607025  0.73408777  0.93111986  0.6092348 ]\n [-0.18207337 -0.3603315  -0.17940901 -0.23290181 -0.105276  ]\n [-0.18985362 -0.5198953  -1.0131667  -0.8455993  -0.6564281 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 6] train=0.449499 val=0.552200 loss=77451.577271 time: 44.569487\n\n[[ 0.452255   -0.31299084  0.22648329  0.17139536  0.28409886]\n [ 0.21109304  0.11708056  0.2565159   0.13900171  0.04933485]\n [ 0.02846311  0.2821089   0.7457465   0.92551816  0.5986448 ]\n [-0.14518592 -0.33234844 -0.17134857 -0.23381485 -0.09909555]\n [-0.17401686 -0.51273435 -1.02822    -0.8637349  -0.6559168 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 7] train=0.445833 val=0.556400 loss=78237.048737 time: 27.457928\n\n[[ 0.39242643 -0.34628582  0.23771971  0.19149111  0.29262406]\n [ 0.16466822  0.08001059  0.23920102  0.12276661  0.03994417]\n [ 0.01637606  0.261539    0.72233385  0.9023852   0.59601194]\n [-0.14037982 -0.3529687  -0.19654231 -0.24452208 -0.08235248]\n [-0.1642766  -0.5246074  -1.0468299  -0.8701091  -0.6333332 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 8] train=0.448417 val=0.543700 loss=77757.978516 time: 25.970762\n\n[[ 0.4065648  -0.36436224  0.23698868  0.22548643  0.3520135 ]\n [ 0.16893445  0.05477772  0.21739805  0.11658255  0.04798339]\n [ 0.03308998  0.24395196  0.69949657  0.8767489   0.5637944 ]\n [-0.09892397 -0.34727535 -0.20931108 -0.2639298  -0.12706773]\n [-0.10703294 -0.490573   -1.0362134  -0.86260235 -0.6473146 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 9] train=0.451803 val=0.546500 loss=77189.254730 time: 25.849416\n\n[[ 0.405656   -0.37398326  0.22047208  0.20370705  0.34143275]\n [ 0.14772753  0.03821338  0.21227388  0.112621    0.05563024]\n [ 0.01681782  0.24266495  0.72862434  0.92109954  0.60926735]\n [-0.12482523 -0.3614838  -0.19294329 -0.22526638 -0.07805261]\n [-0.12223309 -0.50582576 -1.0337119  -0.8422071  -0.6075455 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 10] train=0.449519 val=0.554500 loss=78034.948822 time: 25.628760\n\n[[ 0.40747103 -0.37124118  0.2341737   0.21388681  0.35200542]\n [ 0.15811372  0.04016154  0.21747416  0.10933322  0.05897734]\n [ 0.01582033  0.23238556  0.7241506   0.9073512   0.5919785 ]\n [-0.12551184 -0.3722287  -0.19698466 -0.23326458 -0.08977019]\n [-0.12028826 -0.51007766 -1.0386411  -0.85994333 -0.6167207 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 11] train=0.450601 val=0.552600 loss=77613.701599 time: 24.952468\n\n[[ 0.40006936 -0.39737105  0.19826204  0.17424697  0.33317304]\n [ 0.15726952  0.04809978  0.22079852  0.11037459  0.07575307]\n [ 0.00865673  0.24485242  0.7325354   0.9183651   0.61202776]\n [-0.13489492 -0.37282923 -0.20742318 -0.2461308  -0.09338227]\n [-0.12109002 -0.5048542  -1.0523195  -0.88021547 -0.62528926]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 12] train=0.446154 val=0.526400 loss=78171.736694 time: 25.498360\n\n[[ 0.40248716 -0.38779387  0.1895243   0.1412691   0.3065872 ]\n [ 0.15915547  0.05301428  0.20237298  0.07057402  0.04698141]\n [ 0.01756908  0.25137997  0.72760683  0.90485334  0.608406  ]\n [-0.14904457 -0.38059804 -0.21208696 -0.2476532  -0.08750135]\n [-0.1498757  -0.5135744  -1.0548853  -0.880506   -0.6349775 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 13] train=0.450461 val=0.538200 loss=77669.373108 time: 66.058326\n\n[[ 0.40778998 -0.38035342  0.2074541   0.1769462   0.342027  ]\n [ 0.15840833  0.05414841  0.21512644  0.10040086  0.07357105]\n [ 0.0343882   0.25782126  0.7407315   0.93500674  0.6292754 ]\n [-0.11295168 -0.3758228  -0.21226226 -0.22489153 -0.06971923]\n [-0.1297057  -0.52599925 -1.068676   -0.85520226 -0.6070811 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 14] train=0.444631 val=0.554000 loss=78186.479675 time: 27.076202\n\n[[ 0.3981157  -0.40274456  0.1855002   0.13429654  0.30257133]\n [ 0.170935    0.07938393  0.23283742  0.09215128  0.06471063]\n [ 0.03475282  0.2814757   0.7667915   0.9465232   0.639055  ]\n [-0.13972679 -0.38814008 -0.22221649 -0.23969652 -0.07523746]\n [-0.15402888 -0.5373258  -1.0880615  -0.8822894  -0.62756556]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 15] train=0.448317 val=0.560800 loss=77745.602539 time: 28.694687\n\n[[ 0.4007982  -0.39285374  0.1928086   0.13938722  0.32299492]\n [ 0.15670156  0.0825148   0.23559944  0.10093886  0.08138765]\n [-0.00314994  0.2583665   0.7594952   0.9595845   0.66458607]\n [-0.16395836 -0.4039934  -0.22398862 -0.22431833 -0.04684323]\n [-0.13713571 -0.52075875 -1.0762386  -0.8655438  -0.5996501 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 16] train=0.445954 val=0.554100 loss=77928.299561 time: 28.063915\n\n[[ 0.40453812 -0.39620155  0.16787605  0.09979747  0.30793133]\n [ 0.16253957  0.08246794  0.22914079  0.07092743  0.06127356]\n [ 0.00320132  0.25888     0.7683603   0.96182287  0.6641432 ]\n [-0.14706272 -0.4109033  -0.22775352 -0.22344391 -0.05056109]\n [-0.12583303 -0.5359886  -1.093564   -0.87101537 -0.6171581 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 17] train=0.448037 val=0.557500 loss=77985.413239 time: 24.879559\n\n[[ 0.38307986 -0.4291638   0.1463121   0.10482799  0.30991107]\n [ 0.18874416  0.08869509  0.23398988  0.08443433  0.08328428]\n [ 0.05270426  0.28115013  0.7676732   0.97118485  0.70149964]\n [-0.09963151 -0.3815714  -0.22922716 -0.22216855 -0.02496636]\n [-0.10448859 -0.51545084 -1.1021795  -0.8816662  -0.61777395]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 18] train=0.440465 val=0.545400 loss=78761.920380 time: 23.511547\n\n[[ 0.3779742  -0.42258602  0.15342686  0.12528504  0.32170728]\n [ 0.16596687  0.07870811  0.22723323  0.08340856  0.07626374]\n [ 0.03319703  0.2665708   0.7508446   0.95652825  0.68395257]\n [-0.10338563 -0.38286155 -0.24133709 -0.23199208 -0.04001555]\n [-0.10268742 -0.5108304  -1.1075702  -0.8814393  -0.61931777]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 19] train=0.439744 val=0.555300 loss=78665.115204 time: 25.964797\n\n[[ 0.38485736 -0.40671238  0.18479547  0.14987609  0.3256349 ]\n [ 0.12704177  0.06350812  0.22336066  0.06753091  0.04301707]\n [-0.00811075  0.24837022  0.7338868   0.9284741   0.6472797 ]\n [-0.13848224 -0.38230038 -0.23635747 -0.25183508 -0.06973238]\n [-0.13502625 -0.501871   -1.0804808  -0.87666845 -0.63548595]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 20] train=0.444952 val=0.536600 loss=78006.459991 time: 35.743994\n\n[[ 0.38922232 -0.41218492  0.16655138  0.1230657   0.31169876]\n [ 0.12950632  0.07246533  0.23791517  0.07622215  0.04659763]\n [-0.00339839  0.2626186   0.771673    0.97913027  0.6915861 ]\n [-0.14861982 -0.38046545 -0.21225926 -0.21055704 -0.02579344]\n [-0.15851144 -0.51058054 -1.0747185  -0.85884535 -0.6127334 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 21] train=0.446735 val=0.536500 loss=77924.485077 time: 33.700025\n\n[[ 0.41152236 -0.37490156  0.23060566  0.18825603  0.38111466]\n [ 0.13995656  0.10687756  0.28882074  0.11552347  0.07476277]\n [ 0.00340415  0.2931159   0.8075327   1.0030321   0.7049479 ]\n [-0.15311018 -0.37245834 -0.20774119 -0.21359365 -0.03124   ]\n [-0.16923906 -0.52156216 -1.0931475  -0.8740873  -0.6254721 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 22] train=0.445132 val=0.533800 loss=78027.135895 time: 27.597386\n\n[[ 0.33712098 -0.4467258   0.1463014   0.10706119  0.29548818]\n [ 0.09646565  0.06150639  0.23187429  0.05785448  0.00266814]\n [-0.01268966  0.27780712  0.77598464  0.9609989   0.6486959 ]\n [-0.14661376 -0.3613817  -0.21808358 -0.24524368 -0.07742622]\n [-0.1380322  -0.49906814 -1.0890651  -0.89337206 -0.6562742 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 23] train=0.446434 val=0.534600 loss=78003.248627 time: 25.339737\n\n[[ 0.390722   -0.39816803  0.18453345  0.12738554  0.32461485]\n [ 0.13049413  0.10179877  0.27314943  0.08545818  0.03020849]\n [-0.01014049  0.27448103  0.79353994  0.9972646   0.68592334]\n [-0.15904748 -0.4078308  -0.24995211 -0.24409162 -0.05727414]\n [-0.11681941 -0.5131525  -1.1186147  -0.9154763  -0.6610018 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 24] train=0.447196 val=0.552200 loss=78096.507660 time: 27.729516\n\n[[ 0.39757    -0.41144812  0.16675395  0.10850208  0.32110518]\n [ 0.11297515  0.08176107  0.25502127  0.05318643  0.00528899]\n [-0.00622026  0.26899588  0.79089123  0.9866974   0.6713101 ]\n [-0.13785435 -0.39784473 -0.25406915 -0.25773108 -0.07818192]\n [-0.0696954  -0.47343227 -1.1028104  -0.9098201  -0.6613572 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 25] train=0.446094 val=0.543600 loss=77911.160034 time: 25.434282\n\n[[ 0.41299424 -0.41226178  0.18685125  0.13109964  0.34746808]\n [ 0.130746    0.07872049  0.27963266  0.08346923  0.04339108]\n [ 0.02332743  0.26817396  0.8046843   1.0044105   0.6912552 ]\n [-0.10447237 -0.38900554 -0.24135928 -0.25415704 -0.07085179]\n [-0.07110985 -0.4731077  -1.0940424  -0.9140032  -0.6607314 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 26] train=0.447636 val=0.560900 loss=77720.441040 time: 29.505572\n\n[[ 0.40643722 -0.43092898  0.17742905  0.13196974  0.35590047]\n [ 0.12787779  0.05643348  0.26396978  0.07346306  0.04734005]\n [ 0.0188309   0.25371626  0.8009547   1.0157037   0.71266186]\n [-0.10785086 -0.39781147 -0.24478719 -0.25116378 -0.07258442]\n [-0.06571518 -0.47567028 -1.1009742  -0.93702185 -0.6947668 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 27] train=0.436899 val=0.514800 loss=79643.417725 time: 27.171174\n\n[[ 0.41116688 -0.44370478  0.1917443   0.14751455  0.35992965]\n [ 0.12713373  0.03940098  0.2605278   0.08133598  0.05017928]\n [ 0.03408568  0.24591891  0.7830948   1.0122533   0.71258456]\n [-0.10123467 -0.41092834 -0.25794816 -0.25622582 -0.06709835]\n [-0.05173972 -0.4877001  -1.1040269  -0.9406457  -0.68095666]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 28] train=0.442408 val=0.543900 loss=78228.440155 time: 28.819840\n\n[[ 0.43871418 -0.4268235   0.22148326  0.16835904  0.39098474]\n [ 0.15458873  0.0532721   0.28182173  0.08586588  0.05326642]\n [ 0.02847201  0.22468641  0.77059823  0.9949336   0.6949992 ]\n [-0.12737459 -0.4384063  -0.28024143 -0.2778736  -0.07882094]\n [-0.08241149 -0.5138828  -1.1326215  -0.9739536  -0.7045553 ]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 29] train=0.441306 val=0.558800 loss=78423.058838 time: 28.813068\n\n[[ 0.42758897 -0.43156767  0.23298389  0.16771322  0.39355603]\n [ 0.12658817  0.06102463  0.30403697  0.09466057  0.06939977]\n [ 0.00705284  0.23921444  0.79961646  1.0174841   0.7315595 ]\n [-0.13524076 -0.42778233 -0.27257568 -0.26346523 -0.03519289]\n [-0.09019237 -0.50845927 -1.1348329  -0.9681965  -0.668556  ]]\n<NDArray 5x5 @cpu(0)>\n"
 }
]
```

```{.python .input  n=148}
net5.collect_params()
```

```{.json .output n=148}
[
 {
  "data": {
   "text/plain": "sequential8_ (\n  Parameter sequential8_conv0_weight (shape=(6L, 3L, 5L, 5L), dtype=<type 'numpy.float32'>)\n  Parameter sequential8_conv0_bias (shape=(6L,), dtype=<type 'numpy.float32'>)\n  Parameter sequential8_conv1_weight (shape=(16L, 6L, 5L, 5L), dtype=<type 'numpy.float32'>)\n  Parameter sequential8_conv1_bias (shape=(16L,), dtype=<type 'numpy.float32'>)\n  Parameter sequential8_dense0_weight (shape=(120L, 400L), dtype=float32)\n  Parameter sequential8_dense0_bias (shape=(120L,), dtype=float32)\n  Parameter sequential8_dense1_weight (shape=(84L, 120L), dtype=float32)\n  Parameter sequential8_dense1_bias (shape=(84L,), dtype=float32)\n  Parameter sequential8_dense2_weight (shape=(10L, 84L), dtype=float32)\n  Parameter sequential8_dense2_bias (shape=(10L,), dtype=float32)\n)"
  },
  "execution_count": 148,
  "metadata": {},
  "output_type": "execute_result"
 }
]
```

```{.python .input  n=149}
print net5[0].weight.data()[0][0] #1
train_history5.plot() #1
```

```{.json .output n=149}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[ 0.42758897 -0.43156767  0.23298389  0.16771322  0.39355603]\n [ 0.12658817  0.06102463  0.30403697  0.09466057  0.06939977]\n [ 0.00705284  0.23921444  0.79961646  1.0174841   0.7315595 ]\n [-0.13524076 -0.42778233 -0.27257568 -0.26346523 -0.03519289]\n [-0.09019237 -0.50845927 -1.1348329  -0.9681965  -0.668556  ]]\n<NDArray 5x5 @cpu(0)>\n"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x1250cc290>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 }
]
```

```{.python .input  n=164}
class MyInit(mx.init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.normal(loc=0, scale=0.1, shape=data.shape )
global states
global t,v1,s1,vb1,sb1,v2,s2,vb2,sb2,v3,s3,vb3,sb3,v4,s4,vb4,sb4,v5,s5,vb5,sb5
t=1
v1=nd.zeros((6,3,5,5))
vb1=nd.zeros((6,))
v2=nd.zeros((16,6,5,5))
vb2=nd.zeros((16,))
v3=nd.zeros((120,400))
vb3=nd.zeros((120,))
v4=nd.zeros((84,120))
vb4=nd.zeros((84,))
v5=nd.zeros((10,84))
vb5=nd.zeros((10,))
s1=nd.zeros((6,3,5,5))
sb1=nd.zeros((6,))
s2=nd.zeros((16,6,5,5))
sb2=nd.zeros((16,))
s3=nd.zeros((120,400))
sb3=nd.zeros((120,))
s4=nd.zeros((84,120))
sb4=nd.zeros((84,))
s5=nd.zeros((10,84))
sb5=nd.zeros((10,))
states=[[v1,s1],[vb1,sb1],[v2,s2],[vb2,sb2],[v3,s3],[vb3,sb3],[v4,s4],[vb4,sb4],[v5,s5],[vb5,sb5]]

def adam(net,  lr): 
    global t
    global states
    beta1, beta2, eps = 0.9, 0.999, 1e-6 
    params=[]
    for layer in net:
        try:
            params.append(layer.weight)
            params.append(layer.bias)
        except:
            pass
    for p, (v, s) in zip(params, states):
        v[:] = beta1 * v + (1 - beta1) * p.grad()
        s[:] = beta2 * s + (1 - beta2) * p.grad().square()
        v_bias_corr = v / (1 - beta1 ** t)
        s_bias_corr = s / (1 - beta2 ** t)
        p.data()[:] -= lr * v_bias_corr / (s_bias_corr.sqrt() + eps)
    t += 1
    
ctx_list=[mx.cpu(1)]
ctx=mx.cpu(1)
net6 = nn.Sequential()
with net6.name_scope():
    net6.add(nn.Conv2D(channels=6, kernel_size=5)) 
    net6.add(gluon.nn.Activation(activation='relu'))
    net6.add(nn.MaxPool2D(pool_size=2, strides=2))
    net6.add(nn.Conv2D(channels=16,kernel_size=5))
    net6.add(gluon.nn.Activation(activation='relu'))
    net6.add(nn.MaxPool2D(pool_size=2, strides=2))
    net6.add(nn.Flatten())
    net6.add(nn.Dense(120)) 
    net6.add(gluon.nn.Activation(activation='relu'))
    net6.add(nn.Dense(84)) 
    net6.add(gluon.nn.Activation(activation='relu'))
    net6.add(nn.Dense(num_output))
            
print net6
net6.initialize(MyInit(),ctx=ctx)

lr_decay = 0.001
lr_decay_epoch = [80, 160, np.inf]
lr_decay_count = 0
loss_fn6 = gluon.loss.SoftmaxCrossEntropyLoss()
train_metric6 = mx.metric.Accuracy()
train_history6 = TrainingHistory(['training-error', 'validation-error'])
```

```{.json .output n=164}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): Activation(relu)\n  (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (3): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (4): Activation(relu)\n  (5): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (6): Flatten\n  (7): Dense(None -> 120, linear)\n  (8): Activation(relu)\n  (9): Dense(None -> 84, linear)\n  (10): Activation(relu)\n  (11): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=177}
epochs=10
# lr=0.001
for epoch in range(epochs):
    tic = time.time()
    train_metric6.reset() #1
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        #trainer6.set_learning_rate(trainer6.learning_rate*lr_decay) #2
        lr=lr*lr_decay
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net6(X) for X in data] #1
            loss = [loss_fn6(yhat, y) for yhat, y in zip(output, label)] #1

        # Backpropagation
        for l in loss:
            l.backward()
        if epoch==0:
            if i==0:
                print net6[0].weight.data()[0][0] #1
#         if i==0:
#             for layer in net5:
#                 try:
#                     print layer.weight.grad().shape
#                     print layer.bias.grad().shape
#                 except:
#                     pass
        adam(net6,lr) #1
        # Optimize
        #trainer6.step(batch_size) #1

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric6.update(label, output) #1

    name, acc = train_metric6.get() #1
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net6) #1

    # Update history and print metrics
    train_history6.update([1-acc, 1-val_acc]) #1
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net6[0].weight.data()[0][0] #1

    
file_name = "net-params_std0_01_lr10_last2" #1
net6.save_parameters(file_name) #1
```

```{.json .output n=177}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[ 0.13789798  0.12969457  0.13260467 -0.03708623 -0.05042007]\n [ 0.02743804  0.19840547  0.04729867  0.08759777 -0.05804766]\n [-0.01654569  0.08367206  0.16653602  0.08718304 -0.03725444]\n [-0.0847406   0.02606804  0.12921938 -0.10102243  0.00362401]\n [ 0.02861431 -0.23949026 -0.09344701 -0.08510909  0.03268739]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 0] train=0.567849 val=0.669000 loss=60793.658386 time: 32.694124\n\n[[ 0.13636665  0.12704174  0.12881882 -0.04267247 -0.05662027]\n [ 0.02704046  0.19737223  0.04513711  0.08317082 -0.06245222]\n [-0.01595335  0.08418552  0.16609642  0.08423757 -0.03924312]\n [-0.08429295  0.02573623  0.12768802 -0.10459118  0.00206117]\n [ 0.02725575 -0.24014983 -0.09442585 -0.08765147  0.03244806]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 1] train=0.564543 val=0.673000 loss=61188.886353 time: 27.484092\n\n[[ 0.13754317  0.12608458  0.12670293 -0.04408899 -0.05505164]\n [ 0.02858323  0.19651668  0.04388848  0.08314404 -0.06032525]\n [-0.01468397  0.08353502  0.16548675  0.08413456 -0.03726935]\n [-0.08513089  0.0228903   0.12500499 -0.10693952  0.00310668]\n [ 0.02537756 -0.24453957 -0.09915268 -0.09241859  0.03129689]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 2] train=0.565885 val=0.674800 loss=60930.153015 time: 29.804587\n\n[[ 0.13797902  0.12647745  0.12805325 -0.0429372  -0.05242779]\n [ 0.02900229  0.19744512  0.04600666  0.08465121 -0.05799678]\n [-0.01337466  0.08588045  0.16827753  0.08584973 -0.03484171]\n [-0.08185697  0.02698982  0.12887044 -0.10412004  0.00640141]\n [ 0.03047676 -0.23905452 -0.09478401 -0.08840509  0.03772848]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 3] train=0.568630 val=0.672000 loss=61051.937958 time: 28.343930\n\n[[ 0.13756435  0.12727478  0.12821013 -0.04435198 -0.05227203]\n [ 0.02901619  0.19831266  0.04452152  0.08083773 -0.05921774]\n [-0.01235962  0.08792655  0.16796446  0.08331438 -0.03534507]\n [-0.08134427  0.02919982  0.12987904 -0.1037453   0.00872329]\n [ 0.02860567 -0.23851915 -0.09522386 -0.08920582  0.03913023]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 4] train=0.571875 val=0.653400 loss=60873.841919 time: 26.308269\n\n[[ 0.13699134  0.1296846   0.13176204 -0.04086531 -0.05320457]\n [ 0.02875179  0.20029546  0.0481325   0.08272682 -0.06236728]\n [-0.01159398  0.08991219  0.17030776  0.08284825 -0.04012682]\n [-0.08056306  0.03060492  0.1301123  -0.1064721   0.00192883]\n [ 0.02958076 -0.23687325 -0.09535255 -0.09349126  0.03011519]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 5] train=0.568249 val=0.667500 loss=60666.534332 time: 23.051305\n\n[[ 0.13936105  0.12936467  0.13151647 -0.03981867 -0.05397448]\n [ 0.03168743  0.20055561  0.04722153  0.08236684 -0.06455123]\n [-0.00851083  0.09125568  0.1702865   0.08380328 -0.04024412]\n [-0.07882854  0.03240584  0.1310747  -0.10399358  0.00396311]\n [ 0.02758709 -0.23710857 -0.09572516 -0.09205227  0.0305917 ]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 6] train=0.569631 val=0.665400 loss=60706.551544 time: 29.006557\n\n[[ 0.13649638  0.12461855  0.1271299  -0.0411735  -0.05561705]\n [ 0.02991517  0.1975246   0.04411172  0.08111229 -0.06534667]\n [-0.00857925  0.08994343  0.16805014  0.08270331 -0.04018465]\n [-0.08046263  0.02914278  0.12695666 -0.10636642  0.00406442]\n [ 0.02339    -0.24220975 -0.10058957 -0.09405738  0.03047711]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 7] train=0.571675 val=0.663900 loss=60537.110870 time: 26.883269\n\n[[ 0.13341723  0.12258524  0.12498567 -0.04113579 -0.05331133]\n [ 0.02775827  0.19540967  0.04185833  0.08041772 -0.06435727]\n [-0.00800236  0.09064405  0.16845328  0.08265045 -0.04012038]\n [-0.0801139   0.03069525  0.12834945 -0.10701879  0.00256734]\n [ 0.02324967 -0.24001323 -0.09904889 -0.09429877  0.02933978]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 8] train=0.569571 val=0.661900 loss=60705.753845 time: 27.294043\n\n[[ 0.1360195   0.1250562   0.1265925  -0.04126092 -0.05355776]\n [ 0.02921558  0.19736187  0.04431628  0.08205584 -0.06304758]\n [-0.0078875   0.09123863  0.17017488  0.08381123 -0.03876735]\n [-0.08096681  0.02903236  0.12797819 -0.10655679  0.00301989]\n [ 0.02317789 -0.24187835 -0.10087512 -0.09595191  0.0266788 ]]\n<NDArray 5x5 @cpu(1)>\n[Epoch 9] train=0.573137 val=0.659200 loss=60719.004486 time: 24.792578\n\n[[ 0.13401578  0.12521504  0.12886009 -0.03726609 -0.04742403]\n [ 0.02784427  0.1966699   0.0443279   0.08393182 -0.05940511]\n [-0.00729581  0.09145215  0.16865215  0.08210963 -0.0390407 ]\n [-0.07737559  0.0323826   0.12902638 -0.10681837  0.00284777]\n [ 0.03035182 -0.23573463 -0.09774556 -0.09336603  0.02951111]]\n<NDArray 5x5 @cpu(1)>\n"
 }
]
```

```{.python .input  n=178}
print net6[0].weight.data()[0][0] #1
train_history6.plot() #1
#scale=0.1 lr=0.1 or 0.001
```

```{.json .output n=178}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[ 0.13401578  0.12521504  0.12886009 -0.03726609 -0.04742403]\n [ 0.02784427  0.1966699   0.0443279   0.08393182 -0.05940511]\n [-0.00729581  0.09145215  0.16865215  0.08210963 -0.0390407 ]\n [-0.07737559  0.0323826   0.12902638 -0.10681837  0.00284777]\n [ 0.03035182 -0.23573463 -0.09774556 -0.09336603  0.02951111]]\n<NDArray 5x5 @cpu(1)>\n"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12abfd950>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 }
]
```

```{.python .input}
#net7 without relu after the dense
```

```{.python .input  n=181}
class MyInit(mx.init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.normal(loc=0, scale=0.01, shape=data.shape )
global states
global t,v1,s1,vb1,sb1,v2,s2,vb2,sb2,v3,s3,vb3,sb3,v4,s4,vb4,sb4,v5,s5,vb5,sb5
t=1
v1=nd.zeros((6,3,5,5))
vb1=nd.zeros((6,))
v2=nd.zeros((16,6,5,5))
vb2=nd.zeros((16,))
v3=nd.zeros((120,400))
vb3=nd.zeros((120,))
v4=nd.zeros((84,120))
vb4=nd.zeros((84,))
v5=nd.zeros((10,84))
vb5=nd.zeros((10,))
s1=nd.zeros((6,3,5,5))
sb1=nd.zeros((6,))
s2=nd.zeros((16,6,5,5))
sb2=nd.zeros((16,))
s3=nd.zeros((120,400))
sb3=nd.zeros((120,))
s4=nd.zeros((84,120))
sb4=nd.zeros((84,))
s5=nd.zeros((10,84))
sb5=nd.zeros((10,))
states=[[v1,s1],[vb1,sb1],[v2,s2],[vb2,sb2],[v3,s3],[vb3,sb3],[v4,s4],[vb4,sb4],[v5,s5],[vb5,sb5]]

def adam(net,  lr): 
    global t
    global states
    beta1, beta2, eps = 0.9, 0.999, 1e-6 
    params=[]
    for layer in net:
        try:
            params.append(layer.weight)
            params.append(layer.bias)
        except:
            pass
    for p, (v, s) in zip(params, states):
        v[:] = beta1 * v + (1 - beta1) * p.grad()
        s[:] = beta2 * s + (1 - beta2) * p.grad().square()
        v_bias_corr = v / (1 - beta1 ** t)
        s_bias_corr = s / (1 - beta2 ** t)
        p.data()[:] -= lr * v_bias_corr / (s_bias_corr.sqrt() + eps)
    t += 1
    
ctx_list=[mx.cpu(0)]
ctx=mx.cpu(0)
net7 = nn.Sequential()
with net7.name_scope():
    net7.add(nn.Conv2D(channels=6, kernel_size=5)) 
    net7.add(gluon.nn.Activation(activation='relu'))
    net7.add(nn.MaxPool2D(pool_size=2, strides=2))
    net7.add(nn.Conv2D(channels=16,kernel_size=5))
    net7.add(gluon.nn.Activation(activation='relu'))
    net7.add(nn.MaxPool2D(pool_size=2, strides=2))
    net7.add(nn.Flatten())
    net7.add(nn.Dense(120)) 
    net7.add(nn.Dense(84)) 
    net7.add(nn.Dense(num_output))
            
print net7
net7.initialize(MyInit(),ctx=ctx)

lr_decay = 0.01
lr_decay_epoch = [80, 160, np.inf]
lr_decay_count = 0
loss_fn7= gluon.loss.SoftmaxCrossEntropyLoss() 
train_metric7 = mx.metric.Accuracy()
train_history7 = TrainingHistory(['training-error', 'validation-error'])
lr=0.001
```

```{.json .output n=181}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): Activation(relu)\n  (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (3): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (4): Activation(relu)\n  (5): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (6): Flatten\n  (7): Dense(None -> 120, linear)\n  (8): Dense(None -> 84, linear)\n  (9): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=182}
epochs=5

for epoch in range(epochs):
    tic = time.time()
    train_metric7.reset() #1
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        #trainer7.set_learning_rate(trainer7.learning_rate*lr_decay) #2
        lr=lr*lr_decay
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net7(X) for X in data] #1
            loss = [loss_fn7(yhat, y) for yhat, y in zip(output, label)] #1

        # Backpropagation
        for l in loss:
            l.backward()
        if epoch==0:
            if i==0:
                print net7[0].weight.data()[0][0] #1
#         if i==0:
#             for layer in net5:
#                 try:
#                     print layer.weight.grad().shape
#                     print layer.bias.grad().shape
#                 except:
#                     pass
        adam(net7,lr) #1
        # Optimize
        #trainer7.step(batch_size) #1

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric7.update(label, output) #1

    name, acc = train_metric7.get() #1
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net7) #1

    # Update history and print metrics
    train_history7.update([1-acc, 1-val_acc]) #1
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net7[0].weight.data()[0][0] #1

    
file_name = "net-params_std0_01_lr10_last3" #1
net7.save_parameters(file_name) #1

#scale 0.1 lr0.001 better than scale 0.01 lr 0.01
```

```{.json .output n=182}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "('Init', 'sequential17_conv0_weight', (6L, 3L, 5L, 5L))\n('Init', 'sequential17_conv1_weight', (16L, 6L, 5L, 5L))\n('Init', 'sequential17_dense0_weight', (120L, 400L))\n('Init', 'sequential17_dense1_weight', (84L, 120L))\n('Init', 'sequential17_dense2_weight', (10L, 84L))\n\n[[ 0.0112647   0.00400558 -0.00729577  0.00058491  0.00127478]\n [ 0.00763348 -0.01552309 -0.01568112  0.01074668  0.02022139]\n [ 0.00088336 -0.0134148   0.00172599 -0.01349347  0.0018218 ]\n [ 0.01141438  0.00741596 -0.02789942  0.00111885  0.01062062]\n [-0.01476984 -0.00484234 -0.0005816  -0.00265195  0.01810852]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 0] train=0.212640 val=0.311900 loss=103525.530426 time: 25.763818\n\n[[ 0.02182298  0.00060438 -0.01773865 -0.01328452 -0.01372221]\n [ 0.00839107 -0.02798025 -0.03834615 -0.01491528 -0.0047926 ]\n [ 0.00510747 -0.02423731 -0.01987244 -0.03822344 -0.02141727]\n [ 0.02786763  0.01006063 -0.03599426 -0.01032478  0.00089577]\n [ 0.0234785   0.02152253  0.01376878  0.00806874  0.02994923]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 1] train=0.287620 val=0.383400 loss=94986.755096 time: 25.474015\n\n[[ 0.0694551   0.04322599  0.01968539  0.02164048  0.02240898]\n [ 0.0338055  -0.00823891 -0.02381099 -0.00192402  0.01084295]\n [ 0.01671028 -0.01820659 -0.01884155 -0.03799646 -0.01721389]\n [ 0.03683719  0.01361987 -0.037384   -0.01291505  0.00276133]\n [ 0.03322442  0.02596658  0.01240198  0.00382183  0.02985315]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 2] train=0.331591 val=0.437000 loss=89880.257233 time: 25.029931\n\n[[ 0.08663395  0.06385972  0.04266909  0.0477493   0.05199365]\n [ 0.04081683  0.00215494 -0.01138752  0.01360965  0.03044377]\n [ 0.01242087 -0.02006008 -0.01815957 -0.03339831 -0.00818311]\n [ 0.02517336  0.00344107 -0.04434511 -0.01592508  0.00426301]\n [ 0.01680114  0.00981765 -0.00168134 -0.00648884  0.02463383]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 3] train=0.351863 val=0.452900 loss=87395.806366 time: 27.270699\n\n[[ 0.09576458  0.07748903  0.06048529  0.06810854  0.07341065]\n [ 0.03976563  0.00442693 -0.0057934   0.02227744  0.04151576]\n [ 0.00242093 -0.02728621 -0.02267208 -0.03511465 -0.00732893]\n [ 0.01103156 -0.00788609 -0.05331277 -0.02277601 -0.0011983 ]\n [ 0.00107087 -0.00294644 -0.01309587 -0.01642336  0.0149494 ]]\n<NDArray 5x5 @cpu(0)>\n"
 },
 {
  "name": "stderr",
  "output_type": "stream",
  "text": "Process Process-12489:\nProcess Process-12483:\nProcess Process-12486:\nProcess Process-12488:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n    self.run()\n    self.run()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self.run()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n    idx, samples = key_queue.get()\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n    idx, samples = key_queue.get()\n    idx, samples = key_queue.get()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 115, in get\n    idx, samples = key_queue.get()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 117, in get\n    self._rlock.acquire()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 115, in get\n    self._rlock.acquire()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 115, in get\nKeyboardInterrupt\nKeyboardInterrupt\n    self._rlock.acquire()\nProcess Process-12485:\nTraceback (most recent call last):\nProcess Process-12482:\nKeyboardInterrupt\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\nTraceback (most recent call last):\n    self._target(*self._args, **self._kwargs)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n    res = self._recv()\n    idx, samples = key_queue.get()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 115, in get\n    self.run()\n    self._rlock.acquire()\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 88, in recv\nKeyboardInterrupt\n    buf = self.recv_bytes()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\nKeyboardInterrupt\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n    idx, samples = key_queue.get()\nProcess Process-12484:\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 115, in get\nTraceback (most recent call last):\n    self._rlock.acquire()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\nKeyboardInterrupt\n    self.run()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\nProcess Process-12487:\n    self._target(*self._args, **self._kwargs)\nTraceback (most recent call last):\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 166, in worker_loop\n    idx, samples = key_queue.get()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 115, in get\n    self.run()\n    self._rlock.acquire()\nKeyboardInterrupt\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataloader.py\", line 169, in worker_loop\n    batch = batchify_fn([dataset[i] for i in samples])\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataset.py\", line 133, in __getitem__\n    return self._fn(*item)\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/data/dataset.py\", line 93, in base_fn\n    return (fn(x),) + args\n  File \"/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/gluon/block.py\", line 538, in __call__\n    for hook in self._forward_pre_hooks.values():\nKeyboardInterrupt\n"
 },
 {
  "ename": "KeyboardInterrupt",
  "evalue": "",
  "output_type": "error",
  "traceback": [
   "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
   "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
   "\u001b[0;32m<ipython-input-182-1c3acb4b163d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Update metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_metric7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
   "\u001b[0;32m/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/ndarray/ndarray.pyc\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
   "\u001b[0;32m/Users/lambert/Library/Python/2.7/lib/python/site-packages/mxnet/ndarray/ndarray.pyc\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
   "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
  ]
 }
]
```

```{.python .input}
print net7[0].weight.data()[0][0] #1
train_history7.plot() #1
```

```{.python .input  n=183}
#relu
class MyInit(mx.init.Initializer):
    def _init_weight(self, name, data):
        print('Init', name, data.shape)
        data[:] = nd.random.normal(loc=0, scale=0.1, shape=data.shape )
    
ctx_list=[mx.cpu(0)]
ctx=mx.cpu(0)
net8 = nn.Sequential()
with net8.name_scope():
    net8.add(nn.Conv2D(channels=6, kernel_size=5)) 
    net8.add(gluon.nn.Activation(activation='relu'))
    net8.add(nn.MaxPool2D(pool_size=2, strides=2))
    net8.add(nn.Conv2D(channels=16,kernel_size=5))
    net8.add(gluon.nn.Activation(activation='relu'))
    net8.add(nn.MaxPool2D(pool_size=2, strides=2))
    net8.add(nn.Flatten())
    net8.add(nn.Dense(120)) 
    net8.add(nn.Dense(84)) 
    net8.add(nn.Dense(num_output))
            
print net8
net8.initialize(MyInit(),ctx=ctx)

lr_decay = 0.001
lr_decay_epoch = [80, 160, np.inf]
lr_decay_count = 0
loss_fn8= gluon.loss.SoftmaxCrossEntropyLoss() 
train_metric8 = mx.metric.Accuracy()
train_history8 = TrainingHistory(['training-error', 'validation-error'])
optimizer = 'adam'
optimizer_params = {'learning_rate': 0.01, 'wd': 0.0005}
trainer8 = gluon.Trainer(net8.collect_params(), optimizer, optimizer_params)
```

```{.json .output n=183}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Sequential(\n  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): Activation(relu)\n  (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (3): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1))\n  (4): Activation(relu)\n  (5): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n  (6): Flatten\n  (7): Dense(None -> 120, linear)\n  (8): Dense(None -> 84, linear)\n  (9): Dense(None -> 10, linear)\n)\n"
 }
]
```

```{.python .input  n=186}
epochs=5
for epoch in range(epochs):
    tic = time.time()
    train_metric8.reset() #1
    train_loss = 0

    # Learning rate decay
    if epoch == lr_decay_epoch[lr_decay_count]:
        trainer8.set_learning_rate(trainer8.learning_rate*lr_decay) #2
        lr=lr*lr_decay
        lr_decay_count += 1

    # Loop through each batch of training data
    for i, batch in enumerate(train_data):
        # Extract data and label
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx_list, batch_axis=0)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx_list, batch_axis=0)
        # AutoGrad
        with ag.record():
            output = [net8(X) for X in data] #1
            loss = [loss_fn8(yhat, y) for yhat, y in zip(output, label)] #1

        # Backpropagation
        for l in loss:
            l.backward()
        if epoch==0:
            if i==0:
                print net8[0].weight.data()[0][0] #1
#         if i==0:
#             for layer in net5:
#                 try:
#                     print layer.weight.grad().shape
#                     print layer.bias.grad().shape
#                 except:
#                     pass
        #adam(net8,lr) #1
        # Optimize
        trainer8.step(batch_size) #1

        # Update metrics
        train_loss += sum([l.sum().asscalar() for l in loss])
        train_metric8.update(label, output) #1

    name, acc = train_metric8.get() #1
    # Evaluate on Validation data
    name, val_acc = test(ctx_list, val_data,net8) #1

    # Update history and print metrics
    train_history8.update([1-acc, 1-val_acc]) #1
    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %
        (epoch, acc, val_acc, train_loss, time.time()-tic))
    print net8[0].weight.data()[0][0] #1

    
file_name = "net-params_std0_01_lr10_last4" #1
net8.save_parameters(file_name) #1
```

```{.json .output n=186}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[-0.21146251 -0.19663057  0.02710702 -0.07978154 -0.22644165]\n [-0.09384526 -0.062193   -0.11978793 -0.01368119 -0.12064365]\n [ 0.07076317 -0.05463662 -0.02368186  0.02125142  0.02618599]\n [ 0.07369276  0.10947502 -0.06528305  0.22304384  0.13432029]\n [-0.1270052  -0.06738947  0.0129757  -0.11436871 -0.10446305]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 0] train=0.280749 val=0.330000 loss=94927.293671 time: 27.528402\n\n[[-0.22796261 -0.2019569   0.03135344 -0.07638867 -0.23079081]\n [-0.09640224 -0.05659952 -0.10323135  0.00369195 -0.10737495]\n [ 0.06771596 -0.05799382 -0.02866094  0.01150707  0.02863622]\n [ 0.07562443  0.10651332 -0.06644537  0.2135808   0.14375474]\n [-0.1195487  -0.06022227  0.01710867 -0.10800415 -0.09473462]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 1] train=0.288502 val=0.356400 loss=94135.310974 time: 26.808851\n\n[[-0.22041495 -0.20109734  0.01368248 -0.10256088 -0.25373408]\n [-0.08700833 -0.06457515 -0.11921582 -0.01165682 -0.10991814]\n [ 0.06956507 -0.07095892 -0.04620258 -0.00110264  0.0336417 ]\n [ 0.1081197   0.11812698 -0.05203613  0.22807245  0.16734517]\n [-0.07642423 -0.04202894  0.02730707 -0.10276097 -0.09303307]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 2] train=0.300020 val=0.351100 loss=92840.413483 time: 25.398256\n\n[[-0.20883054 -0.17871942  0.04651559 -0.07119481 -0.21618354]\n [-0.08983204 -0.06312332 -0.11013902 -0.00097644 -0.08157746]\n [ 0.04689056 -0.08000445 -0.0468927  -0.00889575  0.04946042]\n [ 0.08243157  0.10167256 -0.05289514  0.21509634  0.16931471]\n [-0.09072971 -0.04870107  0.03571306 -0.09228781 -0.08048994]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 3] train=0.313582 val=0.382500 loss=91827.367157 time: 27.808537\n\n[[-0.17552362 -0.16128774  0.05626012 -0.05874064 -0.21958846]\n [-0.0928859  -0.08210729 -0.14151779 -0.03418224 -0.11435463]\n [ 0.01780511 -0.11685495 -0.09446137 -0.06029538  0.00792598]\n [ 0.07804959  0.08814775 -0.06514244  0.19484486  0.16312073]\n [-0.06235291 -0.02602807  0.05316311 -0.07625176 -0.04791344]]\n<NDArray 5x5 @cpu(0)>\n[Epoch 4] train=0.316707 val=0.351700 loss=91170.492523 time: 27.099804\n\n[[-0.16915648 -0.14574227  0.08125833 -0.03468856 -0.18578672]\n [-0.09479071 -0.07909011 -0.13505131 -0.0356767  -0.10063481]\n [-0.00276796 -0.12432522 -0.10830262 -0.08655041 -0.00906005]\n [ 0.06086891  0.07924165 -0.07328005  0.17489277  0.14604534]\n [-0.03936048  0.00293861  0.08313554 -0.04761628 -0.02289883]]\n<NDArray 5x5 @cpu(0)>\n"
 }
]
```

```{.python .input  n=187}
print net8[0].weight.data()[0][0] #1
train_history8.plot() #1
```

```{.json .output n=187}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\n[[-0.16915648 -0.14574227  0.08125833 -0.03468856 -0.18578672]\n [-0.09479071 -0.07909011 -0.13505131 -0.0356767  -0.10063481]\n [-0.00276796 -0.12432522 -0.10830262 -0.08655041 -0.00906005]\n [ 0.06086891  0.07924165 -0.07328005  0.17489277  0.14604534]\n [-0.03936048  0.00293861  0.08313554 -0.04761628 -0.02289883]]\n<NDArray 5x5 @cpu(0)>\n"
 },
 {
  "data": {
   "image/png": "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\n",
   "text/plain": "<matplotlib.figure.Figure at 0x12392d8d0>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 }
]
```

## Problem2 

### d)

From the acc, we can know that after adding BN layer, our
model yield 1.5% higher acc than the original one. What's more by using adam, we
can see that the model can converge faster and the acc-curve is much more sooth
than others.

```{.python .input}

```
